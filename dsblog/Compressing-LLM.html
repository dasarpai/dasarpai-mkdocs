<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Compressing-LLM.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Compressing Large Language Model - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#compressing-large-language-model class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Compressing Large Language Model </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#is-this-article-for-me class=md-nav__link> <span class=md-ellipsis> Is this article for me? </span> </a> </li> <li class=md-nav__item> <a href=#1-what-is-llm-compression class=md-nav__link> <span class=md-ellipsis> 1. What is LLM Compression? </span> </a> </li> <li class=md-nav__item> <a href=#2-why-is-llm-compression-necessary class=md-nav__link> <span class=md-ellipsis> 2. Why is LLM Compression Necessary? </span> </a> </li> <li class=md-nav__item> <a href=#3-what-are-the-different-techniques-for-llm-compression class=md-nav__link> <span class=md-ellipsis> 3. What are the Different Techniques for LLM Compression? </span> </a> <nav class=md-nav aria-label="3. What are the Different Techniques for LLM Compression?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31-how-does-quantization-work-in-llm-compression class=md-nav__link> <span class=md-ellipsis> 3.1. How Does Quantization Work in LLM Compression? </span> </a> </li> <li class=md-nav__item> <a href=#32-what-is-pruning-and-how-does-it-help-in-compressing-llms class=md-nav__link> <span class=md-ellipsis> 3.2. What is Pruning, and How Does it Help in Compressing LLMs? </span> </a> </li> <li class=md-nav__item> <a href=#33-can-you-explain-knowledge-distillation-in-the-context-of-llms class=md-nav__link> <span class=md-ellipsis> 3.3. Can You Explain Knowledge Distillation in the Context of LLMs? </span> </a> </li> <li class=md-nav__item> <a href=#34-what-is-low-rank-factorization-and-its-role-in-llm-compression class=md-nav__link> <span class=md-ellipsis> 3.4. What is Low-Rank Factorization and its Role in LLM Compression? </span> </a> </li> <li class=md-nav__item> <a href=#35-how-effective-are-weight-sharing-techniques-in-compressing-llms class=md-nav__link> <span class=md-ellipsis> 3.5. How Effective are Weight Sharing Techniques in Compressing LLMs? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-what-are-the-trade-offs-involved-in-llm-compression class=md-nav__link> <span class=md-ellipsis> 4. What are the Trade-offs Involved in LLM Compression? </span> </a> <nav class=md-nav aria-label="4. What are the Trade-offs Involved in LLM Compression?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-trade-offs-in-llm-compression class=md-nav__link> <span class=md-ellipsis> Key Trade-offs in LLM Compression: </span> </a> </li> <li class=md-nav__item> <a href=#examples-of-trade-offs class=md-nav__link> <span class=md-ellipsis> Examples of Trade-offs: </span> </a> </li> <li class=md-nav__item> <a href=#decision-making-in-compression class=md-nav__link> <span class=md-ellipsis> Decision-Making in Compression: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#5-how-does-fine-tuning-work-in-the-context-of-compressed-llms class=md-nav__link> <span class=md-ellipsis> 5. How Does Fine-Tuning Work in the Context of Compressed LLMs? </span> </a> <nav class=md-nav aria-label="5. How Does Fine-Tuning Work in the Context of Compressed LLMs?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#how-fine-tuning-works class=md-nav__link> <span class=md-ellipsis> How Fine-Tuning Works: </span> </a> </li> <li class=md-nav__item> <a href=#benefits-of-fine-tuning-in-compressed-llms class=md-nav__link> <span class=md-ellipsis> Benefits of Fine-Tuning in Compressed LLMs: </span> </a> </li> <li class=md-nav__item> <a href=#challenges class=md-nav__link> <span class=md-ellipsis> Challenges: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#6-what-role-does-hardware-play-in-llm-compression class=md-nav__link> <span class=md-ellipsis> 6. What Role Does Hardware Play in LLM Compression? </span> </a> <nav class=md-nav aria-label="6. What Role Does Hardware Play in LLM Compression?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-hardware-considerations-in-llm-compression class=md-nav__link> <span class=md-ellipsis> Key Hardware Considerations in LLM Compression: </span> </a> </li> <li class=md-nav__item> <a href=#impact-of-hardware-on-compression-techniques class=md-nav__link> <span class=md-ellipsis> Impact of Hardware on Compression Techniques: </span> </a> </li> <li class=md-nav__item> <a href=#challenges_1 class=md-nav__link> <span class=md-ellipsis> Challenges: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#7-what-are-the-ethical-considerations-in-llm-compression class=md-nav__link> <span class=md-ellipsis> 7. What are the Ethical Considerations in LLM Compression? </span> </a> <nav class=md-nav aria-label="7. What are the Ethical Considerations in LLM Compression?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-ethical-considerations-in-llm-compression class=md-nav__link> <span class=md-ellipsis> Key Ethical Considerations in LLM Compression: </span> </a> </li> <li class=md-nav__item> <a href=#mitigating-ethical-risks class=md-nav__link> <span class=md-ellipsis> Mitigating Ethical Risks: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#8-what-are-the-future-directions-in-llm-compression class=md-nav__link> <span class=md-ellipsis> 8. What are the Future Directions in LLM Compression? </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/Compressing-LLM.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/Compressing-LLM.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="Compressing Large Language Model" src=../assets/images/dspost/dsp6099-Compressing-LLM.jpg></p> <h1 id=compressing-large-language-model>Compressing Large Language Model<a class=headerlink href=#compressing-large-language-model title="Permanent link">&para;</a></h1> <h2 id=is-this-article-for-me>Is this article for me?<a class=headerlink href=#is-this-article-for-me title="Permanent link">&para;</a></h2> <p>If you are looking answers to following question then "Yes" - What is LLM compression? - Why is LLM compression necessary? - What are the different techniques for LLM compression? - How does quantization work in LLM compression? - What is pruning, and how does it help in compressing LLMs? - Can you explain knowledge distillation in the context of LLMs? - What is low-rank factorization and its role in LLM compression? - How effective are weight sharing techniques in compressing LLMs? - What are the trade-offs involved in LLM compression? - How does fine-tuning work in the context of compressed LLMs? - What are the benefits of fine-tuning in compressed LLMs? - What role does hardware play in LLM compression? - What are the ethical considerations in LLM compression? - What are the future directions in LLM compression?</p> <h2 id=1-what-is-llm-compression>1. <strong>What is LLM Compression?</strong><a class=headerlink href=#1-what-is-llm-compression title="Permanent link">&para;</a></h2> <p>LLM (Large Language Model) compression refers to a set of techniques and methodologies aimed at reducing the size of large language models while maintaining their performance as much as possible. Large language models, such as GPT, BERT, and their variants, often contain hundreds of millions to billions of parameters, making them resource-intensive to deploy and run. The sheer size of these models poses challenges in terms of storage, computation, and real-time inference, especially when deploying on devices with limited hardware resources like mobile phones or edge devices.</p> <p><strong>Compression</strong> in this context is the process of transforming a large, complex model into a smaller, more efficient version. This smaller model should ideally perform almost as well as the original, full-sized model, but with reduced memory requirements and faster computation times. The goal is to achieve an optimal balance between the model's size and its performance, enabling more practical deployment across various platforms and use cases.</p> <h2 id=2-why-is-llm-compression-necessary>2. <strong>Why is LLM Compression Necessary?</strong><a class=headerlink href=#2-why-is-llm-compression-necessary title="Permanent link">&para;</a></h2> <ul> <li> <p><strong>Resource Constraints:</strong> Many real-world applications, such as voice assistants, chatbots, or on-device AI, require models that can run efficiently on devices with limited computational power and memory. Compressing LLMs makes it feasible to deploy these models in such environments.</p> </li> <li> <p><strong>Cost Efficiency:</strong> Running large models on cloud infrastructure can be expensive due to the high demand for computational resources. Compressing the model reduces the cost associated with cloud-based inference and training.</p> </li> <li> <p><strong>Latency Reduction:</strong> In applications requiring real-time or near-real-time responses, such as conversational AI, high latency can significantly degrade the user experience. Compression helps reduce the computational overhead, leading to faster inference times.</p> </li> <li> <p><strong>Environmental Impact:</strong> Large-scale model training and deployment consume significant amounts of energy, contributing to the carbon footprint of AI technologies. Compressing models can lead to more energy-efficient AI systems.</p> </li> <li> <p><strong>Scalability:</strong> Smaller models are easier to scale across multiple platforms and devices, making it possible to reach a broader audience with AI-powered applications.</p> </li> </ul> <h2 id=3-what-are-the-different-techniques-for-llm-compression>3. <strong>What are the Different Techniques for LLM Compression?</strong><a class=headerlink href=#3-what-are-the-different-techniques-for-llm-compression title="Permanent link">&para;</a></h2> <p>LLM compression involves various techniques, each with its unique approach to reducing model size:</p> <ul> <li> <p><strong>Quantization:</strong> Reduces the precision of the model's weights, typically converting 32-bit floating-point numbers to 16-bit or 8-bit integers.</p> </li> <li> <p><strong>Pruning:</strong> Involves removing less important neurons or weights from the model, effectively "trimming the fat" without significantly affecting performance.</p> </li> <li> <p><strong>Knowledge Distillation:</strong> A process where a smaller model (student) is trained to replicate the behavior of a larger, more complex model (teacher), resulting in a more compact model.</p> </li> <li> <p><strong>Low-Rank Factorization:</strong> Decomposes large matrices into products of smaller matrices, thereby reducing the number of parameters.</p> </li> <li> <p><strong>Weight Sharing:</strong> Uses the same set of weights across different parts of the model, reducing redundancy and the total number of unique parameters.</p> </li> </ul> <p>These techniques, either used individually or in combination, allow developers to create more efficient versions of large language models that are better suited for deployment in various real-world scenarios.</p> <p>By understanding what LLM compression is and why it’s important, you lay the foundation for exploring the specific techniques and strategies used to achieve it, which can be further discussed in subsequent sections of your article.</p> <h3 id=31-how-does-quantization-work-in-llm-compression>3.1. <strong>How Does Quantization Work in LLM Compression?</strong><a class=headerlink href=#31-how-does-quantization-work-in-llm-compression title="Permanent link">&para;</a></h3> <p><strong>Quantization</strong> is a technique used to reduce the size of a large language model (LLM) by lowering the precision of the model’s parameters. Typically, models are trained using 32-bit floating-point numbers (FP32) for representing weights and activations. Quantization reduces these 32-bit numbers to lower bit-widths, such as 16-bit (FP16) or 8-bit integers (INT8), which significantly decreases the memory footprint of the model.</p> <ul> <li> <p><strong>Linear Quantization:</strong> The most common approach, where the continuous range of 32-bit floating-point values is mapped to a discrete set of lower-bit values. For instance, FP32 values are rounded or truncated to 8-bit integers. This reduction in precision leads to smaller model sizes and faster computation.</p> </li> <li> <p><strong>Dynamic Quantization:</strong> In dynamic quantization, weights are quantized during inference rather than at the time of model training. This allows for on-the-fly conversion of weights to lower precision, often resulting in a balance between speed and accuracy.</p> </li> <li> <p><strong>Static Quantization:</strong> Here, both the weights and activations are quantized ahead of time. This method is more efficient than dynamic quantization as it doesn’t require on-the-fly computation, but it can be less flexible.</p> </li> <li> <p><strong>Quantization-Aware Training:</strong> In this approach, the model is trained with quantization in mind. During training, the model simulates the effects of lower precision, allowing it to learn to operate effectively despite the reduced precision. This generally results in better performance than post-training quantization.</p> </li> </ul> <h3 id=32-what-is-pruning-and-how-does-it-help-in-compressing-llms>3.2. <strong>What is Pruning, and How Does it Help in Compressing LLMs?</strong><a class=headerlink href=#32-what-is-pruning-and-how-does-it-help-in-compressing-llms title="Permanent link">&para;</a></h3> <p><strong>Pruning</strong> is a technique used to reduce the size of large language models by eliminating unnecessary or less important parameters (weights) or neurons. The basic idea is that not all parts of a trained model contribute equally to its performance; some parameters may have little to no impact on the final predictions. By identifying and removing these less important components, the model becomes more efficient.</p> <ul> <li> <p><strong>Magnitude-Based Pruning:</strong> This is one of the simplest forms of pruning, where weights with small magnitudes (close to zero) are removed, assuming that they contribute little to the model’s output.</p> </li> <li> <p><strong>Structured Pruning:</strong> Involves removing entire neurons, channels, or layers based on their importance to the model’s performance. For example, a layer with many redundant neurons might be pruned to make the model more compact.</p> </li> <li> <p><strong>Unstructured Pruning:</strong> Individual weights across the model are pruned, without regard to their location in specific layers or structures. This can lead to sparse matrices, which require specialized libraries to take full advantage of the sparsity.</p> </li> <li> <p><strong>Iterative Pruning:</strong> Pruning is often done iteratively—small portions of the model are pruned, and the model is fine-tuned afterward to recover any lost accuracy. This cycle can be repeated multiple times to achieve the desired level of compression.</p> </li> </ul> <h3 id=33-can-you-explain-knowledge-distillation-in-the-context-of-llms>3.3. <strong>Can You Explain Knowledge Distillation in the Context of LLMs?</strong><a class=headerlink href=#33-can-you-explain-knowledge-distillation-in-the-context-of-llms title="Permanent link">&para;</a></h3> <p><strong>Knowledge Distillation</strong> is a technique where a smaller model (referred to as the "student") is trained to mimic the behavior of a larger, more complex model (the "teacher"). The idea is to transfer the knowledge captured by the large model into a more compact, efficient version without significantly losing accuracy. This method is particularly useful in compressing large language models, as it allows for the creation of smaller models that retain much of the performance of their larger counterparts.</p> <ul> <li> <p><strong>Teacher-Student Framework:</strong> The process begins by training a large, powerful model (the teacher) on a given task. Once trained, this model’s predictions are used as a form of "soft labels" to train the smaller model (the student). The student model is trained not just on the original dataset but also on the outputs (or logits) of the teacher model.</p> </li> <li> <p><strong>Soft Labels vs. Hard Labels:</strong> The teacher model typically outputs a probability distribution over classes (soft labels) rather than just the correct class (hard labels). These soft labels provide more information about the relationships between different classes, which helps the student model learn more effectively.</p> </li> <li> <p><strong>Loss Function:</strong> During training, the student model’s loss function combines the original task's loss (e.g., cross-entropy loss with hard labels) with a distillation loss, which measures how well the student model’s outputs match the teacher model’s soft labels.</p> </li> <li> <p><strong>Temperature Scaling:</strong> A technique often used in knowledge distillation to soften the probability distributions produced by the teacher model, making it easier for the student model to learn from them. A higher temperature makes the teacher’s predictions more uniform, emphasizing the relative probabilities between classes rather than the absolute values.</p> </li> </ul> <h3 id=34-what-is-low-rank-factorization-and-its-role-in-llm-compression>3.4. <strong>What is Low-Rank Factorization and its Role in LLM Compression?</strong><a class=headerlink href=#34-what-is-low-rank-factorization-and-its-role-in-llm-compression title="Permanent link">&para;</a></h3> <p><strong>Low-rank factorization</strong> is a technique used to compress large language models by approximating large weight matrices with the product of smaller, lower-rank matrices. In large language models, many operations involve matrix multiplications, and these matrices can be extremely large, contributing significantly to the overall size and computational complexity of the model. Low-rank factorization reduces this complexity by representing the large matrix as the multiplication of two or more smaller matrices.</p> <ul> <li> <p><strong>Matrix Decomposition:</strong> At the core of low-rank factorization is matrix decomposition. For a given large matrix <span class=arithmatex>\( W \)</span>, which could represent weights in a neural network layer, the idea is to find two (or more) smaller matrices <span class=arithmatex>\( U \)</span> and <span class=arithmatex>\( V \)</span> such that their product <span class=arithmatex>\( U \times V \)</span> approximates <span class=arithmatex>\( W \)</span>. Here, <span class=arithmatex>\( U \)</span> and <span class=arithmatex>\( V \)</span> have a lower rank, meaning they have fewer rows and columns compared to <span class=arithmatex>\( W \)</span>.</p> </li> <li> <p><strong>Singular Value Decomposition (SVD):</strong> One of the most common methods for low-rank factorization is Singular Value Decomposition. In SVD, a matrix is decomposed into three matrices <span class=arithmatex>\( U \)</span>, <span class=arithmatex>\( \Sigma \)</span>, and <span class=arithmatex>\( V^T \)</span>, where <span class=arithmatex>\( \Sigma \)</span> is a diagonal matrix containing singular values. By keeping only the top <span class=arithmatex>\( k \)</span> singular values, the matrices can be truncated, resulting in a lower-rank approximation.</p> </li> <li> <p><strong>Parameter Reduction:</strong> By reducing the rank, the number of parameters in the matrix decreases, leading to a smaller model size. This is particularly effective in layers where the full rank is not necessary for maintaining model performance.</p> </li> <li> <p><strong>Application in Neural Networks:</strong> In neural networks, low-rank factorization is typically applied to the weight matrices of fully connected layers, convolutional layers, or even attention mechanisms in transformer models. By factorizing these matrices, the model's size is reduced, and the computational load during inference is decreased.</p> </li> </ul> <h3 id=35-how-effective-are-weight-sharing-techniques-in-compressing-llms>3.5. <strong>How Effective are Weight Sharing Techniques in Compressing LLMs?</strong><a class=headerlink href=#35-how-effective-are-weight-sharing-techniques-in-compressing-llms title="Permanent link">&para;</a></h3> <p><strong>Weight sharing</strong> is a technique used in model compression where multiple parts of a neural network share the same weights. Instead of having unique weights for each neuron or connection, weight sharing allows the same set of weights to be reused across different layers or regions of the model. This technique significantly reduces the number of unique parameters in the model, leading to a smaller and more efficient model.</p> <ul> <li> <p><strong>Shared Weights Across Layers:</strong> In some neural network architectures, especially those with repetitive structures like convolutional neural networks (CNNs), the same set of weights can be shared across multiple layers. This reduces the total number of parameters without needing to learn new weights for each layer.</p> </li> <li> <p><strong>Grouped Convolution:</strong> In CNNs, grouped convolution is a form of weight sharing where different groups of input channels are convolved with the same set of filters. This reduces the number of filters needed, thereby compressing the model.</p> </li> <li> <p><strong>Tensor Factorization:</strong> Similar to low-rank factorization, tensor factorization techniques can be used to share weights across different dimensions of a tensor, such as in multi-head attention mechanisms in transformers. This reduces the number of unique parameters while maintaining the model's ability to process complex patterns.</p> </li> <li> <p><strong>Parameter Tying:</strong> In language models, parameter tying involves using the same parameters (weights) for different layers or components of the model. For example, the weights used in the embedding layer might be tied to those used in the final output layer, reducing the number of parameters.</p> </li> </ul> <h2 id=4-what-are-the-trade-offs-involved-in-llm-compression>4. <strong>What are the Trade-offs Involved in LLM Compression?</strong><a class=headerlink href=#4-what-are-the-trade-offs-involved-in-llm-compression title="Permanent link">&para;</a></h2> <p>LLM compression offers many benefits, such as reduced model size, faster inference times, and lower resource consumption. However, there are trade-offs that need to be carefully considered to ensure that the compressed model still meets the performance requirements of the target application.</p> <h3 id=key-trade-offs-in-llm-compression><strong>Key Trade-offs in LLM Compression:</strong><a class=headerlink href=#key-trade-offs-in-llm-compression title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Accuracy vs. Size:</strong> One of the most significant trade-offs in LLM compression is between model accuracy and size. As the model is compressed, whether through quantization, pruning, or other techniques, there is often a loss in accuracy. The challenge lies in finding the optimal balance where the model is small enough to be practical but still accurate enough to perform well on the intended task.</p> </li> <li> <p><strong>Speed vs. Complexity:</strong> Compression techniques often make models less complex, which can speed up inference times. However, this simplification might reduce the model’s ability to handle complex tasks or subtle nuances in data, leading to a potential drop in performance.</p> </li> <li> <p><strong>Generalization vs. Overfitting:</strong> Compression can sometimes improve generalization by forcing the model to focus on the most important features (similar to regularization). However, if not done carefully, it can also cause the model to lose the ability to capture important details, leading to underfitting.</p> </li> <li> <p><strong>Deployment Constraints:</strong> Compressed models are easier to deploy on devices with limited resources, such as mobile phones or IoT devices. However, the compression process itself can introduce constraints, such as the need for specific hardware support (e.g., for quantized models) or the requirement to use specialized libraries to take full advantage of sparsity in pruned models.</p> </li> <li> <p><strong>Retraining and Fine-Tuning Costs:</strong> After applying compression techniques, models often require retraining or fine-tuning to recover lost accuracy. This process can be time-consuming and computationally expensive, particularly for large models. The cost of retraining must be weighed against the benefits of compression.</p> </li> <li> <p><strong>Scalability vs. Customization:</strong> Compressed models are often more scalable across different platforms due to their smaller size. However, the process of compression might reduce the model’s ability to be fine-tuned or customized for specific tasks, limiting its versatility.</p> </li> </ul> <h3 id=examples-of-trade-offs><strong>Examples of Trade-offs:</strong><a class=headerlink href=#examples-of-trade-offs title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Quantization:</strong> Moving from 32-bit to 8-bit quantization can lead to faster inference and smaller model size but might also result in a drop in accuracy, especially in models sensitive to precision, like those used in certain NLP tasks.</p> </li> <li> <p><strong>Pruning:</strong> Pruning can effectively reduce model size by removing unnecessary parameters, but if too many important weights are pruned, the model’s performance can degrade significantly. Fine-tuning after pruning is often required to mitigate this.</p> </li> <li> <p><strong>Knowledge Distillation:</strong> While knowledge distillation can produce a smaller model with performance close to the original, the student model might not capture all the nuances of the teacher model, particularly if the teacher model is highly complex.</p> </li> </ul> <h3 id=decision-making-in-compression><strong>Decision-Making in Compression:</strong><a class=headerlink href=#decision-making-in-compression title="Permanent link">&para;</a></h3> <p>When compressing an LLM, developers must consider the specific requirements of their application, such as the acceptable trade-offs between size and accuracy, or between speed and model complexity. For instance, a model deployed on a mobile device might prioritize size and speed over absolute accuracy, while a model used in a high-stakes environment might prioritize accuracy, even if it means retaining a larger model size.</p> <p>Ultimately, the success of LLM compression depends on finding the right balance for the specific use case, taking into account the unique trade-offs that each compression technique presents.</p> <h2 id=5-how-does-fine-tuning-work-in-the-context-of-compressed-llms>5. <strong>How Does Fine-Tuning Work in the Context of Compressed LLMs?</strong><a class=headerlink href=#5-how-does-fine-tuning-work-in-the-context-of-compressed-llms title="Permanent link">&para;</a></h2> <p><strong>Fine-tuning</strong> in the context of compressed large language models (LLMs) is the process of retraining a pre-trained model that has undergone compression (through techniques like quantization, pruning, or distillation) to restore or even enhance its performance on a specific task or dataset. Fine-tuning is crucial because compression techniques often introduce a loss of accuracy or performance, and fine-tuning helps mitigate these effects by adapting the model to the specific nuances of the task at hand.</p> <h3 id=how-fine-tuning-works><strong>How Fine-Tuning Works:</strong><a class=headerlink href=#how-fine-tuning-works title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Initial Training:</strong> Before fine-tuning, the LLM is first trained on a large dataset to learn general language patterns. This model is then compressed using one or more compression techniques to reduce its size, computational requirements, or both.</p> </li> <li> <p><strong>Fine-Tuning on a Target Task:</strong> After compression, the model is further trained (or fine-tuned) on a smaller, task-specific dataset. This fine-tuning process typically involves adjusting the model's parameters with a lower learning rate, which allows the model to adapt to the new task without forgetting the general knowledge it acquired during the initial training.</p> </li> <li> <p><strong>Training with Augmented Data:</strong> Sometimes, the fine-tuning process includes augmenting the training data with additional examples or variations to help the model generalize better to the task. This is especially important when the compression process has caused a reduction in the model's ability to generalize.</p> </li> <li> <p><strong>Layer-Wise Fine-Tuning:</strong> In some cases, only certain layers of the model are fine-tuned, especially if the model has been pruned or compressed in a way that affects specific layers. This approach can help in retaining the benefits of compression while enhancing the model’s performance on the target task.</p> </li> </ul> <h3 id=benefits-of-fine-tuning-in-compressed-llms><strong>Benefits of Fine-Tuning in Compressed LLMs:</strong><a class=headerlink href=#benefits-of-fine-tuning-in-compressed-llms title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Restoring Accuracy:</strong> Fine-tuning can help recover some of the accuracy lost during compression, ensuring that the model performs well on the specific task for which it is intended.</p> </li> <li> <p><strong>Task-Specific Optimization:</strong> By fine-tuning on a task-specific dataset, the model becomes better at the particular task, even if the original, uncompressed model was trained on a broad range of language tasks.</p> </li> <li> <p><strong>Increased Efficiency:</strong> Fine-tuning allows the use of a smaller, more efficient model that still meets the performance requirements for a given application, making it possible to deploy the model on resource-constrained devices.</p> </li> </ul> <h3 id=challenges><strong>Challenges:</strong><a class=headerlink href=#challenges title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Risk of Overfitting:</strong> Fine-tuning on a small dataset can lead to overfitting, where the model becomes too specialized to the training data and fails to generalize to new examples.</p> </li> <li> <p><strong>Computational Cost:</strong> While fine-tuning a compressed model is generally less computationally intensive than training from scratch, it still requires significant computational resources, especially for very large models.</p> </li> <li> <p><strong>Balancing Generalization and Specialization:</strong> The fine-tuning process needs to balance retaining the general language understanding learned during pre-training and adapting to the specific task. If not done carefully, the model might lose its ability to generalize across different tasks.</p> </li> </ul> <h2 id=6-what-role-does-hardware-play-in-llm-compression>6. <strong>What Role Does Hardware Play in LLM Compression?</strong><a class=headerlink href=#6-what-role-does-hardware-play-in-llm-compression title="Permanent link">&para;</a></h2> <p>The effectiveness of LLM compression is closely tied to the underlying hardware on which the model is trained, fine-tuned, and deployed. Different hardware architectures have varying capabilities and limitations, which can significantly impact the performance and efficiency of compressed models.</p> <h3 id=key-hardware-considerations-in-llm-compression><strong>Key Hardware Considerations in LLM Compression:</strong><a class=headerlink href=#key-hardware-considerations-in-llm-compression title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Support for Low-Precision Arithmetic:</strong> Hardware that supports low-precision arithmetic, such as 8-bit or 16-bit operations, is crucial for effectively utilizing quantized models. GPUs (like those from NVIDIA) and specialized AI accelerators (such as Google’s TPU) often have native support for low-precision operations, allowing for faster computation and reduced memory usage.</p> </li> <li> <p><strong>Memory Bandwidth and Cache Size:</strong> Compressed models require less memory, but the efficiency of memory access is still critical. Hardware with high memory bandwidth and large cache sizes can better handle the data access patterns of compressed models, particularly when dealing with sparse matrices from pruned models or shared weights.</p> </li> <li> <p><strong>Support for Sparse Computations:</strong> Unstructured pruning often results in sparse matrices, which can be inefficient to process on hardware not optimized for sparse operations. Specialized hardware, such as certain AI accelerators, can take advantage of sparsity to perform computations more quickly and with lower power consumption.</p> </li> <li> <p><strong>Parallel Processing Capabilities:</strong> The ability to process multiple operations in parallel is important for speeding up the inference of compressed models. Hardware with a high degree of parallelism, such as GPUs or multi-core CPUs, can better handle the reduced computational complexity of compressed models.</p> </li> <li> <p><strong>Energy Efficiency:</strong> For deployment on mobile or edge devices, energy efficiency is a critical factor. Hardware that can perform low-power, high-speed computations will benefit most from model compression, as the reduced model size and complexity align well with the limited power budgets of such devices.</p> </li> </ul> <h3 id=impact-of-hardware-on-compression-techniques><strong>Impact of Hardware on Compression Techniques:</strong><a class=headerlink href=#impact-of-hardware-on-compression-techniques title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Quantization:</strong> The success of quantization depends heavily on the hardware’s ability to efficiently process low-precision arithmetic. Devices like NVIDIA GPUs and Google TPUs are designed to handle 16-bit and 8-bit computations, making them ideal for running quantized models.</p> </li> <li> <p><strong>Pruning:</strong> The efficiency gains from pruning are most pronounced when the hardware can exploit the resulting sparsity. CPUs with SIMD (Single Instruction, Multiple Data) extensions or GPUs with support for sparse matrix operations can accelerate the inference of pruned models.</p> </li> <li> <p><strong>Knowledge Distillation:</strong> The distillation process itself may not require specialized hardware, but the deployment of the distilled model benefits from hardware that can efficiently handle the reduced complexity of the student model.</p> </li> </ul> <h3 id=challenges_1><strong>Challenges:</strong><a class=headerlink href=#challenges_1 title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Hardware Compatibility:</strong> Not all hardware platforms support the same range of compression techniques. For example, older CPUs might not efficiently handle low-precision arithmetic, limiting the benefits of quantization.</p> </li> <li> <p><strong>Deployment Flexibility:</strong> Compressed models may be optimized for specific hardware, which can limit the ability to deploy the model across different devices or platforms without additional modifications or optimizations.</p> </li> <li> <p><strong>Cost Considerations:</strong> High-performance hardware that can fully exploit compressed models (such as GPUs or TPUs) may be costly, which could offset some of the savings achieved through compression in terms of deployment.</p> </li> </ul> <h2 id=7-what-are-the-ethical-considerations-in-llm-compression>7. <strong>What are the Ethical Considerations in LLM Compression?</strong><a class=headerlink href=#7-what-are-the-ethical-considerations-in-llm-compression title="Permanent link">&para;</a></h2> <p>The compression of large language models (LLMs) raises several ethical considerations that must be addressed to ensure that these technologies are deployed responsibly and fairly. While compression offers clear benefits in terms of efficiency and accessibility, it also introduces potential risks and challenges that could have ethical implications.</p> <h3 id=key-ethical-considerations-in-llm-compression><strong>Key Ethical Considerations in LLM Compression:</strong><a class=headerlink href=#key-ethical-considerations-in-llm-compression title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Bias Amplification:</strong> Compression techniques, particularly those that reduce the complexity of a model, might inadvertently amplify biases present in the original model. If a model is compressed too aggressively, it may lose some of its ability to balance different perspectives, leading to more biased or less accurate predictions, especially for underrepresented groups.</p> </li> <li> <p><strong>Fairness and Representation:</strong> Ensuring that compressed models perform equitably across different demographics and use cases is critical. If a model is fine-tuned or compressed using data that is not representative of the broader population, it may fail to perform well for certain groups, raising concerns about fairness and inclusivity.</p> </li> <li> <p><strong>Transparency and Accountability:</strong> The process of compressing a model often involves complex techniques that can obscure the model’s decision-making process. This lack of transparency can make it more difficult to hold AI systems accountable for their outputs, particularly when they are deployed in sensitive areas such as healthcare, finance, or criminal justice.</p> </li> <li> <p><strong>Accessibility vs. Power Concentration:</strong> While compression makes powerful AI models more accessible by reducing the hardware requirements needed for deployment, it can also concentrate power in the hands of those who control the most advanced compression techniques. This could lead to a situation where only a few organizations or entities have the ability to deploy highly efficient, compressed models at scale.</p> </li> <li> <p><strong>Environmental Impact:</strong> Compressing models can lead to more energy-efficient deployments, which is beneficial for reducing the carbon footprint of AI systems. However, the process of developing and fine-tuning compressed models can still be resource-intensive, and the overall environmental impact should be considered.</p> </li> </ul> <h3 id=mitigating-ethical-risks><strong>Mitigating Ethical Risks:</strong><a class=headerlink href=#mitigating-ethical-risks title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Bias Mitigation:</strong> When compressing models, it is important to actively monitor and mitigate any potential biases that may be introduced or amplified. This can be done by using diverse and representative datasets during the compression process and by regularly auditing the compressed models for fairness.</p> </li> <li> <p><strong>Transparency in Compression Techniques:</strong> Developers should strive to maintain transparency in the compression process, providing clear documentation on how models have been compressed and the trade-offs that were made. This transparency is crucial for building trust and ensuring accountability.</p> </li> <li> <p><strong>Inclusive Design:</strong> Ensuring that compressed models are tested and validated across a wide range of use cases and demographic groups can help prevent unintended consequences. This includes fine-tuning and evaluating models on datasets that reflect the diversity of the populations they will serve.</p> </li> <li> <p><strong>Environmental Considerations:</strong> Developers should consider the environmental impact of the entire model lifecycle, including the energy costs associated with training, compressing, and deploying models. Where possible, efforts should be made to minimize these impacts, such as by using energy-efficient hardware and renewable energy sources.</p> </li> </ul> <h2 id=8-what-are-the-future-directions-in-llm-compression>8. <strong>What are the Future Directions in LLM Compression?</strong><a class=headerlink href=#8-what-are-the-future-directions-in-llm-compression title="Permanent link">&para;</a></h2> <p>The field of LLM compression is rapidly evolving, with ongoing research and development focused on pushing the boundaries of what can be achieved in terms of model efficiency, performance, and accessibility. Several future directions in LLM compression hold promise for advancing the state of the art and addressing the challenges that currently exist. Researchers are continually developing new and more sophisticated algorithms for model compression. These may include hybrid techniques that combine multiple approaches (e.g., quantization with pruning) to achieve even greater reductions in model size and computational requirements without</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>