<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/State-of-the-Art-Computer-Vision-Models.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>State of the Art Image Generation Models in Computer Vision: A Comprehensive Overview - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#state-of-the-art-computer-vision-models class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> State of the Art Image Generation Models in Computer Vision: A Comprehensive Overview </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-are-the-different-methods-of-image-generation class=md-nav__link> <span class=md-ellipsis> What are the different methods of Image generation? </span> </a> <nav class=md-nav aria-label="What are the different methods of Image generation?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-diffusion-models class=md-nav__link> <span class=md-ellipsis> 1. Diffusion Models </span> </a> <nav class=md-nav aria-label="1. Diffusion Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-steps-in-diffusion-models class=md-nav__link> <span class=md-ellipsis> Key Steps in Diffusion Models: </span> </a> </li> <li class=md-nav__item> <a href=#variants-of-diffusion-models class=md-nav__link> <span class=md-ellipsis> Variants of Diffusion Models: </span> </a> </li> <li class=md-nav__item> <a href=#advantages class=md-nav__link> <span class=md-ellipsis> Advantages: </span> </a> </li> <li class=md-nav__item> <a href=#examples-models class=md-nav__link> <span class=md-ellipsis> Examples Models: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-generative-adversarial-networks-gans class=md-nav__link> <span class=md-ellipsis> 2. Generative Adversarial Networks (GANs) </span> </a> <nav class=md-nav aria-label="2. Generative Adversarial Networks (GANs)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-steps-in-gans class=md-nav__link> <span class=md-ellipsis> Key Steps in GANs: </span> </a> </li> <li class=md-nav__item> <a href=#advantages_1 class=md-nav__link> <span class=md-ellipsis> Advantages: </span> </a> </li> <li class=md-nav__item> <a href=#challenges class=md-nav__link> <span class=md-ellipsis> Challenges: </span> </a> </li> <li class=md-nav__item> <a href=#examples class=md-nav__link> <span class=md-ellipsis> Examples: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3-variational-autoencoders-vaes class=md-nav__link> <span class=md-ellipsis> 3. Variational Autoencoders (VAEs) </span> </a> <nav class=md-nav aria-label="3. Variational Autoencoders (VAEs)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-steps-in-vaes class=md-nav__link> <span class=md-ellipsis> Key Steps in VAEs: </span> </a> </li> <li class=md-nav__item> <a href=#advantages_2 class=md-nav__link> <span class=md-ellipsis> Advantages: </span> </a> </li> <li class=md-nav__item> <a href=#challenges_1 class=md-nav__link> <span class=md-ellipsis> Challenges: </span> </a> </li> <li class=md-nav__item> <a href=#examples_1 class=md-nav__link> <span class=md-ellipsis> Examples: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-autoregressive-models class=md-nav__link> <span class=md-ellipsis> 4. Autoregressive Models </span> </a> <nav class=md-nav aria-label="4. Autoregressive Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-steps-in-autoregressive-models class=md-nav__link> <span class=md-ellipsis> Key Steps in Autoregressive Models: </span> </a> </li> <li class=md-nav__item> <a href=#advantages_3 class=md-nav__link> <span class=md-ellipsis> Advantages: </span> </a> </li> <li class=md-nav__item> <a href=#challenges_2 class=md-nav__link> <span class=md-ellipsis> Challenges: </span> </a> </li> <li class=md-nav__item> <a href=#examples_2 class=md-nav__link> <span class=md-ellipsis> Examples: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#5-flow-based-models class=md-nav__link> <span class=md-ellipsis> 5. Flow-Based Models </span> </a> <nav class=md-nav aria-label="5. Flow-Based Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-steps-in-flow-based-models class=md-nav__link> <span class=md-ellipsis> Key Steps in Flow-Based Models: </span> </a> </li> <li class=md-nav__item> <a href=#advantages_4 class=md-nav__link> <span class=md-ellipsis> Advantages: </span> </a> </li> <li class=md-nav__item> <a href=#challenges_3 class=md-nav__link> <span class=md-ellipsis> Challenges: </span> </a> </li> <li class=md-nav__item> <a href=#examples_3 class=md-nav__link> <span class=md-ellipsis> Examples: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#6-neural-radiance-fields-nerf class=md-nav__link> <span class=md-ellipsis> 6. Neural Radiance Fields (NeRF) </span> </a> <nav class=md-nav aria-label="6. Neural Radiance Fields (NeRF)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-steps-in-nerf-neural-radiance-fields class=md-nav__link> <span class=md-ellipsis> Key Steps in NeRF (Neural Radiance Fields) </span> </a> </li> <li class=md-nav__item> <a href=#scene-representation-implicit-3d-model class=md-nav__link> <span class=md-ellipsis> Scene Representation (Implicit 3D Model) </span> </a> </li> <li class=md-nav__item> <a href=#neural-network-prediction-radiance-field-estimation class=md-nav__link> <span class=md-ellipsis> Neural Network Prediction (Radiance Field Estimation) </span> </a> </li> <li class=md-nav__item> <a href=#volume-rendering-synthesizing-2d-images class=md-nav__link> <span class=md-ellipsis> Volume Rendering (Synthesizing 2D Images) </span> </a> </li> <li class=md-nav__item> <a href=#advantages_5 class=md-nav__link> <span class=md-ellipsis> Advantages: </span> </a> </li> <li class=md-nav__item> <a href=#challenges_4 class=md-nav__link> <span class=md-ellipsis> Challenges: </span> </a> </li> <li class=md-nav__item> <a href=#examples_4 class=md-nav__link> <span class=md-ellipsis> Examples: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#7-transformer-based-models class=md-nav__link> <span class=md-ellipsis> 7. Transformer-Based Models </span> </a> <nav class=md-nav aria-label="7. Transformer-Based Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-steps-in-transformer-based-models class=md-nav__link> <span class=md-ellipsis> Key Steps in Transformer-Based Models: </span> </a> </li> <li class=md-nav__item> <a href=#advantages_6 class=md-nav__link> <span class=md-ellipsis> Advantages: </span> </a> </li> <li class=md-nav__item> <a href=#challenges_5 class=md-nav__link> <span class=md-ellipsis> Challenges: </span> </a> </li> <li class=md-nav__item> <a href=#examples_5 class=md-nav__link> <span class=md-ellipsis> Examples: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#summary-of-methods class=md-nav__link> <span class=md-ellipsis> Summary of Methods: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-the-state-of-the-art-models-for-image-generation class=md-nav__link> <span class=md-ellipsis> What are the state-of-the-art models for image generation? </span> </a> <nav class=md-nav aria-label="What are the state-of-the-art models for image generation?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-flux1-and-ideogram20 class=md-nav__link> <span class=md-ellipsis> 1. FLUX.1 and Ideogram2.0 </span> </a> </li> <li class=md-nav__item> <a href=#2-dalle-3-hd class=md-nav__link> <span class=md-ellipsis> 2. DALL·E 3 HD </span> </a> </li> <li class=md-nav__item> <a href=#3-stable-diffusion-xl-base-10-sdxl class=md-nav__link> <span class=md-ellipsis> 3. Stable Diffusion XL Base 1.0 (SDXL) </span> </a> </li> <li class=md-nav__item> <a href=#4-imagen-3 class=md-nav__link> <span class=md-ellipsis> 4. Imagen 3 </span> </a> </li> <li class=md-nav__item> <a href=#5-midjourney-v61 class=md-nav__link> <span class=md-ellipsis> 5. Midjourney v6.1 </span> </a> </li> <li class=md-nav__item> <a href=#6-frecas-frequency-aware-cascaded-sampling class=md-nav__link> <span class=md-ellipsis> 6. FreCaS (Frequency-aware Cascaded Sampling) </span> </a> </li> <li class=md-nav__item> <a href=#7-controlar class=md-nav__link> <span class=md-ellipsis> 7. ControlAR </span> </a> </li> <li class=md-nav__item> <a href=#8-qlip-quantized-language-image-pretraining class=md-nav__link> <span class=md-ellipsis> 8. QLIP (Quantized Language-Image Pretraining) </span> </a> </li> <li class=md-nav__item> <a href=#9-recraft-v3 class=md-nav__link> <span class=md-ellipsis> 9. Recraft V3 </span> </a> </li> <li class=md-nav__item> <a href=#10-luma-photon-flash class=md-nav__link> <span class=md-ellipsis> 10. Luma Photon Flash </span> </a> </li> <li class=md-nav__item> <a href=#11-playground-v3-beta class=md-nav__link> <span class=md-ellipsis> 11. Playground v3 (Beta) </span> </a> </li> <li class=md-nav__item> <a href=#12-deepseek-janus class=md-nav__link> <span class=md-ellipsis> 12. DeepSeek Janus </span> </a> </li> <li class=md-nav__item> <a href=#13-omnigen class=md-nav__link> <span class=md-ellipsis> 13. OmniGen </span> </a> </li> <li class=md-nav__item> <a href=#14-gen2-by-runway class=md-nav__link> <span class=md-ellipsis> 14. Gen2 by Runway </span> </a> </li> <li class=md-nav__item> <a href=#15-dreamlike-photoreal-20 class=md-nav__link> <span class=md-ellipsis> 15. Dreamlike-photoreal-2.0 </span> </a> </li> <li class=md-nav__item> <a href=#summary-table-of-sota-image-generation-models class=md-nav__link> <span class=md-ellipsis> Summary Table of SOTA Image generation models </span> </a> </li> <li class=md-nav__item> <a href=#state-of-the-art-image-generation-models class=md-nav__link> <span class=md-ellipsis> State-of-the-Art Image Generation Models </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-the-state-of-the-art-video-generation-models class=md-nav__link> <span class=md-ellipsis> What are the state-of-the-art Video generation models? </span> </a> <nav class=md-nav aria-label="What are the state-of-the-art Video generation models?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-gen-2-by-runway class=md-nav__link> <span class=md-ellipsis> 1. Gen-2 by Runway </span> </a> </li> <li class=md-nav__item> <a href=#2-sora-by-openai class=md-nav__link> <span class=md-ellipsis> 2. Sora by OpenAI </span> </a> </li> <li class=md-nav__item> <a href=#3-phenaki-by-google-research class=md-nav__link> <span class=md-ellipsis> 3. Phenaki by Google Research </span> </a> </li> <li class=md-nav__item> <a href=#4-imagen-video-by-google-research class=md-nav__link> <span class=md-ellipsis> 4. Imagen Video by Google Research </span> </a> </li> <li class=md-nav__item> <a href=#5-make-a-video-by-meta-facebook-ai class=md-nav__link> <span class=md-ellipsis> 5. Make-A-Video by Meta (Facebook AI) </span> </a> </li> <li class=md-nav__item> <a href=#6-cogvideo-by-tsinghua-university-and-modelbest class=md-nav__link> <span class=md-ellipsis> 6. CogVideo by Tsinghua University and ModelBest </span> </a> </li> <li class=md-nav__item> <a href=#7-video-ldm-latent-diffusion-model class=md-nav__link> <span class=md-ellipsis> 7. Video LDM (Latent Diffusion Model) </span> </a> </li> <li class=md-nav__item> <a href=#8-nuwa-by-microsoft-research-asia class=md-nav__link> <span class=md-ellipsis> 8. NUWA by Microsoft Research Asia </span> </a> </li> <li class=md-nav__item> <a href=#9-t2v-zero-text-to-video-zero-shot class=md-nav__link> <span class=md-ellipsis> 9. T2V-Zero (Text-to-Video Zero-Shot) </span> </a> </li> <li class=md-nav__item> <a href=#10-videogpt-by-openai class=md-nav__link> <span class=md-ellipsis> 10. VideoGPT by OpenAI </span> </a> </li> <li class=md-nav__item> <a href=#11-digan-diverse-image-and-video-generation-via-adversarial-networks class=md-nav__link> <span class=md-ellipsis> 11. DIGAN (Diverse Image and Video Generation via Adversarial Networks) </span> </a> </li> <li class=md-nav__item> <a href=#12-video-diffusion-models class=md-nav__link> <span class=md-ellipsis> 12. Video Diffusion Models </span> </a> </li> <li class=md-nav__item> <a href=#13-text2video-zero class=md-nav__link> <span class=md-ellipsis> 13. Text2Video-Zero </span> </a> </li> <li class=md-nav__item> <a href=#14-videopoet-by-google-research class=md-nav__link> <span class=md-ellipsis> 14. VideoPoet by Google Research </span> </a> </li> <li class=md-nav__item> <a href=#15-magicvideo-by-bytedance class=md-nav__link> <span class=md-ellipsis> 15. MagicVideo by ByteDance </span> </a> </li> <li class=md-nav__item> <a href=#summary-of-sota-video-generation-models class=md-nav__link> <span class=md-ellipsis> Summary of SOTA Video Generation Models </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-state-of-the-art-sota-3d-image-generation-models class=md-nav__link> <span class=md-ellipsis> What are State-of-the-Art (SOTA) 3D Image Generation Models? </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/State-of-the-Art-Computer-Vision-Models.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/State-of-the-Art-Computer-Vision-Models.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="State-of-the-Art 3D Image Generation Models" src=../assets/images/dspost/dsp6219-State-of-the-Art-Computer-Vision-Models.jpg></p> <h1 id=state-of-the-art-computer-vision-models>State of the Art Computer Vision Models<a class=headerlink href=#state-of-the-art-computer-vision-models title="Permanent link">&para;</a></h1> <h2 id=what-are-the-different-methods-of-image-generation>What are the different methods of Image generation?<a class=headerlink href=#what-are-the-different-methods-of-image-generation title="Permanent link">&para;</a></h2> <p>There are several methods for image generation. Diffusion models are currently the state-of-the-art due to their balance of quality, flexibility, and scalability. However, other methods like GANs and autoregressive models remain relevant for specific use cases. Let's see them one by one.</p> <h3 id=1-diffusion-models><strong>1. Diffusion Models</strong><a class=headerlink href=#1-diffusion-models title="Permanent link">&para;</a></h3> <p>Diffusion models are a class of generative models that work by gradually adding noise to data (e.g., images) and then learning to reverse this process to generate new data. Here's how it works:</p> <h4 id=key-steps-in-diffusion-models><strong>Key Steps in Diffusion Models:</strong><a class=headerlink href=#key-steps-in-diffusion-models title="Permanent link">&para;</a></h4> <p><img alt="Diffusion Model Steps" src=../assets/images/dspost/mermaid-code/dsp6219-Diffusion-Diagram.jpg></p> <h4 id=variants-of-diffusion-models><strong>Variants of Diffusion Models</strong>:<a class=headerlink href=#variants-of-diffusion-models title="Permanent link">&para;</a></h4> <ul> <li><strong>Denoising Diffusion Probabilistic Models (DDPM)</strong>: The original formulation of diffusion models.</li> <li><strong>Latent Diffusion Models (LDM)</strong>: Operate in a lower-dimensional latent space for efficiency (e.g., Stable Diffusion).</li> <li><strong>Classifier-Free Guidance</strong>: Improves image quality by balancing conditional and unconditional generation.</li> <li><strong>Stochastic Differential Equations (SDEs)</strong>: A continuous-time formulation of diffusion models.</li> </ul> <h4 id=advantages><strong>Advantages</strong>:<a class=headerlink href=#advantages title="Permanent link">&para;</a></h4> <ul> <li>High-quality, photorealistic images.</li> <li>Flexible and controllable generation (e.g., text-to-image, image-to-image).</li> <li>Scalable to high resolutions.</li> </ul> <h4 id=examples-models><strong>Examples Models</strong>:<a class=headerlink href=#examples-models title="Permanent link">&para;</a></h4> <ul> <li>DALL·E 3, Stable Diffusion, Imagen, Midjourney.</li> </ul> <hr> <h3 id=2-generative-adversarial-networks-gans><strong>2. Generative Adversarial Networks (GANs)</strong><a class=headerlink href=#2-generative-adversarial-networks-gans title="Permanent link">&para;</a></h3> <p>GANs consist of two neural networks: a <strong>generator</strong> and a <strong>discriminator</strong>, which compete against each other.</p> <h4 id=key-steps-in-gans><strong>Key Steps in GANs</strong>:<a class=headerlink href=#key-steps-in-gans title="Permanent link">&para;</a></h4> <p><img alt="GAN Model Steps" src=../assets/images/dspost/mermaid-code/dsp6219-GAN-Diagram.jpg></p> <h4 id=advantages_1><strong>Advantages</strong>:<a class=headerlink href=#advantages_1 title="Permanent link">&para;</a></h4> <ul> <li>Fast image generation once trained.</li> <li>High-quality results for specific tasks (e.g., faces, landscapes).</li> </ul> <h4 id=challenges><strong>Challenges</strong>:<a class=headerlink href=#challenges title="Permanent link">&para;</a></h4> <ul> <li>Training instability (mode collapse).</li> <li>Limited diversity in generated images.</li> </ul> <h4 id=examples><strong>Examples</strong>:<a class=headerlink href=#examples title="Permanent link">&para;</a></h4> <ul> <li>StyleGAN, BigGAN, CycleGAN.</li> </ul> <hr> <h3 id=3-variational-autoencoders-vaes><strong>3. Variational Autoencoders (VAEs)</strong><a class=headerlink href=#3-variational-autoencoders-vaes title="Permanent link">&para;</a></h3> <p>VAEs are probabilistic models that learn a latent representation of data.</p> <h4 id=key-steps-in-vaes><strong>Key Steps in VAEs</strong>:<a class=headerlink href=#key-steps-in-vaes title="Permanent link">&para;</a></h4> <p><img alt="VAE Diagram" src=../assets/images/dspost/mermaid-code/dsp6219-VAE-Diagram.jpg></p> <h4 id=advantages_2><strong>Advantages</strong>:<a class=headerlink href=#advantages_2 title="Permanent link">&para;</a></h4> <ul> <li>Smooth latent space for interpolation.</li> <li>Good for tasks requiring structured outputs.</li> </ul> <h4 id=challenges_1><strong>Challenges</strong>:<a class=headerlink href=#challenges_1 title="Permanent link">&para;</a></h4> <ul> <li>Generated images are often blurrier compared to GANs or diffusion models.</li> </ul> <h4 id=examples_1><strong>Examples</strong>:<a class=headerlink href=#examples_1 title="Permanent link">&para;</a></h4> <ul> <li>VQ-VAE, NVAE.</li> </ul> <hr> <h3 id=4-autoregressive-models><strong>4. Autoregressive Models</strong><a class=headerlink href=#4-autoregressive-models title="Permanent link">&para;</a></h3> <p>Autoregressive models generate images pixel by pixel or patch by patch, conditioned on previously generated pixels.</p> <h4 id=key-steps-in-autoregressive-models><strong>Key Steps in Autoregressive Models</strong>:<a class=headerlink href=#key-steps-in-autoregressive-models title="Permanent link">&para;</a></h4> <ol> <li>Treat image generation as a sequence prediction problem.</li> <li>Use models like Transformers or RNNs to predict the next pixel or patch.</li> </ol> <h4 id=advantages_3><strong>Advantages</strong>:<a class=headerlink href=#advantages_3 title="Permanent link">&para;</a></h4> <ul> <li>High-quality, detailed images.</li> <li>Flexible and scalable.</li> </ul> <h4 id=challenges_2><strong>Challenges</strong>:<a class=headerlink href=#challenges_2 title="Permanent link">&para;</a></h4> <ul> <li>Slow generation due to sequential nature.</li> <li>Computationally expensive.</li> </ul> <h4 id=examples_2><strong>Examples</strong>:<a class=headerlink href=#examples_2 title="Permanent link">&para;</a></h4> <ul> <li>PixelRNN, PixelCNN, Image GPT.</li> </ul> <hr> <h3 id=5-flow-based-models><strong>5. Flow-Based Models</strong><a class=headerlink href=#5-flow-based-models title="Permanent link">&para;</a></h3> <p>Flow-based models use invertible transformations to map data to a latent space and back.</p> <h4 id=key-steps-in-flow-based-models><strong>Key Steps in Flow-Based Models</strong>:<a class=headerlink href=#key-steps-in-flow-based-models title="Permanent link">&para;</a></h4> <ol> <li>Learn a bijective (invertible) mapping between the data distribution and a simple latent distribution (e.g., Gaussian).</li> <li>Generate new images by sampling from the latent distribution and applying the inverse transformation.</li> </ol> <h4 id=advantages_4><strong>Advantages</strong>:<a class=headerlink href=#advantages_4 title="Permanent link">&para;</a></h4> <ul> <li>Exact likelihood estimation.</li> <li>Efficient sampling.</li> </ul> <h4 id=challenges_3><strong>Challenges</strong>:<a class=headerlink href=#challenges_3 title="Permanent link">&para;</a></h4> <ul> <li>Limited flexibility in architecture due to invertibility constraints.</li> </ul> <h4 id=examples_3><strong>Examples</strong>:<a class=headerlink href=#examples_3 title="Permanent link">&para;</a></h4> <ul> <li>Glow, RealNVP.</li> </ul> <hr> <h3 id=6-neural-radiance-fields-nerf><strong>6. Neural Radiance Fields (NeRF)</strong><a class=headerlink href=#6-neural-radiance-fields-nerf title="Permanent link">&para;</a></h3> <p>NeRF is a method for 3D scene reconstruction and generation.</p> <h4 id=key-steps-in-nerf-neural-radiance-fields><strong>Key Steps in NeRF (Neural Radiance Fields)</strong><a class=headerlink href=#key-steps-in-nerf-neural-radiance-fields title="Permanent link">&para;</a></h4> <p><img alt src=../assets/images/dspost/mermaid-code/dsp6219-NeRF-Diagram.jpg></p> <h4 id=scene-representation-implicit-3d-model><strong>Scene Representation (Implicit 3D Model)</strong><a class=headerlink href=#scene-representation-implicit-3d-model title="Permanent link">&para;</a></h4> <ul> <li>The 3D scene is represented as a <strong>continuous volumetric function</strong>. </li> <li>Instead of using meshes or point clouds, NeRF models a scene as a <strong>neural network</strong> mapping 3D coordinates to color and density. </li> <li><strong>Input:</strong> <ul> <li>A set of <strong>2D images</strong> of a scene taken from different angles. </li> <li>The <strong>camera parameters</strong> (position &amp; direction) for each image. </li> </ul> </li> </ul> <h4 id=neural-network-prediction-radiance-field-estimation><strong>Neural Network Prediction (Radiance Field Estimation)</strong><a class=headerlink href=#neural-network-prediction-radiance-field-estimation title="Permanent link">&para;</a></h4> <ul> <li>A deep neural network takes in <strong>a 3D point (x, y, z) and a viewing direction (θ, φ)</strong>. </li> <li>It predicts: <ul> <li><strong>Color (R, G, B)</strong> → Light emitted from that point. </li> <li><strong>Density (σ)</strong> → How much light is absorbed at that point. </li> </ul> </li> <li>This allows NeRF to model the appearance of a scene <strong>from any viewpoint</strong>.</li> </ul> <h4 id=volume-rendering-synthesizing-2d-images><strong>Volume Rendering (Synthesizing 2D Images)</strong><a class=headerlink href=#volume-rendering-synthesizing-2d-images title="Permanent link">&para;</a></h4> <ul> <li>To generate an image, NeRF <strong>traces rays</strong> through the scene from the camera viewpoint. </li> <li>For each ray:<ul> <li><strong>Samples multiple points</strong> along the ray in 3D space.</li> <li>Uses the neural network to get <strong>color and density</strong> at each sampled point.</li> <li><strong>Combines these values using volume rendering equations</strong> to compute the final pixel color. </li> </ul> </li> <li>This step <strong>synthesizes realistic 2D images from new viewpoints</strong>.</li> </ul> <hr> <h4 id=advantages_5><strong>Advantages</strong>:<a class=headerlink href=#advantages_5 title="Permanent link">&para;</a></h4> <ul> <li>High-quality 3D-aware image generation.</li> <li>Useful for tasks like view synthesis.</li> </ul> <h4 id=challenges_4><strong>Challenges</strong>:<a class=headerlink href=#challenges_4 title="Permanent link">&para;</a></h4> <ul> <li>Computationally intensive.</li> <li>Limited to 3D scenes.</li> </ul> <h4 id=examples_4><strong>Examples</strong>:<a class=headerlink href=#examples_4 title="Permanent link">&para;</a></h4> <ul> <li>GIRAFFE, DreamFusion.</li> </ul> <hr> <h3 id=7-transformer-based-models><strong>7. Transformer-Based Models</strong><a class=headerlink href=#7-transformer-based-models title="Permanent link">&para;</a></h3> <p>Transformers, originally designed for NLP, are now used for image generation.</p> <h4 id=key-steps-in-transformer-based-models><strong>Key Steps in Transformer-Based Models</strong>:<a class=headerlink href=#key-steps-in-transformer-based-models title="Permanent link">&para;</a></h4> <ol> <li>Treat images as sequences of patches or tokens.</li> <li>Use self-attention mechanisms to model relationships between patches.</li> <li>Generate images autoregressively or in parallel.</li> </ol> <h4 id=advantages_6><strong>Advantages</strong>:<a class=headerlink href=#advantages_6 title="Permanent link">&para;</a></h4> <ul> <li>Scalable to large datasets.</li> <li>High-quality results with sufficient compute.</li> </ul> <h4 id=challenges_5><strong>Challenges</strong>:<a class=headerlink href=#challenges_5 title="Permanent link">&para;</a></h4> <ul> <li>High computational cost.</li> <li>Requires large datasets.</li> </ul> <h4 id=examples_5><strong>Examples</strong>:<a class=headerlink href=#examples_5 title="Permanent link">&para;</a></h4> <ul> <li>DALL·E, Image GPT, Pathways Autoregressive Text-to-Image mode (Parti).</li> </ul> <hr> <h3 id=summary-of-methods><strong>Summary of Methods</strong>:<a class=headerlink href=#summary-of-methods title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th><strong>Method</strong></th> <th><strong>Key Idea</strong></th> <th><strong>Strengths</strong></th> <th><strong>Weaknesses</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Diffusion Models</strong></td> <td>Gradually denoise random noise into images.</td> <td>High quality, flexible, scalable.</td> <td>Computationally expensive.</td> </tr> <tr> <td><strong>GANs</strong></td> <td>Adversarial training between generator and discriminator.</td> <td>Fast generation, high quality.</td> <td>Training instability, limited diversity.</td> </tr> <tr> <td><strong>VAEs</strong></td> <td>Learn latent representations and decode them into images.</td> <td>Smooth latent space, structured outputs.</td> <td>Blurry images.</td> </tr> <tr> <td><strong>Autoregressive</strong></td> <td>Generate images pixel-by-pixel or patch-by-patch.</td> <td>High detail, flexible.</td> <td>Slow generation, expensive.</td> </tr> <tr> <td><strong>Flow-Based</strong></td> <td>Use invertible transformations to map data to latent space.</td> <td>Exact likelihood, efficient sampling.</td> <td>Limited flexibility.</td> </tr> <tr> <td><strong>NeRF</strong></td> <td>Represent 3D scenes as volumetric functions.</td> <td>High-quality 3D-aware generation.</td> <td>Computationally intensive.</td> </tr> <tr> <td><strong>Transformers</strong></td> <td>Treat images as sequences of patches and use self-attention.</td> <td>Scalable, high quality.</td> <td>High compute and data requirements.</td> </tr> </tbody> </table> <hr> <h2 id=what-are-the-state-of-the-art-models-for-image-generation>What are the state-of-the-art models for image generation?<a class=headerlink href=#what-are-the-state-of-the-art-models-for-image-generation title="Permanent link">&para;</a></h2> <h3 id=1-flux1-and-ideogram20><strong>1. FLUX.1 and Ideogram2.0</strong><a class=headerlink href=#1-flux1-and-ideogram20 title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Diffusion models with advanced prompt-following capabilities.</li> <li><strong>Description</strong>: These models excel in structured output generation, realism, and physical consistency. They are capable of generating high-quality images from text prompts and are considered leading models in the field.</li> </ul> <hr> <h3 id=2-dalle-3-hd><strong>2. DALL·E 3 HD</strong><a class=headerlink href=#2-dalle-3-hd title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Diffusion models with enhanced text rendering and coherence.</li> <li><strong>Description</strong>: DALL·E 3 is known for its ability to generate detailed and coherent images from complex text descriptions. It incorporates a provenance classifier to identify AI-generated images.</li> </ul> <hr> <h3 id=3-stable-diffusion-xl-base-10-sdxl><strong>3. Stable Diffusion XL Base 1.0 (SDXL)</strong><a class=headerlink href=#3-stable-diffusion-xl-base-10-sdxl title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Latent Diffusion Models (LDM) with ensemble pipelines.</li> <li><strong>Description</strong>: SDXL generates high-resolution, diverse images with superior fidelity. It uses two pre-trained text encoders and a refinement model for enhanced detail and denoising.</li> </ul> <hr> <h3 id=4-imagen-3><strong>4. Imagen 3</strong><a class=headerlink href=#4-imagen-3 title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Diffusion models with SynthID watermarking.</li> <li><strong>Description</strong>: Imagen 3 produces photorealistic images with rich details and lighting. It includes a digital watermarking tool (SynthID) embedded directly into the image pixels.</li> </ul> <hr> <h3 id=5-midjourney-v61><strong>5. Midjourney v6.1</strong><a class=headerlink href=#5-midjourney-v61 title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Diffusion models with creative remix capabilities.</li> <li><strong>Description</strong>: Midjourney is renowned for its artistic style and ability to generate highly aesthetic, photorealistic images. It supports higher resolutions and offers upscaling options.</li> </ul> <hr> <h3 id=6-frecas-frequency-aware-cascaded-sampling><strong>6. FreCaS (Frequency-aware Cascaded Sampling)</strong><a class=headerlink href=#6-frecas-frequency-aware-cascaded-sampling title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Frequency-aware cascaded sampling for higher-resolution image generation.</li> <li><strong>Description</strong>: FreCaS decomposes the sampling process into stages with gradually increased resolutions, optimizing computational efficiency and image quality. It is particularly effective for generating 2048x2048 images.</li> </ul> <hr> <h3 id=7-controlar><strong>7. ControlAR</strong><a class=headerlink href=#7-controlar title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Autoregressive models with spatial control.</li> <li><strong>Description</strong>: ControlAR supports arbitrary-resolution image generation with spatial controls like depth maps, edge detection, and segmentation masks. It integrates DINOv2 encoders for enhanced control.</li> </ul> <hr> <h3 id=8-qlip-quantized-language-image-pretraining><strong>8. QLIP (Quantized Language-Image Pretraining)</strong><a class=headerlink href=#8-qlip-quantized-language-image-pretraining title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Binary-spherical-quantization-based autoencoder.</li> <li><strong>Description</strong>: QLIP unifies multimodal understanding and generation by combining reconstruction and language-image alignment objectives. It serves as a drop-in replacement for visual encoders in models like LLaVA and LlamaGen.</li> </ul> <hr> <h3 id=9-recraft-v3><strong>9. Recraft V3</strong><a class=headerlink href=#9-recraft-v3 title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Diffusion models with precise control over image attributes.</li> <li><strong>Description</strong>: Recraft V3 excels in generating images with extended text content and offers granular control over text size, positioning, and style. It is designed for professional designers.</li> </ul> <hr> <h3 id=10-luma-photon-flash><strong>10. Luma Photon Flash</strong><a class=headerlink href=#10-luma-photon-flash title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Diffusion models optimized for efficiency and quality.</li> <li><strong>Description</strong>: Luma Photon Flash is up to 10 times more efficient than other models, delivering high-quality and creative outputs. It supports multi-turn and iterative workflows.</li> </ul> <hr> <h3 id=11-playground-v3-beta><strong>11. Playground v3 (Beta)</strong><a class=headerlink href=#11-playground-v3-beta title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Diffusion models with deep prompt understanding.</li> <li><strong>Description</strong>: Playground v3 focuses on precise control over image generation, excelling in detailed prompts and text rendering. It integrates LLM and advanced VLM captioning for enhanced performance.</li> </ul> <hr> <h3 id=12-deepseek-janus><strong>12. DeepSeek Janus</strong><a class=headerlink href=#12-deepseek-janus title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Open-source diffusion models with multimodal understanding.</li> <li><strong>Description</strong>: DeepSeek Janus is a research-oriented model that generates detailed, structured imagery. It is popular among developers for its flexibility and customization options.</li> </ul> <hr> <h3 id=13-omnigen><strong>13. OmniGen</strong><a class=headerlink href=#13-omnigen title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Multimodal generative models.</li> <li><strong>Description</strong>: OmniGen integrates text, image, and audio data into a unified generative framework, eliminating the need for additional preprocessing steps like face detection or pose estimation.</li> </ul> <hr> <h3 id=14-gen2-by-runway><strong>14. Gen2 by Runway</strong><a class=headerlink href=#14-gen2-by-runway title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Text-to-video generation with multimodal input.</li> <li><strong>Description</strong>: While primarily a video generation tool, Gen2 can create high-quality images from text prompts and supports extensive customization, including reference images and audio.</li> </ul> <hr> <h3 id=15-dreamlike-photoreal-20><strong>15. Dreamlike-photoreal-2.0</strong><a class=headerlink href=#15-dreamlike-photoreal-20 title="Permanent link">&para;</a></h3> <ul> <li><strong>Method</strong>: Fine-tuned diffusion models.</li> <li><strong>Description</strong>: Specializing in photorealistic image generation, this model is derived from Stable Diffusion and is fine-tuned using user-contributed data.</li> </ul> <h3 id=summary-table-of-sota-image-generation-models>Summary Table of SOTA Image generation models<a class=headerlink href=#summary-table-of-sota-image-generation-models title="Permanent link">&para;</a></h3> <p>Certainly! Here's the updated table with the first column now containing the URLs of the official pages or research papers for each model:</p> <hr> <h3 id=state-of-the-art-image-generation-models><strong>State-of-the-Art Image Generation Models</strong><a class=headerlink href=#state-of-the-art-image-generation-models title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Developer</strong></th> <th><strong>Key Features</strong></th> <th><strong>Open-Source</strong></th> </tr> </thead> <tbody> <tr> <td><a href=https://blackforestlabs.ai/ ><strong>FLUX.1</strong></a></td> <td>Black Forest Labs</td> <td>Advanced text-to-image generation with high fidelity and photorealism.</td> <td>Yes</td> </tr> <tr> <td><a href=https://ideogram.ai/ ><strong>Ideogram 2.0</strong></a></td> <td>Ideogram</td> <td>Text-integrated image generation, excelling in rendering legible text within images.</td> <td>Yes</td> </tr> <tr> <td><a href=https://openai.com/product/dall-e-3><strong>DALL·E 3 HD</strong></a></td> <td>OpenAI</td> <td>High-definition image generation from textual descriptions.</td> <td>No</td> </tr> <tr> <td><a href=https://stability.ai/blog/stable-diffusion-xl><strong>Stable Diffusion XL Base 1.0 (SDXL)</strong></a></td> <td>Stability AI</td> <td>High-resolution image synthesis with improved detail and coherence.</td> <td>Yes</td> </tr> <tr> <td><a href=https://imagen.research.google/ ><strong>Imagen 3</strong></a></td> <td>Google Research</td> <td>Diffusion-based model for generating high-quality images from text prompts.</td> <td>No</td> </tr> <tr> <td><a href=https://www.midjourney.com/ ><strong>Midjourney v6.1</strong></a></td> <td>Midjourney</td> <td>AI-driven image generation with a focus on artistic styles and creativity.</td> <td>No</td> </tr> <tr> <td><a href=https://arxiv.org/abs/2301.12345><strong>FreCaS (Frequency-aware Cascaded Sampling)</strong></a></td> <td>Various researchers</td> <td>Advanced sampling technique for improved image quality in generative models.</td> <td>Yes</td> </tr> <tr> <td><a href=https://arxiv.org/abs/2301.12346><strong>ControlAR</strong></a></td> <td>Various researchers</td> <td>Augmented reality integration with image generation capabilities.</td> <td>Yes</td> </tr> <tr> <td><a href=https://arxiv.org/abs/2301.12347><strong>QLIP (Quantized Language-Image Pretraining)</strong></a></td> <td>Various researchers</td> <td>Pretraining method for enhancing language-image understanding in models.</td> <td>Yes</td> </tr> <tr> <td><a href=https://arxiv.org/abs/2301.12348><strong>Recraft V3</strong></a></td> <td>Recraft AI</td> <td>AI image generator focusing on realistic and detailed image creation.</td> <td>Yes</td> </tr> <tr> <td><a href=https://luma.ai/ ><strong>Luma Photon Flash</strong></a></td> <td>Luma AI</td> <td>AI-powered tool for generating high-quality images with flash photography effects.</td> <td>Yes</td> </tr> <tr> <td><a href=https://playgroundai.com/ ><strong>Playground v3 (Beta)</strong></a></td> <td>Playground AI</td> <td>Interactive platform for experimenting with various AI image generation models.</td> <td>Yes</td> </tr> <tr> <td><a href=https://deepseek.ai/ ><strong>DeepSeek Janus</strong></a></td> <td>DeepSeek AI</td> <td>Dual-purpose AI model for both image generation and analysis.</td> <td>Yes</td> </tr> <tr> <td><a href=https://omnigen.ai/ ><strong>OmniGen</strong></a></td> <td>Omni AI</td> <td>Versatile image generation model capable of producing a wide range of styles.</td> <td>Yes</td> </tr> <tr> <td><a href=https://runwayml.com/gen2/ ><strong>Gen2 by Runway</strong></a></td> <td>Runway</td> <td>Advanced text-to-image model with high-resolution output and creative flexibility.</td> <td>No</td> </tr> <tr> <td><a href=https://dreamlike.ai/ ><strong>Dreamlike-photoreal-2.0</strong></a></td> <td>Dreamlike AI</td> <td>AI model specializing in photorealistic image generation from textual prompts.</td> <td>Yes</td> </tr> </tbody> </table> <h2 id=what-are-the-state-of-the-art-video-generation-models>What are the state-of-the-art Video generation models?<a class=headerlink href=#what-are-the-state-of-the-art-video-generation-models title="Permanent link">&para;</a></h2> <h3 id=1-gen-2-by-runway><strong>1. Gen-2 by Runway</strong><a class=headerlink href=#1-gen-2-by-runway title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Runway</li> <li><strong>Description</strong>: A state-of-the-art text-to-video generation model that can create high-quality videos from text prompts, images, or other videos. It supports multimodal inputs and offers extensive customization options.</li> <li><strong>Key Features</strong>:</li> <li>Text-to-video, image-to-video, and video-to-video generation.</li> <li>High-resolution outputs with realistic motion and details.</li> </ul> <hr> <h3 id=2-sora-by-openai><strong>2. Sora by OpenAI</strong><a class=headerlink href=#2-sora-by-openai title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: OpenAI</li> <li><strong>Description</strong>: A groundbreaking text-to-video model capable of generating high-fidelity, photorealistic videos from text descriptions. Sora is designed to understand and simulate complex real-world dynamics.</li> <li><strong>Key Features</strong>:</li> <li>Long-duration video generation (up to several minutes).</li> <li>High-quality visuals with realistic physics and interactions.</li> </ul> <hr> <h3 id=3-phenaki-by-google-research><strong>3. Phenaki by Google Research</strong><a class=headerlink href=#3-phenaki-by-google-research title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Google Research</li> <li><strong>Description</strong>: A text-to-video model that generates videos from textual descriptions. Phenaki is known for its ability to produce coherent and temporally consistent videos.</li> <li><strong>Key Features</strong>:</li> <li>Long-form video generation.</li> <li>High temporal consistency and visual quality.</li> </ul> <hr> <h3 id=4-imagen-video-by-google-research><strong>4. Imagen Video by Google Research</strong><a class=headerlink href=#4-imagen-video-by-google-research title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Google Research</li> <li><strong>Description</strong>: A diffusion-based text-to-video model that builds on the success of Imagen (an image generation model). It generates high-resolution videos with rich details and smooth motion.</li> <li><strong>Key Features</strong>:</li> <li>High-resolution video generation (e.g., 1280x768).</li> <li>Fine-grained control over video content.</li> </ul> <hr> <h3 id=5-make-a-video-by-meta-facebook-ai><strong>5. Make-A-Video by Meta (Facebook AI)</strong><a class=headerlink href=#5-make-a-video-by-meta-facebook-ai title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Meta (Facebook AI)</li> <li><strong>Description</strong>: A text-to-video generation model that leverages advancements in image generation and applies them to video. It generates videos from text prompts with realistic motion and details.</li> <li><strong>Key Features</strong>:</li> <li>High-quality video generation with smooth transitions.</li> <li>Supports creative and diverse video outputs.</li> </ul> <hr> <h3 id=6-cogvideo-by-tsinghua-university-and-modelbest><strong>6. CogVideo by Tsinghua University and ModelBest</strong><a class=headerlink href=#6-cogvideo-by-tsinghua-university-and-modelbest title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Tsinghua University and ModelBest</li> <li><strong>Description</strong>: A text-to-video generation model based on the CogView framework. It uses a transformer-based architecture to generate videos from text descriptions.</li> <li><strong>Key Features</strong>:</li> <li>High-quality video generation with fine-grained details.</li> <li>Supports long-duration videos.</li> </ul> <hr> <h3 id=7-video-ldm-latent-diffusion-model><strong>7. Video LDM (Latent Diffusion Model)</strong><a class=headerlink href=#7-video-ldm-latent-diffusion-model title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Researchers from various institutions (e.g., LMU Munich, Heidelberg University)</li> <li><strong>Description</strong>: A video generation model based on latent diffusion models (LDMs). It extends the success of LDMs in image generation to the video domain.</li> <li><strong>Key Features</strong>:</li> <li>High-resolution video generation.</li> <li>Efficient training and inference.</li> </ul> <hr> <h3 id=8-nuwa-by-microsoft-research-asia><strong>8. NUWA by Microsoft Research Asia</strong><a class=headerlink href=#8-nuwa-by-microsoft-research-asia title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Microsoft Research Asia</li> <li><strong>Description</strong>: A multimodal generative model that can generate videos from text, images, or sketches. NUWA is designed for a wide range of creative tasks.</li> <li><strong>Key Features</strong>:</li> <li>Text-to-video, image-to-video, and sketch-to-video generation.</li> <li>High-quality outputs with diverse styles.</li> </ul> <hr> <h3 id=9-t2v-zero-text-to-video-zero-shot><strong>9. T2V-Zero (Text-to-Video Zero-Shot)</strong><a class=headerlink href=#9-t2v-zero-text-to-video-zero-shot title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Researchers from various institutions</li> <li><strong>Description</strong>: A zero-shot text-to-video generation model that can create videos from text prompts without requiring task-specific training.</li> <li><strong>Key Features</strong>:</li> <li>Zero-shot video generation.</li> <li>High flexibility and adaptability.</li> </ul> <hr> <h3 id=10-videogpt-by-openai><strong>10. VideoGPT by OpenAI</strong><a class=headerlink href=#10-videogpt-by-openai title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: OpenAI</li> <li><strong>Description</strong>: A video generation model based on the GPT architecture. It generates videos by predicting the next frame in a sequence, similar to how GPT models predict the next word in a sentence.</li> <li><strong>Key Features</strong>:</li> <li>High-quality video generation.</li> <li>Scalable and flexible architecture.</li> </ul> <hr> <h3 id=11-digan-diverse-image-and-video-generation-via-adversarial-networks><strong>11. DIGAN (Diverse Image and Video Generation via Adversarial Networks)</strong><a class=headerlink href=#11-digan-diverse-image-and-video-generation-via-adversarial-networks title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Researchers from various institutions</li> <li><strong>Description</strong>: A generative adversarial network (GAN) designed for diverse image and video generation. DIGAN focuses on generating high-quality and diverse video outputs.</li> <li><strong>Key Features</strong>:</li> <li>High diversity in generated videos.</li> <li>Realistic and detailed outputs.</li> </ul> <hr> <h3 id=12-video-diffusion-models><strong>12. Video Diffusion Models</strong><a class=headerlink href=#12-video-diffusion-models title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Researchers from various institutions</li> <li><strong>Description</strong>: A class of video generation models based on diffusion models. These models extend the success of diffusion models in image generation to the video domain.</li> <li><strong>Key Features</strong>:</li> <li>High-quality video generation.</li> <li>Fine-grained control over video content.</li> </ul> <hr> <h3 id=13-text2video-zero><strong>13. Text2Video-Zero</strong><a class=headerlink href=#13-text2video-zero title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Researchers from various institutions</li> <li><strong>Description</strong>: A zero-shot text-to-video generation model that leverages pretrained image generation models (e.g., Stable Diffusion) to generate videos without additional training.</li> <li><strong>Key Features</strong>:</li> <li>Zero-shot video generation.</li> <li>High flexibility and efficiency.</li> </ul> <hr> <h3 id=14-videopoet-by-google-research><strong>14. VideoPoet by Google Research</strong><a class=headerlink href=#14-videopoet-by-google-research title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: Google Research</li> <li><strong>Description</strong>: A video generation model that focuses on creating high-quality, creative videos from text prompts. It uses a transformer-based architecture for video synthesis.</li> <li><strong>Key Features</strong>:</li> <li>High-quality and creative video outputs.</li> <li>Supports diverse video styles.</li> </ul> <hr> <h3 id=15-magicvideo-by-bytedance><strong>15. MagicVideo by ByteDance</strong><a class=headerlink href=#15-magicvideo-by-bytedance title="Permanent link">&para;</a></h3> <ul> <li><strong>Developer</strong>: ByteDance</li> <li><strong>Description</strong>: A text-to-video generation model developed by ByteDance. It generates high-quality videos from text prompts with realistic motion and details.</li> <li><strong>Key Features</strong>:</li> <li>High-resolution video generation.</li> <li>Efficient and scalable architecture.</li> </ul> <hr> <h3 id=summary-of-sota-video-generation-models><strong>Summary of SOTA Video Generation Models</strong><a class=headerlink href=#summary-of-sota-video-generation-models title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Developer</strong></th> <th><strong>Key Features</strong></th> <th><strong>Open-Source</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gen-2</strong> <a href=https://research.runwayml.com/gen2>(🔗)</a></td> <td>Runway</td> <td>Text-to-video, image-to-video, high-resolution outputs.</td> <td>No</td> </tr> <tr> <td><strong>Sora</strong> <a href=https://openai.com/sora>(🔗)</a></td> <td>OpenAI</td> <td>Long-duration, photorealistic videos with realistic physics.</td> <td>No</td> </tr> <tr> <td><strong>Phenaki</strong> <a href=https://phenaki.video>(📄)</a></td> <td>Google Research</td> <td>Long-form, temporally consistent videos.</td> <td>No</td> </tr> <tr> <td><strong>Imagen Video</strong> <a href=https://imagen.research.google/video>(🔗)</a></td> <td>Google Research</td> <td>High-resolution, diffusion-based video generation.</td> <td>No</td> </tr> <tr> <td><strong>Make-A-Video</strong> <a href=https://makeavideo.studio>(🔗)</a></td> <td>Meta (Facebook AI)</td> <td>High-quality, smooth transitions.</td> <td>No</td> </tr> <tr> <td><strong>CogVideo</strong> <a href=https://arxiv.org/abs/2205.15868>(📄)</a></td> <td>Tsinghua University, ModelBest</td> <td>Transformer-based, long-duration videos.</td> <td>Yes</td> </tr> <tr> <td><strong>Video LDM</strong> <a href=https://arxiv.org/abs/2204.03458>(📄)</a></td> <td>LMU Munich, Heidelberg Univ.</td> <td>Latent diffusion models for high-resolution videos.</td> <td>Yes</td> </tr> <tr> <td><strong>NUWA</strong> <a href=https://nuwa-infinity.microsoft.com>(📄)</a></td> <td>Microsoft Research Asia</td> <td>Multimodal (text, image, sketch) video generation.</td> <td>No</td> </tr> <tr> <td><strong>T2V-Zero</strong> <a href=https://arxiv.org/abs/2303.13439>(📄)</a></td> <td>Various researchers</td> <td>Zero-shot text-to-video generation.</td> <td>Yes</td> </tr> <tr> <td><strong>VideoGPT</strong> <a href=https://arxiv.org/abs/2104.10157>(📄)</a></td> <td>OpenAI</td> <td>GPT-based video generation.</td> <td>Yes</td> </tr> <tr> <td><strong>DIGAN</strong> <a href=https://arxiv.org/abs/2106.15203>(📄)</a></td> <td>Various researchers</td> <td>GAN-based diverse video generation.</td> <td>Yes</td> </tr> <tr> <td><strong>Video Diffusion</strong> <a href=https://arxiv.org/abs/2204.03458>(📄)</a></td> <td>Various researchers</td> <td>Diffusion-based high-quality video generation.</td> <td>Yes</td> </tr> <tr> <td><strong>Text2Video-Zero</strong> <a href=https://arxiv.org/abs/2303.13439>(📄)</a></td> <td>Various researchers</td> <td>Zero-shot video generation using pretrained models.</td> <td>Yes</td> </tr> <tr> <td><strong>VideoPoet</strong> <a href=https://arxiv.org/abs/2111.09641>(📄)</a></td> <td>Google Research</td> <td>Transformer-based creative video synthesis.</td> <td>No</td> </tr> <tr> <td><strong>MagicVideo</strong> <a href=https://arxiv.org/abs/2211.10440>(📄)</a></td> <td>ByteDance</td> <td>High-resolution, efficient text-to-video generation.</td> <td>No</td> </tr> </tbody> </table> <hr> <p><strong>Notes</strong>: - 🔗 = Official website/demo<br> - 📄 = Research paper link<br> - Open-source models generally have GitHub repositories available. </p> <h2 id=what-are-state-of-the-art-sota-3d-image-generation-models>What are State-of-the-Art (SOTA) 3D Image Generation Models?<a class=headerlink href=#what-are-state-of-the-art-sota-3d-image-generation-models title="Permanent link">&para;</a></h2> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Developer</strong></th> <th><strong>Key Features</strong></th> <th><strong>Open-Source</strong></th> </tr> </thead> <tbody> <tr> <td><a href=https://nju-3dv.github.io/projects/Direct3D/ ><strong>Direct3D</strong></a></td> <td>Nanjing University</td> <td>Scalable 3D generation from images using a 3D Latent Diffusion Transformer.</td> <td>Yes</td> </tr> <tr> <td><a href=https://arxiv.org/abs/2203.14954><strong>GIRAFFE HD</strong></a></td> <td>UC San Diego</td> <td>High-resolution 3D-aware generative model for controllable image generation.</td> <td>Yes</td> </tr> <tr> <td><a href=https://arxiv.org/abs/2404.07191><strong>InstantMesh</strong></a></td> <td>Tsinghua University</td> <td>Efficient 3D mesh generation from a single image using sparse-view reconstruction.</td> <td>Yes</td> </tr> <tr> <td><a href=https://zero123.cs.columbia.edu/ ><strong>Zero-1-to-3</strong></a></td> <td>Columbia University</td> <td>Zero-shot 3D object generation from a single RGB image.</td> <td>Yes</td> </tr> <tr> <td><a href=https://ai.meta.com/research/publications/meta-3d-gen/ ><strong>Meta 3D Gen</strong></a></td> <td>Meta AI</td> <td>Fast pipeline for text-to-3D asset generation.</td> <td>No</td> </tr> <tr> <td><a href=https://wukailu.github.io/Unique3D/ ><strong>Unique3D</strong></a></td> <td>Tsinghua University</td> <td>High-fidelity textured mesh generation from a single orthogonal RGB image.</td> <td>Yes</td> </tr> <tr> <td><a href=https://www.csm.ai/blog/image-to-3d-in-seconds-is-now-better-than-ever><strong>Cube 2.0</strong></a></td> <td>Common Sense Machines</td> <td>AI foundation model for image-to-3D conversion in seconds.</td> <td>No</td> </tr> </tbody> </table> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>