<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/DSR-Important-AI-Paper-List.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Important AI Paper List - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta name=author content="Hari Thapliyaal"><meta name=description content="A curated collection of influential research papers in artificial intelligence, machine learning, and deep learning. This comprehensive list includes groundbreaking papers that have shaped the field of AI and continue to influence its development."><meta name=keywords content="AI Research Papers, Machine Learning Papers, Deep Learning Research, AI Academic Resources, Research Collection, ML Publications, AI Development Papers, Scientific Literature"><meta property=og:type content=article><meta property=og:locale content=en_US><meta property=og:site_name content=DasarpAI><meta property=og:title content="Important AI Paper List"><meta property=og:description content="A curated collection of influential research papers in artificial intelligence, machine learning, and deep learning. This comprehensive list includes groundbreaking papers that have shaped the field of AI and continue to influence its development."><meta property=og:url content=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/DSR-Important-AI-Paper-List.html><meta property=og:image content=../../assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta name=twitter:card content=summary_large_image><meta name=twitter:site content=@dasarpai><meta name=twitter:title content="Important AI Paper List"><meta name=twitter:description content="A curated collection of influential research papers in artificial intelligence, machine learning, and deep learning. This comprehensive list includes groundbreaking papers that have shaped the field of AI and continue to influence its development."><meta name=twitter:image content=../../assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/DSR-Important-AI-Paper-List.html><link rel=stylesheet href=../assets/stylesheets/custom.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#important-ai-paper-list class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@harithapliyal</strong> on <a rel=me href=https://linkedin.com/in/harithapliyal> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/dasarpai> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Important AI Paper List </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduciton class=md-nav__link> <span class=md-ellipsis> Introduciton </span> </a> </li> <li class=md-nav__item> <a href=#citations class=md-nav__link> <span class=md-ellipsis> Citations </span> </a> <nav class=md-nav aria-label=Citations> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bahdanau2015 class=md-nav__link> <span class=md-ellipsis> [Bahdanau2015] </span> </a> </li> <li class=md-nav__item> <a href=#bao2020 class=md-nav__link> <span class=md-ellipsis> [Bao2020] </span> </a> </li> <li class=md-nav__item> <a href=#brown2020 class=md-nav__link> <span class=md-ellipsis> [Brown2020] </span> </a> </li> <li class=md-nav__item> <a href=#chen2020a class=md-nav__link> <span class=md-ellipsis> [Chen2020a] </span> </a> </li> <li class=md-nav__item> <a href=#chen2020b class=md-nav__link> <span class=md-ellipsis> [Chen2020b] </span> </a> </li> <li class=md-nav__item> <a href=#conneau2019 class=md-nav__link> <span class=md-ellipsis> [Conneau2019] </span> </a> </li> <li class=md-nav__item> <a href=#devlin2019 class=md-nav__link> <span class=md-ellipsis> [Devlin2019] </span> </a> </li> <li class=md-nav__item> <a href=#dong2019 class=md-nav__link> <span class=md-ellipsis> [Dong2019] </span> </a> </li> <li class=md-nav__item> <a href=#fan2019 class=md-nav__link> <span class=md-ellipsis> [Fan2019] </span> </a> </li> <li class=md-nav__item> <a href=#gehring2017 class=md-nav__link> <span class=md-ellipsis> [Gehring2017] </span> </a> </li> <li class=md-nav__item> <a href=#gong2020 class=md-nav__link> <span class=md-ellipsis> [Gong2020] </span> </a> </li> <li class=md-nav__item> <a href=#gu2020 class=md-nav__link> <span class=md-ellipsis> [Gu2020] </span> </a> </li> <li class=md-nav__item> <a href=#guan2020 class=md-nav__link> <span class=md-ellipsis> [Guan2020] </span> </a> </li> <li class=md-nav__item> <a href=#hendrycks2020 class=md-nav__link> <span class=md-ellipsis> [Hendrycks2020] </span> </a> </li> <li class=md-nav__item> <a href=#keskar2019 class=md-nav__link> <span class=md-ellipsis> [Keskar2019] </span> </a> </li> <li class=md-nav__item> <a href=#kryscinski2018 class=md-nav__link> <span class=md-ellipsis> [Kryscinski2018] </span> </a> </li> <li class=md-nav__item> <a href=#lan2020 class=md-nav__link> <span class=md-ellipsis> [Lan2020] </span> </a> </li> <li class=md-nav__item> <a href=#lewis2020 class=md-nav__link> <span class=md-ellipsis> [Lewis2020] </span> </a> </li> <li class=md-nav__item> <a href=#li2019 class=md-nav__link> <span class=md-ellipsis> [Li2019] </span> </a> </li> <li class=md-nav__item> <a href=#li2020 class=md-nav__link> <span class=md-ellipsis> [Li2020] </span> </a> </li> <li class=md-nav__item> <a href=#li2021a class=md-nav__link> <span class=md-ellipsis> [Li2021a] </span> </a> </li> <li class=md-nav__item> <a href=#li2021b class=md-nav__link> <span class=md-ellipsis> [Li2021b] </span> </a> </li> <li class=md-nav__item> <a href=#li2021c class=md-nav__link> <span class=md-ellipsis> [Li2021c] </span> </a> </li> <li class=md-nav__item> <a href=#lin2020 class=md-nav__link> <span class=md-ellipsis> [Lin2020] </span> </a> </li> <li class=md-nav__item> <a href=#liu2019 class=md-nav__link> <span class=md-ellipsis> [Liu2019] </span> </a> </li> <li class=md-nav__item> <a href=#mager2020 class=md-nav__link> <span class=md-ellipsis> [Mager2020] </span> </a> </li> <li class=md-nav__item> <a href=#peters2018 class=md-nav__link> <span class=md-ellipsis> [Peters2018] </span> </a> </li> <li class=md-nav__item> <a href=#qiu2020 class=md-nav__link> <span class=md-ellipsis> [Qiu2020] </span> </a> </li> <li class=md-nav__item> <a href=#radford2019 class=md-nav__link> <span class=md-ellipsis> [Radford2019] </span> </a> </li> <li class=md-nav__item> <a href=#raffel2020 class=md-nav__link> <span class=md-ellipsis> [Raffel2020] </span> </a> </li> <li class=md-nav__item> <a href=#ribeiro2020 class=md-nav__link> <span class=md-ellipsis> [Ribeiro2020] </span> </a> </li> <li class=md-nav__item> <a href=#ross-2012 class=md-nav__link> <span class=md-ellipsis> [Ross, 2012] </span> </a> </li> <li class=md-nav__item> <a href=#rothe2020 class=md-nav__link> <span class=md-ellipsis> [Rothe2020] </span> </a> </li> <li class=md-nav__item> <a href=#sanh2019 class=md-nav__link> <span class=md-ellipsis> [Sanh2019] </span> </a> </li> <li class=md-nav__item> <a href=#see2017 class=md-nav__link> <span class=md-ellipsis> [See2017] </span> </a> </li> <li class=md-nav__item> <a href=#song2019 class=md-nav__link> <span class=md-ellipsis> [Song2019] </span> </a> </li> <li class=md-nav__item> <a href=#sun2019a class=md-nav__link> <span class=md-ellipsis> [Sun2019a] </span> </a> </li> <li class=md-nav__item> <a href=#sun2019b class=md-nav__link> <span class=md-ellipsis> [Sun2019b] </span> </a> </li> <li class=md-nav__item> <a href=#vaswani2017 class=md-nav__link> <span class=md-ellipsis> [Vaswani2017] </span> </a> </li> <li class=md-nav__item> <a href=#wada2018 class=md-nav__link> <span class=md-ellipsis> [Wada2018] </span> </a> </li> <li class=md-nav__item> <a href=#wolf2019 class=md-nav__link> <span class=md-ellipsis> [Wolf2019] </span> </a> </li> <li class=md-nav__item> <a href=#xia2020 class=md-nav__link> <span class=md-ellipsis> [Xia2020] </span> </a> </li> <li class=md-nav__item> <a href=#xu2020a class=md-nav__link> <span class=md-ellipsis> [Xu2020a] </span> </a> </li> <li class=md-nav__item> <a href=#xu2020b class=md-nav__link> <span class=md-ellipsis> [Xu2020b] </span> </a> </li> <li class=md-nav__item> <a href=#yang2020a class=md-nav__link> <span class=md-ellipsis> [Yang2020a] </span> </a> </li> <li class=md-nav__item> <a href=#yang2020b class=md-nav__link> <span class=md-ellipsis> [Yang2020b] </span> </a> </li> <li class=md-nav__item> <a href=#zaib2020 class=md-nav__link> <span class=md-ellipsis> [Zaib2020] </span> </a> </li> <li class=md-nav__item> <a href=#zeng2020 class=md-nav__link> <span class=md-ellipsis> [Zeng2020] </span> </a> </li> <li class=md-nav__item> <a href=#zhang2019a class=md-nav__link> <span class=md-ellipsis> [Zhang2019a] </span> </a> </li> <li class=md-nav__item> <a href=#zhang2019b class=md-nav__link> <span class=md-ellipsis> [Zhang2019b] </span> </a> </li> <li class=md-nav__item> <a href=#zhang2019c class=md-nav__link> <span class=md-ellipsis> [Zhang2019c] </span> </a> </li> <li class=md-nav__item> <a href=#zhang2020 class=md-nav__link> <span class=md-ellipsis> [Zhang2020] </span> </a> </li> <li class=md-nav__item> <a href=#zhao2020 class=md-nav__link> <span class=md-ellipsis> [Zhao2020] </span> </a> </li> <li class=md-nav__item> <a href=#zheng2019 class=md-nav__link> <span class=md-ellipsis> [Zheng2019] </span> </a> </li> <li class=md-nav__item> <a href=#zhou2020 class=md-nav__link> <span class=md-ellipsis> [Zhou2020] </span> </a> </li> <li class=md-nav__item> <a href=#cohana2018 class=md-nav__link> <span class=md-ellipsis> [CohanA2018] </span> </a> </li> <li class=md-nav__item> <a href=#nenkovaa2007 class=md-nav__link> <span class=md-ellipsis> [NenkovaA2007] </span> </a> </li> <li class=md-nav__item> <a href=#radforda class=md-nav__link> <span class=md-ellipsis> [RadfordA] </span> </a> </li> <li class=md-nav__item> <a href=#rasimma2013 class=md-nav__link> <span class=md-ellipsis> [RasimMA2013] </span> </a> </li> <li class=md-nav__item> <a href=#rasimma class=md-nav__link> <span class=md-ellipsis> [RasimMA] </span> </a> </li> <li class=md-nav__item> <a href=#vaswania2017 class=md-nav__link> <span class=md-ellipsis> [VaswaniA2017] </span> </a> </li> <li class=md-nav__item> <a href=#raffelc2019 class=md-nav__link> <span class=md-ellipsis> [RaffelC2019] </span> </a> </li> <li class=md-nav__item> <a href=#bahdanaud2014 class=md-nav__link> <span class=md-ellipsis> [BahdanauD2014] </span> </a> </li> <li class=md-nav__item> <a href=#gunese2004 class=md-nav__link> <span class=md-ellipsis> [GunesE2004] </span> </a> </li> <li class=md-nav__item> <a href=#zhangh class=md-nav__link> <span class=md-ellipsis> [ZhangH] </span> </a> </li> <li class=md-nav__item> <a href=#devlinj2019 class=md-nav__link> <span class=md-ellipsis> [DevlinJ2019] </span> </a> </li> <li class=md-nav__item> <a href=#howardj class=md-nav__link> <span class=md-ellipsis> [HowardJ] </span> </a> </li> <li class=md-nav__item> <a href=#zhangj2019 class=md-nav__link> <span class=md-ellipsis> [ZhangJ2019] </span> </a> </li> <li class=md-nav__item> <a href=#kaikhahk class=md-nav__link> <span class=md-ellipsis> [KaikhahK] </span> </a> </li> <li class=md-nav__item> <a href=#xuk class=md-nav__link> <span class=md-ellipsis> [XuK] </span> </a> </li> <li class=md-nav__item> <a href=#chin-yewl class=md-nav__link> <span class=md-ellipsis> [Chin-YewL] </span> </a> </li> <li class=md-nav__item> <a href=#m2019 class=md-nav__link> <span class=md-ellipsis> [M2019] </span> </a> </li> <li class=md-nav__item> <a href=#ch2011 class=md-nav__link> <span class=md-ellipsis> [Ch2011] </span> </a> </li> <li class=md-nav__item> <a href=#conroyjm class=md-nav__link> <span class=md-ellipsis> [ConroyJM] </span> </a> </li> <li class=md-nav__item> <a href=#petersm2018 class=md-nav__link> <span class=md-ellipsis> [PetersM2018] </span> </a> </li> <li class=md-nav__item> <a href=#rusham2015 class=md-nav__link> <span class=md-ellipsis> [RushAM2015] </span> </a> </li> <li class=md-nav__item> <a href=#vinyalso2015 class=md-nav__link> <span class=md-ellipsis> [VinyalsO2015] </span> </a> </li> <li class=md-nav__item> <a href=#dragomirrr2004 class=md-nav__link> <span class=md-ellipsis> [DragomirRR2004] </span> </a> </li> <li class=md-nav__item> <a href=#mihalcear2004 class=md-nav__link> <span class=md-ellipsis> [MihalceaR2004] </span> </a> </li> <li class=md-nav__item> <a href=#nallapatir2016 class=md-nav__link> <span class=md-ellipsis> [NallapatiR2016] </span> </a> </li> <li class=md-nav__item> <a href=#oakr2015 class=md-nav__link> <span class=md-ellipsis> [OakR2015] </span> </a> </li> <li class=md-nav__item> <a href=#parkerr2011 class=md-nav__link> <span class=md-ellipsis> [ParkerR2011] </span> </a> </li> <li class=md-nav__item> <a href=#chopras2016 class=md-nav__link> <span class=md-ellipsis> [ChopraS2016] </span> </a> </li> <li class=md-nav__item> <a href=#evans2008 class=md-nav__link> <span class=md-ellipsis> [EvanS2008] </span> </a> </li> <li class=md-nav__item> <a href=#edunovs2019 class=md-nav__link> <span class=md-ellipsis> [EdunovS2019] </span> </a> </li> <li class=md-nav__item> <a href=#narayans2018 class=md-nav__link> <span class=md-ellipsis> [NarayanS2018] </span> </a> </li> <li class=md-nav__item> <a href=#peterj2017 class=md-nav__link> <span class=md-ellipsis> [PeterJ2017] </span> </a> </li> <li class=md-nav__item> <a href=#guptav2010 class=md-nav__link> <span class=md-ellipsis> [GuptaV2010] </span> </a> </li> <li class=md-nav__item> <a href=#sanhv2019 class=md-nav__link> <span class=md-ellipsis> [SanhV2019] </span> </a> </li> <li class=md-nav__item> <a href=#liuy2019 class=md-nav__link> <span class=md-ellipsis> [LiuY2019] </span> </a> </li> <li class=md-nav__item> <a href=#yany2020 class=md-nav__link> <span class=md-ellipsis> [YanY2020] </span> </a> </li> <li class=md-nav__item> <a href=#daiz class=md-nav__link> <span class=md-ellipsis> [DaiZ] </span> </a> </li> <li class=md-nav__item> <a href=#lanz2019 class=md-nav__link> <span class=md-ellipsis> [LanZ2019] </span> </a> </li> <li class=md-nav__item> <a href=#yangz2019 class=md-nav__link> <span class=md-ellipsis> [YangZ2019] </span> </a> </li> <li class=md-nav__item> <a href=#mart2016 class=md-nav__link> <span class=md-ellipsis> [Mart2016] </span> </a> </li> <li class=md-nav__item> <a href=#rohan2019 class=md-nav__link> <span class=md-ellipsis> [Rohan2019] </span> </a> </li> <li class=md-nav__item> <a href=#martin2017 class=md-nav__link> <span class=md-ellipsis> [Martin2017] </span> </a> </li> <li class=md-nav__item> <a href=#matthew2017 class=md-nav__link> <span class=md-ellipsis> [Matthew2017] </span> </a> </li> <li class=md-nav__item> <a href=#jimmy2016 class=md-nav__link> <span class=md-ellipsis> [Jimmy2016] </span> </a> </li> <li class=md-nav__item> <a href=#lo2019 class=md-nav__link> <span class=md-ellipsis> [Lo2019] </span> </a> </li> <li class=md-nav__item> <a href=#yoshua2003 class=md-nav__link> <span class=md-ellipsis> [Yoshua2003] </span> </a> </li> <li class=md-nav__item> <a href=#thorsten2007 class=md-nav__link> <span class=md-ellipsis> [Thorsten2007] </span> </a> </li> <li class=md-nav__item> <a href=#miles2016 class=md-nav__link> <span class=md-ellipsis> [Miles2016] </span> </a> </li> <li class=md-nav__item> <a href=#miles2019 class=md-nav__link> <span class=md-ellipsis> [Miles2019] </span> </a> </li> <li class=md-nav__item> <a href=#xi2016 class=md-nav__link> <span class=md-ellipsis> [Xi2016] </span> </a> </li> <li class=md-nav__item> <a href=#rewon2019 class=md-nav__link> <span class=md-ellipsis> [Rewon2019] </span> </a> </li> <li class=md-nav__item> <a href=#ronan2008 class=md-nav__link> <span class=md-ellipsis> [Ronan2008] </span> </a> </li> <li class=md-nav__item> <a href=#ronan2011 class=md-nav__link> <span class=md-ellipsis> [Ronan2011] </span> </a> </li> <li class=md-nav__item> <a href=#ruth1987 class=md-nav__link> <span class=md-ellipsis> [Ruth1987] </span> </a> </li> <li class=md-nav__item> <a href=#andrew2015 class=md-nav__link> <span class=md-ellipsis> [Andrew2015] </span> </a> </li> <li class=md-nav__item> <a href=#zihang2019 class=md-nav__link> <span class=md-ellipsis> [Zihang2019] </span> </a> </li> <li class=md-nav__item> <a href=#jacob2018 class=md-nav__link> <span class=md-ellipsis> [Jacob2018] </span> </a> </li> <li class=md-nav__item> <a href=#john2011 class=md-nav__link> <span class=md-ellipsis> [John2011] </span> </a> </li> <li class=md-nav__item> <a href=#matthew2017_1 class=md-nav__link> <span class=md-ellipsis> [Matthew2017] </span> </a> </li> <li class=md-nav__item> <a href=#angela2018 class=md-nav__link> <span class=md-ellipsis> [Angela2018] </span> </a> </li> <li class=md-nav__item> <a href=#angela2019 class=md-nav__link> <span class=md-ellipsis> [Angela2019] </span> </a> </li> <li class=md-nav__item> <a href=#boris2019 class=md-nav__link> <span class=md-ellipsis> [Boris2019] </span> </a> </li> <li class=md-nav__item> <a href=#ian2014 class=md-nav__link> <span class=md-ellipsis> [Ian2014] </span> </a> </li> <li class=md-nav__item> <a href=#max2016 class=md-nav__link> <span class=md-ellipsis> [Max2016] </span> </a> </li> <li class=md-nav__item> <a href=#kaiming2016 class=md-nav__link> <span class=md-ellipsis> [Kaiming2016] </span> </a> </li> <li class=md-nav__item> <a href=#karl2015 class=md-nav__link> <span class=md-ellipsis> [Karl2015] </span> </a> </li> <li class=md-nav__item> <a href=#ari2019 class=md-nav__link> <span class=md-ellipsis> [Ari2019] </span> </a> </li> <li class=md-nav__item> <a href=#jeremy2018 class=md-nav__link> <span class=md-ellipsis> [Jeremy2018] </span> </a> </li> <li class=md-nav__item> <a href=#hakan2016 class=md-nav__link> <span class=md-ellipsis> [Hakan2016] </span> </a> </li> <li class=md-nav__item> <a href=#melvin2017 class=md-nav__link> <span class=md-ellipsis> [Melvin2017] </span> </a> </li> <li class=md-nav__item> <a href=#mandar2017 class=md-nav__link> <span class=md-ellipsis> [Mandar2017] </span> </a> </li> <li class=md-nav__item> <a href=#david2017 class=md-nav__link> <span class=md-ellipsis> [David2017] </span> </a> </li> <li class=md-nav__item> <a href=#lukasz2017 class=md-nav__link> <span class=md-ellipsis> [Lukasz2017] </span> </a> </li> <li class=md-nav__item> <a href=#ukasz2018 class=md-nav__link> <span class=md-ellipsis> [Łukasz2018] </span> </a> </li> <li class=md-nav__item> <a href=#nitish2019 class=md-nav__link> <span class=md-ellipsis> [Nitish2019] </span> </a> </li> <li class=md-nav__item> <a href=#diederik2014 class=md-nav__link> <span class=md-ellipsis> [Diederik2014] </span> </a> </li> <li class=md-nav__item> <a href=#diederik2013 class=md-nav__link> <span class=md-ellipsis> [Diederik2013] </span> </a> </li> <li class=md-nav__item> <a href=#ryan2015 class=md-nav__link> <span class=md-ellipsis> [Ryan2015] </span> </a> </li> <li class=md-nav__item> <a href=#catherine2016 class=md-nav__link> <span class=md-ellipsis> [Catherine2016] </span> </a> </li> <li class=md-nav__item> <a href=#wojciech2019 class=md-nav__link> <span class=md-ellipsis> [Wojciech2019] </span> </a> </li> <li class=md-nav__item> <a href=#tom2019 class=md-nav__link> <span class=md-ellipsis> [Tom2019] </span> </a> </li> <li class=md-nav__item> <a href=#guillaume2019 class=md-nav__link> <span class=md-ellipsis> [Guillaume2019] </span> </a> </li> <li class=md-nav__item> <a href=#guillaume2019_1 class=md-nav__link> <span class=md-ellipsis> [Guillaume2019] </span> </a> </li> <li class=md-nav__item> <a href=#hector2012 class=md-nav__link> <span class=md-ellipsis> [Hector2012] </span> </a> </li> <li class=md-nav__item> <a href=#patrick2019 class=md-nav__link> <span class=md-ellipsis> [Patrick2019] </span> </a> </li> <li class=md-nav__item> <a href=#minh-thang2015 class=md-nav__link> <span class=md-ellipsis> [Minh-Thang2015] </span> </a> </li> <li class=md-nav__item> <a href=#julian2015 class=md-nav__link> <span class=md-ellipsis> [Julian2015] </span> </a> </li> <li class=md-nav__item> <a href=#bryan6294 class=md-nav__link> <span class=md-ellipsis> [Bryan6294] </span> </a> </li> <li class=md-nav__item> <a href=#bryan2018 class=md-nav__link> <span class=md-ellipsis> [Bryan2018] </span> </a> </li> <li class=md-nav__item> <a href=#15stephen2017 class=md-nav__link> <span class=md-ellipsis> [15Stephen2017] </span> </a> </li> <li class=md-nav__item> <a href=#tomas2013 class=md-nav__link> <span class=md-ellipsis> [Tomas2013] </span> </a> </li> <li class=md-nav__item> <a href=#margaret7596 class=md-nav__link> <span class=md-ellipsis> [Margaret7596] </span> </a> </li> <li class=md-nav__item> <a href=#amit2019 class=md-nav__link> <span class=md-ellipsis> [Amit2019] </span> </a> </li> <li class=md-nav__item> <a href=#vinod2010 class=md-nav__link> <span class=md-ellipsis> [Vinod2010] </span> </a> </li> <li class=md-nav__item> <a href=#ramesh2016 class=md-nav__link> <span class=md-ellipsis> [Ramesh2016] </span> </a> </li> <li class=md-nav__item> <a href=#matthew2018 class=md-nav__link> <span class=md-ellipsis> [Matthew2018] </span> </a> </li> <li class=md-nav__item> <a href=#carol1979 class=md-nav__link> <span class=md-ellipsis> [Carol1979] </span> </a> </li> <li class=md-nav__item> <a href=#shana1980 class=md-nav__link> <span class=md-ellipsis> [Shana1980] </span> </a> </li> <li class=md-nav__item> <a href=#ofir2016 class=md-nav__link> <span class=md-ellipsis> [Ofir2016] </span> </a> </li> <li class=md-nav__item> <a href=#alec2018 class=md-nav__link> <span class=md-ellipsis> [Alec2018] </span> </a> </li> <li class=md-nav__item> <a href=#alec2019 class=md-nav__link> <span class=md-ellipsis> [Alec2019] </span> </a> </li> <li class=md-nav__item> <a href=#nazneen2019 class=md-nav__link> <span class=md-ellipsis> [Nazneen2019] </span> </a> </li> <li class=md-nav__item> <a href=#pranav2016 class=md-nav__link> <span class=md-ellipsis> [Pranav2016] </span> </a> </li> <li class=md-nav__item> <a href=#alexander2015 class=md-nav__link> <span class=md-ellipsis> [Alexander2015] </span> </a> </li> <li class=md-nav__item> <a href=#evan2008 class=md-nav__link> <span class=md-ellipsis> [Evan2008] </span> </a> </li> <li class=md-nav__item> <a href=#thomas2019 class=md-nav__link> <span class=md-ellipsis> [Thomas2019] </span> </a> </li> <li class=md-nav__item> <a href=#abigail2017 class=md-nav__link> <span class=md-ellipsis> [Abigail2017] </span> </a> </li> <li class=md-nav__item> <a href=#rico2015 class=md-nav__link> <span class=md-ellipsis> [Rico2015] </span> </a> </li> <li class=md-nav__item> <a href=#noam2018 class=md-nav__link> <span class=md-ellipsis> [Noam2018] </span> </a> </li> <li class=md-nav__item> <a href=#jack008 class=md-nav__link> <span class=md-ellipsis> [Jack008] </span> </a> </li> <li class=md-nav__item> <a href=#ilya2014 class=md-nav__link> <span class=md-ellipsis> [Ilya2014] </span> </a> </li> <li class=md-nav__item> <a href=#trieu2018 class=md-nav__link> <span class=md-ellipsis> [Trieu2018] </span> </a> </li> <li class=md-nav__item> <a href=#adam2016 class=md-nav__link> <span class=md-ellipsis> [Adam2016] </span> </a> </li> <li class=md-nav__item> <a href=#lav6008 class=md-nav__link> <span class=md-ellipsis> [Lav6008] </span> </a> </li> <li class=md-nav__item> <a href=#curran2018 class=md-nav__link> <span class=md-ellipsis> [Curran2018] </span> </a> </li> <li class=md-nav__item> <a href=#sean2019 class=md-nav__link> <span class=md-ellipsis> [Sean2019] </span> </a> </li> <li class=md-nav__item> <a href=#yonghui2016 class=md-nav__link> <span class=md-ellipsis> [Yonghui2016] </span> </a> </li> <li class=md-nav__item> <a href=#stratos2019 class=md-nav__link> <span class=md-ellipsis> [Stratos2019] </span> </a> </li> <li class=md-nav__item> <a href=#zhilin2018 class=md-nav__link> <span class=md-ellipsis> [Zhilin2018] </span> </a> </li> <li class=md-nav__item> <a href=#rowan-2019 class=md-nav__link> <span class=md-ellipsis> [Rowan 2019] </span> </a> </li> <li class=md-nav__item> <a href=#fangxiaoyu2022 class=md-nav__link> <span class=md-ellipsis> [Fangxiaoyu2022] </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#nlp-papers-available-on-my-google-drive class=md-nav__link> <span class=md-ellipsis> NLP Papers Available on my Google Drive </span> </a> </li> <li class=md-nav__item> <a href=#ai-papers-available-on-my-google-drive class=md-nav__link> <span class=md-ellipsis> AI Papers Available on my Google Drive </span> </a> </li> <li class=md-nav__item> <a href=#recent-papers class=md-nav__link> <span class=md-ellipsis> Recent Papers </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <article class="md-content__inner md-typeset"> <p><img alt="Important AI Paper List" src=../assets/images/dspost/dsp6090-rps-Important-AI-Paper-List.jpg></p> <h1 id=important-ai-paper-list>Important AI Paper List<a class=headerlink href=#important-ai-paper-list title="Permanent link">&para;</a></h1> <h2 id=introduciton>Introduciton<a class=headerlink href=#introduciton title="Permanent link">&para;</a></h2> <p>In almost all citations it becomes very difficult to read the title of research papers. Why? Because the contributors' information is first and most of the time, it is difficult to read the name other than native people. For example, if an Indian find a native name like "Vivek Ramaswami, Kartikeyan Karunanidhi" it is easy for them to read the name but the same name becomes difficult to read for non-Indian people, and vice-versa. Giving respect to the creator is very important but more than we need to know what have they done. I know from my experience, for almost every researcher, it becomes very difficult to track good AI research papers. For me, it is more difficult because I need to maintain this blog and I want to give references to the work across different webpages. Therefore I am creating a citation key, which includes the Last name of the first researcher + year of presenting that paper. Along with this, I am describing the title of the paper and where it was presented. If you find a particular title interesting for your work you can search that paper on "google scholar", Mendeley, sci-hub or other places with which you are familiar and comfortable. Post that you can download and read that paper at your leisure. Hope you find this list of some use for your work.</p> <h2 id=citations>Citations<a class=headerlink href=#citations title="Permanent link">&para;</a></h2> <blockquote> <p><strong>Pretrained Language Models for Text Generation: A Survey</strong></p> </blockquote> <h3 id=bahdanau2015>[Bahdanau2015]<a class=headerlink href=#bahdanau2015 title="Permanent link">&para;</a></h3> <p>Neural machine translation by jointly learning to align and translate. In ICLR, 2015.</p> <h3 id=bao2020>[Bao2020]<a class=headerlink href=#bao2020 title="Permanent link">&para;</a></h3> <p>PLATO-2: towards building an open- domain chatbot via curriculum learning. arXiv preprint arXiv:2006.16779, 2020.</p> <h3 id=brown2020>[Brown2020]<a class=headerlink href=#brown2020 title="Permanent link">&para;</a></h3> <p>Language models are few-shot learners. In NeurIPS, 2020.</p> <h3 id=chen2020a>[Chen2020a]<a class=headerlink href=#chen2020a title="Permanent link">&para;</a></h3> <p>Distilling knowledge learned in BERT for text generation. In ACL, 2020.</p> <h3 id=chen2020b>[Chen2020b]<a class=headerlink href=#chen2020b title="Permanent link">&para;</a></h3> <p>Few-shot NLG with pre-trained language model. In ACL, 2020.</p> <h3 id=conneau2019>[Conneau2019]<a class=headerlink href=#conneau2019 title="Permanent link">&para;</a></h3> <p>Cross-lingual language model pretraining. In NeurIPS, 2019.</p> <h3 id=devlin2019>[Devlin2019]<a class=headerlink href=#devlin2019 title="Permanent link">&para;</a></h3> <p>BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019.</p> <h3 id=dong2019>[Dong2019]<a class=headerlink href=#dong2019 title="Permanent link">&para;</a></h3> <p>Unified language model pretraining for natural language understanding and generation. In NeurIPS, 2019.</p> <h3 id=fan2019>[Fan2019]<a class=headerlink href=#fan2019 title="Permanent link">&para;</a></h3> <p>Unsupervised pre-training for sequence to sequence speech recognition. CoRR, arXiv preprint arXiv:1910.12418, 2019.</p> <h3 id=gehring2017>[Gehring2017]<a class=headerlink href=#gehring2017 title="Permanent link">&para;</a></h3> <p>Convolutional sequence to sequence learning. In ICML, 2017.</p> <h3 id=gong2020>[Gong2020]<a class=headerlink href=#gong2020 title="Permanent link">&para;</a></h3> <p>Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching. In COLING, 2020.</p> <h3 id=gu2020>[Gu2020]<a class=headerlink href=#gu2020 title="Permanent link">&para;</a></h3> <p>A tailored pre-training model for task-oriented dialog generation. arXiv preprint arXiv:2004.13835, 2020.</p> <h3 id=guan2020>[Guan2020]<a class=headerlink href=#guan2020 title="Permanent link">&para;</a></h3> <p>Survey on automatic text summarization and transformer models applicability. In CCRIS, 2020.</p> <h3 id=hendrycks2020>[Hendrycks2020]<a class=headerlink href=#hendrycks2020 title="Permanent link">&para;</a></h3> <p>Pretrained transformers improve out-of- distribution robustness. In ACL, 2020.</p> <h3 id=keskar2019>[Keskar2019]<a class=headerlink href=#keskar2019 title="Permanent link">&para;</a></h3> <p>CTRL: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.</p> <h3 id=kryscinski2018>[Kryscinski2018]<a class=headerlink href=#kryscinski2018 title="Permanent link">&para;</a></h3> <p>Improving abstraction in text summarization. In EMNLP, 2018.</p> <h3 id=lan2020>[Lan2020]<a class=headerlink href=#lan2020 title="Permanent link">&para;</a></h3> <p>ALBERT: A lite BERT for self-supervised learning of language representations. In ICLR, 2020.</p> <h3 id=lewis2020>[Lewis2020]<a class=headerlink href=#lewis2020 title="Permanent link">&para;</a></h3> <p>BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, 2020.</p> <h3 id=li2019>[Li2019]<a class=headerlink href=#li2019 title="Permanent link">&para;</a></h3> <p>Generating long and informative reviews with aspect-aware coarse-to-fine decoding. In ACL, pages 1969–1979, 2019.</p> <h3 id=li2020>[Li2020]<a class=headerlink href=#li2020 title="Permanent link">&para;</a></h3> <p>Knowledge-enhanced personalized review generation with capsule graph neural network. In CIKM, pages 735–744, 2020.</p> <h3 id=li2021a>[Li2021a]<a class=headerlink href=#li2021a title="Permanent link">&para;</a></h3> <p>TextBox: A unified, modularized, and extensible framework for text generation. In ACL, 2021.</p> <h3 id=li2021b>[Li2021b]<a class=headerlink href=#li2021b title="Permanent link">&para;</a></h3> <p>Few-shot knowledge graph-to-text generation with pretrained language models. In Findings of ACL, 2021.</p> <h3 id=li2021c>[Li2021c]<a class=headerlink href=#li2021c title="Permanent link">&para;</a></h3> <p>Knowledge-based review generation by coherence enhanced text planning. In SIGIR, 2021.</p> <h3 id=lin2020>[Lin2020]<a class=headerlink href=#lin2020 title="Permanent link">&para;</a></h3> <p>Pretraining multilingual neural machine translation by leveraging alignment information. In EMNLP, 2020.</p> <h3 id=liu2019>[Liu2019]<a class=headerlink href=#liu2019 title="Permanent link">&para;</a></h3> <p>Text summarization with pretrained encoders. In EMNLP, 2019.</p> <h3 id=mager2020>[Mager2020]<a class=headerlink href=#mager2020 title="Permanent link">&para;</a></h3> <p>GPT-too: A language-model-first approach for AMR-to-text generation. In ACL, 2020.</p> <h3 id=peters2018>[Peters2018]<a class=headerlink href=#peters2018 title="Permanent link">&para;</a></h3> <p>Deep contextualized word representations. In NAACL-HLT, 2018.</p> <h3 id=qiu2020>[Qiu2020]<a class=headerlink href=#qiu2020 title="Permanent link">&para;</a></h3> <p>Pre-trained models for natural language processing: A survey. arXiv preprint arXiv:2003.08271, 2020.</p> <h3 id=radford2019>[Radford2019]<a class=headerlink href=#radford2019 title="Permanent link">&para;</a></h3> <p>Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p> <h3 id=raffel2020>[Raffel2020]<a class=headerlink href=#raffel2020 title="Permanent link">&para;</a></h3> <p>Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020.</p> <h3 id=ribeiro2020>[Ribeiro2020]<a class=headerlink href=#ribeiro2020 title="Permanent link">&para;</a></h3> <p>Investigating pretrained language models for graph-to-text generation. arXiv preprint arXiv:2007.08426, 2020.</p> <h3 id=ross-2012>[Ross, 2012]<a class=headerlink href=#ross-2012 title="Permanent link">&para;</a></h3> <p>Guide for conducting risk assessments. In NIST Special Publication, 2012.</p> <h3 id=rothe2020>[Rothe2020]<a class=headerlink href=#rothe2020 title="Permanent link">&para;</a></h3> <p>Leveraging pre-trained checkpoints for sequence generation tasks. TACL, 2020.</p> <h3 id=sanh2019>[Sanh2019]<a class=headerlink href=#sanh2019 title="Permanent link">&para;</a></h3> <p>Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p> <h3 id=see2017>[See2017]<a class=headerlink href=#see2017 title="Permanent link">&para;</a></h3> <p>Get to the point: Summarization with pointer-generator networks. In ACL, 2017.</p> <h3 id=song2019>[Song2019]<a class=headerlink href=#song2019 title="Permanent link">&para;</a></h3> <p>MASS: masked sequence to sequence pre-training for language generation. In ICML, 2019.</p> <h3 id=sun2019a>[Sun2019a]<a class=headerlink href=#sun2019a title="Permanent link">&para;</a></h3> <p>Contrastive bidirectional transformer for temporal representation learning. arXiv preprint arXiv:1906.05743, 2019.</p> <h3 id=sun2019b>[Sun2019b]<a class=headerlink href=#sun2019b title="Permanent link">&para;</a></h3> <p>Videobert: A joint model for video and language representation learning. In ICCV, 2019.</p> <h3 id=vaswani2017>[Vaswani2017]<a class=headerlink href=#vaswani2017 title="Permanent link">&para;</a></h3> <p>Attention is all you need. In NIPS, 2017.</p> <h3 id=wada2018>[Wada2018]<a class=headerlink href=#wada2018 title="Permanent link">&para;</a></h3> <p>Unsupervised cross-lingual word embedding by multilingual neural language models. arXiv preprint arXiv:1809.02306, 2018.</p> <h3 id=wolf2019>[Wolf2019]<a class=headerlink href=#wolf2019 title="Permanent link">&para;</a></h3> <p>Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149, 2019.</p> <h3 id=xia2020>[Xia2020]<a class=headerlink href=#xia2020 title="Permanent link">&para;</a></h3> <p>XGPT: cross-modal generative pre-training for image captioning. arXiv preprint arXiv:2003.01473, 2020.</p> <h3 id=xu2020a>[Xu2020a]<a class=headerlink href=#xu2020a title="Permanent link">&para;</a></h3> <p>Discourse-aware neural extractive text summarization. In ACL, 2020.</p> <h3 id=xu2020b>[Xu2020b]<a class=headerlink href=#xu2020b title="Permanent link">&para;</a></h3> <p>Unsupervised extractive summarization by pre-training hierarchical transformers. In EMNLP, 2020.</p> <h3 id=yang2020a>[Yang2020a]<a class=headerlink href=#yang2020a title="Permanent link">&para;</a></h3> <p>CSP: code-switching pre-training for neural machine translation. In EMNLP, 2020.</p> <h3 id=yang2020b>[Yang2020b]<a class=headerlink href=#yang2020b title="Permanent link">&para;</a></h3> <p>TED: A pretrained unsupervised summarization model with theme modeling and denoising. In EMNLP (Findings), 2020.</p> <h3 id=zaib2020>[Zaib2020]<a class=headerlink href=#zaib2020 title="Permanent link">&para;</a></h3> <p>A short survey of pre-trained language models for conversational AI-A new age in NLP. In ACSW, 2020.</p> <h3 id=zeng2020>[Zeng2020]<a class=headerlink href=#zeng2020 title="Permanent link">&para;</a></h3> <p>Generalized conditioned dialogue generation based on pre-trained language model. arXiv preprint arXiv:2010.11140, 2020.</p> <h3 id=zhang2019a>[Zhang2019a]<a class=headerlink href=#zhang2019a title="Permanent link">&para;</a></h3> <p>Pretraining-based natural language generation for text summarization. In CoNLL, 2019.</p> <h3 id=zhang2019b>[Zhang2019b]<a class=headerlink href=#zhang2019b title="Permanent link">&para;</a></h3> <p>HIBERT: document level pre-training of hierarchical bidirectional transformers for document summarization. In ACL, 2019.</p> <h3 id=zhang2019c>[Zhang2019c]<a class=headerlink href=#zhang2019c title="Permanent link">&para;</a></h3> <p>ERNIE: enhanced language representation with informative entities. In ACL, 2019.</p> <h3 id=zhang2020>[Zhang2020]<a class=headerlink href=#zhang2020 title="Permanent link">&para;</a></h3> <p>DIALOGPT : Largescale generative pre-training for conversational response generation. In ACL, 2020.</p> <h3 id=zhao2020>[Zhao2020]<a class=headerlink href=#zhao2020 title="Permanent link">&para;</a></h3> <p>Knowledge-grounded dialogue generation with pretrained language models. In EMNLP, 2020.</p> <h3 id=zheng2019>[Zheng2019]<a class=headerlink href=#zheng2019 title="Permanent link">&para;</a></h3> <p>Sentence centrality revisited for unsupervised summarization. In ACL, 2019.</p> <h3 id=zhou2020>[Zhou2020]<a class=headerlink href=#zhou2020 title="Permanent link">&para;</a></h3> <p>Unified vision-language pre-training for image captioning and VQA. In AAAI, 2020</p> <blockquote> <p><strong>Survey on Automatic Text Summarization and Transformer Models Applicability</strong></p> </blockquote> <h3 id=cohana2018>[CohanA2018]<a class=headerlink href=#cohana2018 title="Permanent link">&para;</a></h3> <p>A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents. In Proceedings of the 2018 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies. 615–621.</p> <h3 id=nenkovaa2007>[NenkovaA2007]<a class=headerlink href=#nenkovaa2007 title="Permanent link">&para;</a></h3> <p>The pyramid method: Incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing 4, 2 (2007).</p> <h3 id=radforda>[RadfordA]<a class=headerlink href=#radforda title="Permanent link">&para;</a></h3> <p>Improving language understanding by generative pre-training. <a href=http://www.cs.ubc.ca/~amuham01/LING530/ >www.cs.ubc.ca/~amuham01/LING530/</a> papers/radford2018improving.pdf</p> <h3 id=rasimma2013>[RasimMA2013]<a class=headerlink href=#rasimma2013 title="Permanent link">&para;</a></h3> <p>Multiple documents summarization based on evolutionary optimization algorithm. Expert Systems with Applications 40, 5 (2013), 1675–1689.</p> <h3 id=rasimma>[RasimMA]<a class=headerlink href=#rasimma title="Permanent link">&para;</a></h3> <p>MCMR: Maximum coverage and minimum redundant text summarization model. Expert Systems with Applications 38, 12 (2011), 14514–14522.</p> <h3 id=vaswania2017>[VaswaniA2017]<a class=headerlink href=#vaswania2017 title="Permanent link">&para;</a></h3> <p>Attention is all you need. Advances in neural information processing systems (2017), 5998–6008.</p> <h3 id=raffelc2019>[RaffelC2019]<a class=headerlink href=#raffelc2019 title="Permanent link">&para;</a></h3> <p>Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683 (2019).</p> <h3 id=bahdanaud2014>[BahdanauD2014]<a class=headerlink href=#bahdanaud2014 title="Permanent link">&para;</a></h3> <p>Neural machine translation by jointly learning to align and translate. arXiv:1409.0473 (2014).</p> <h3 id=gunese2004>[GunesE2004]<a class=headerlink href=#gunese2004 title="Permanent link">&para;</a></h3> <p>LexRank: Graph-based lexical centrality as salience in text summarization. Journal ofArtificial Intelligence 20, 1 (2004), 457–479.</p> <h3 id=zhangh>[ZhangH]<a class=headerlink href=#zhangh title="Permanent link">&para;</a></h3> <p>Pretraining-Based Natural Language Generation for Text Summarization. In Proceedings ofthe 23<sup>rd</sup> Conference on Computational Natural Language Learning (CoNLL). 789–797.</p> <h3 id=devlinj2019>[DevlinJ2019]<a class=headerlink href=#devlinj2019 title="Permanent link">&para;</a></h3> <p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings ofthe 2019Conference ofthe NorthAmerican ChapteroftheAssociation forComputational Linguistics: Human Language Technologies. 4171–4186.</p> <h3 id=howardj>[HowardJ]<a class=headerlink href=#howardj title="Permanent link">&para;</a></h3> <p>Universal Language Model Fine-tuning for Text Classification. In Proceedings ofthe 56<sup>th</sup> Annual Meeting ofthe Association for Computational Linguistics. 328–339.</p> <h3 id=zhangj2019>[ZhangJ2019]<a class=headerlink href=#zhangj2019 title="Permanent link">&para;</a></h3> <p>PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. arXiv:1912.08777 (2019).</p> <h3 id=kaikhahk>[KaikhahK]<a class=headerlink href=#kaikhahk title="Permanent link">&para;</a></h3> <p>Text summarization using neural networks. In Proceeding of second conference on intelligent system. 40–44.</p> <h3 id=xuk>[XuK]<a class=headerlink href=#xuk title="Permanent link">&para;</a></h3> <p>Show, attend and tell: Neural image caption generation with visual attention. In Proceedings ofthe International conference on machine learning. 2048–2057.</p> <h3 id=chin-yewl>[Chin-YewL]<a class=headerlink href=#chin-yewl title="Permanent link">&para;</a></h3> <p>ROUGE: A package for automatic evaluation of summaries. In Proceedings ofACL Workshop “Text Summarization Branches Out”. 8.</p> <h3 id=m2019>[M2019]<a class=headerlink href=#m2019 title="Permanent link">&para;</a></h3> <p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461 (2019).</p> <h3 id=ch2011>[Ch2011]<a class=headerlink href=#ch2011 title="Permanent link">&para;</a></h3> <p>A statistical approach for automatic text summarization by extraction. In Proceedings of2011 International Conference on Communication Systems and Network Technologies. 268–271.</p> <h3 id=conroyjm>[ConroyJM]<a class=headerlink href=#conroyjm title="Permanent link">&para;</a></h3> <p>Text Summarization via Hidden Markov Models. In Proceedings ofthe 24<sup>th</sup> Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. 406–407.</p> <h3 id=petersm2018>[PetersM2018]<a class=headerlink href=#petersm2018 title="Permanent link">&para;</a></h3> <p>Deep Contextualized Word Representations. In Proceedings ofthe 2018 Conference ofthe North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies. 2227–2237.</p> <h3 id=rusham2015>[RushAM2015]<a class=headerlink href=#rusham2015 title="Permanent link">&para;</a></h3> <p>A neural attention model for abstractive sentence summarization. arXiv:1509.00685 (2015).</p> <h3 id=vinyalso2015>[VinyalsO2015]<a class=headerlink href=#vinyalso2015 title="Permanent link">&para;</a></h3> <p>Pointer networks. Advances in neural information processing systems (2015), 2692–2700.</p> <h3 id=dragomirrr2004>[DragomirRR2004]<a class=headerlink href=#dragomirrr2004 title="Permanent link">&para;</a></h3> <p>Centroid-based summarization of multiple documents. Information Processing &amp; Management 40, 6 (2004), 919–938.</p> <h3 id=mihalcear2004>[MihalceaR2004]<a class=headerlink href=#mihalcear2004 title="Permanent link">&para;</a></h3> <p>Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing. 404–411.</p> <h3 id=nallapatir2016>[NallapatiR2016]<a class=headerlink href=#nallapatir2016 title="Permanent link">&para;</a></h3> <p>Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv:1602.06023 (2016).</p> <h3 id=oakr2015>[OakR2015]<a class=headerlink href=#oakr2015 title="Permanent link">&para;</a></h3> <p>Extractive techniques for automatic document summarization: a survey. International Journal of Innovative Research in Computer and Communication Engineering 4, 3 (2016), 4158–4164.</p> <h3 id=parkerr2011>[ParkerR2011]<a class=headerlink href=#parkerr2011 title="Permanent link">&para;</a></h3> <p>English Gigaword. <a href=https://catalog.ldc.upenn.edu/LDC2011T07>https://catalog.ldc.upenn.edu/LDC2011T07</a></p> <h3 id=chopras2016>[ChopraS2016]<a class=headerlink href=#chopras2016 title="Permanent link">&para;</a></h3> <p>Abstractive sentence summarization with attentive recurrent neural networks. In Proceedings ofthe 2016 Conference ofthe North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies. 93–98.</p> <h3 id=evans2008>[EvanS2008]<a class=headerlink href=#evans2008 title="Permanent link">&para;</a></h3> <p>The New York Times Annotated Corpus. <a href=https://catalog.ldc.upenn>https://catalog.ldc.upenn</a>. edu/LDC2008T19</p> <h3 id=edunovs2019>[EdunovS2019]<a class=headerlink href=#edunovs2019 title="Permanent link">&para;</a></h3> <p>Pre-trained language model representations for language generation. In Proceedings ofthe 2019 Conference ofthe North American Chapter ofthe Association for Computational Linguistics. 4052–4059.</p> <h3 id=narayans2018>[NarayanS2018]<a class=headerlink href=#narayans2018 title="Permanent link">&para;</a></h3> <p>Pretraining-Based Natural Language Generation for Text Summarization. In Proceedings ofthe 2018 Conference on Empirical Methods in Natural Language Processing. 1797–1807.</p> <h3 id=peterj2017>[PeterJ2017]<a class=headerlink href=#peterj2017 title="Permanent link">&para;</a></h3> <p>Get to the point: Summarization with pointer-generator networks. arXiv:1704.04368 (2017).</p> <h3 id=guptav2010>[GuptaV2010]<a class=headerlink href=#guptav2010 title="Permanent link">&para;</a></h3> <p>A Survey of Text Summarization Extractive Techniques. Journal ofEmerging Technologies in Web Intelligence 2, 3 (2010), 258–268.</p> <h3 id=sanhv2019>[SanhV2019]<a class=headerlink href=#sanhv2019 title="Permanent link">&para;</a></h3> <p>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arxiv.org/pdf/1910.01108 (2019).</p> <h3 id=liuy2019>[LiuY2019]<a class=headerlink href=#liuy2019 title="Permanent link">&para;</a></h3> <p>Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692 (2019).</p> <h3 id=yany2020>[YanY2020]<a class=headerlink href=#yany2020 title="Permanent link">&para;</a></h3> <p>ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training. arXiv:2001.04063 (2020).</p> <h3 id=daiz>[DaiZ]<a class=headerlink href=#daiz title="Permanent link">&para;</a></h3> <p>Transformer- XL: Attentive Language Models beyond a Fixed-Length Context. In Proceedings ofthe 57<sup>th</sup> Annual Meeting ofthe Association for Computational Linguistics. 2978–2988.</p> <h3 id=lanz2019>[LanZ2019]<a class=headerlink href=#lanz2019 title="Permanent link">&para;</a></h3> <p>Albert: A lite bert for self-supervised learning of language representations. arXiv:1909.11942 (2019).</p> <h3 id=yangz2019>[YangZ2019]<a class=headerlink href=#yangz2019 title="Permanent link">&para;</a></h3> <p>Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems (2019), 5754–5764.</p> <blockquote> <p><strong>CTRL: A Conditional Transformer Language Model For Controllable Generation</strong></p> </blockquote> <h3 id=mart2016>[Mart2016]<a class=headerlink href=#mart2016 title="Permanent link">&para;</a></h3> <p>Tensorflow: A system for large-scale machine learning. In 12<sup>th</sup> {USENIX} Symposium on Operating Systems Design and Imple-mentation ({OSDI} 16), pp. 265–283, 2016.</p> <h3 id=rohan2019>[Rohan2019]<a class=headerlink href=#rohan2019 title="Permanent link">&para;</a></h3> <p>Memory-efficient adaptive optimiza-tion for large-scale learning. arXiv preprint arXiv:1901.11150, 2019.</p> <h3 id=martin2017>[Martin2017]<a class=headerlink href=#martin2017 title="Permanent link">&para;</a></h3> <p>Wasserstein generative adversarial networks. ´In International conference on machine learning, pp. 214–223, 2017.</p> <h3 id=matthew2017>[Matthew2017]<a class=headerlink href=#matthew2017 title="Permanent link">&para;</a></h3> <p>Factsheets: Increasing trust in AI servicesthrough supplier’s declarations of conformity, August 2018. arXiv:1808.07261 [cs.CY].Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machinetranslation. arXiv preprint arXiv:1710.11041, 2017.</p> <h3 id=jimmy2016>[Jimmy2016]<a class=headerlink href=#jimmy2016 title="Permanent link">&para;</a></h3> <p>Layer normalization. CoRR, abs/1607.06450,2016.</p> <h3 id=lo2019>[Lo2019]<a class=headerlink href=#lo2019 title="Permanent link">&para;</a></h3> <p>Findings of the2019 conference on machine translation (wmt19). In Proceedings of the Fourth Conference onMachine Translation (Volume 2: Shared Task Papers, Day 1), pp. 1–61, 2019.</p> <h3 id=yoshua2003>[Yoshua2003]<a class=headerlink href=#yoshua2003 title="Permanent link">&para;</a></h3> <p>A neural probabilistic ´language model. Journal of machine learning research, 3(Feb):1137–1155, 2003.</p> <h3 id=thorsten2007>[Thorsten2007]<a class=headerlink href=#thorsten2007 title="Permanent link">&para;</a></h3> <p>Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods inNatural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),pp. 858–867, 2007.</p> <h3 id=miles2016>[Miles2016]<a class=headerlink href=#miles2016 title="Permanent link">&para;</a></h3> <p>Artificial intelligence and responsible innovation. In Vincent C. Muller (ed.), ¨Fundamental Issues of Artificial Intelligence, pp. 543–554. Springer, 2016.</p> <h3 id=miles2019>[Miles2019]<a class=headerlink href=#miles2019 title="Permanent link">&para;</a></h3> <p>The malicious use of artificial intelligence: Forecasting,prevention, and mitigation, February 2019. arXiv:1802.07228 [cs.AI].Isaac Caswell, Ciprian Chelba, and David Grangier. Tagged back-translation. arXiv preprintarXiv:1906.06442, 2019.</p> <h3 id=xi2016>[Xi2016]<a class=headerlink href=#xi2016 title="Permanent link">&para;</a></h3> <p>Infogan:Interpretable representation learning by information maximizing generative adversarial nets. InAdvances in neural information processing systems, pp. 2172–2180, 2016.</p> <h3 id=rewon2019>[Rewon2019]<a class=headerlink href=#rewon2019 title="Permanent link">&para;</a></h3> <p>Generating long sequences with sparsetransformers. arXiv preprint arXiv:1904.10509, 2019.</p> <h3 id=ronan2008>[Ronan2008]<a class=headerlink href=#ronan2008 title="Permanent link">&para;</a></h3> <p>A unified architecture for natural language processing: Deepneural networks with multitask learning. In Proceedings of the 25<sup>th</sup> international conference onMachine learning, pp. 160–167. ACM, 2008.</p> <h3 id=ronan2011>[Ronan2011]<a class=headerlink href=#ronan2011 title="Permanent link">&para;</a></h3> <p>Natural language processing (almost) from scratch. Journal of machine learning research,12(Aug):2493–2537, 2011.</p> <h3 id=ruth1987>[Ruth1987]<a class=headerlink href=#ruth1987 title="Permanent link">&para;</a></h3> <p>The consumption junction: A proposal for research strategies in the sociol-ogy of technology. In Wiebe E. Bijker, Thomas P. Hughes, and Trevor J. Pinch (eds.), The SocialConstruction of Technological Systems, pp. 261–280. MIT Press, Cambridge, MA, USA, 1987.</p> <h3 id=andrew2015>[Andrew2015]<a class=headerlink href=#andrew2015 title="Permanent link">&para;</a></h3> <p>Semi-supervised sequence learning. In Advances in neural infor-mation processing systems, pp. 3079–3087, 2015.</p> <h3 id=zihang2019>[Zihang2019]<a class=headerlink href=#zihang2019 title="Permanent link">&para;</a></h3> <p>Transformer-xl: Attentive language models beyond a fixed-length context. arXivpreprint arXiv:1901.02860, 2019.</p> <h3 id=jacob2018>[Jacob2018]<a class=headerlink href=#jacob2018 title="Permanent link">&para;</a></h3> <p>Bert: Pre-training of deepbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p> <h3 id=john2011>[John2011]<a class=headerlink href=#john2011 title="Permanent link">&para;</a></h3> <p>Adaptive subgradient methods for online learning andstochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.</p> <h3 id=matthew2017_1>[Matthew2017]<a class=headerlink href=#matthew2017_1 title="Permanent link">&para;</a></h3> <p>Searchqa: A new q&amp;a dataset augmented with context from a search engine. arXiv preprintarXiv:1704.05179, 2017.</p> <h3 id=angela2018>[Angela2018]<a class=headerlink href=#angela2018 title="Permanent link">&para;</a></h3> <p>Hierarchical neural story generation. arXiv preprintarXiv:1805.04833, 2018.</p> <h3 id=angela2019>[Angela2019]<a class=headerlink href=#angela2019 title="Permanent link">&para;</a></h3> <p>Eli5:Long form question answering. arXiv preprint arXiv:1907.09190, 2019.</p> <h3 id=boris2019>[Boris2019]<a class=headerlink href=#boris2019 title="Permanent link">&para;</a></h3> <p>Stochastic gradient methods with layer-wise adaptive moments for training of deep networks. arXiv preprint arXiv:1905.11286, 2019.</p> <h3 id=ian2014>[Ian2014]<a class=headerlink href=#ian2014 title="Permanent link">&para;</a></h3> <p>Generative adversarial nets. In Advances in neural infor-mation processing systems, pp. 2672–2680, 2014.</p> <h3 id=max2016>[Max2016]<a class=headerlink href=#max2016 title="Permanent link">&para;</a></h3> <p>Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies, pp.708–719, New Orleans, Louisiana, June 2018.</p> <h3 id=kaiming2016>[Kaiming2016]<a class=headerlink href=#kaiming2016 title="Permanent link">&para;</a></h3> <p>Deep residual learning for image recog-nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.770–778, 2016.</p> <h3 id=karl2015>[Karl2015]<a class=headerlink href=#karl2015 title="Permanent link">&para;</a></h3> <p>Teaching machines to read and comprehend. In Advances inneural information processing systems, pp. 1693–1701, 2015.</p> <h3 id=ari2019>[Ari2019]<a class=headerlink href=#ari2019 title="Permanent link">&para;</a></h3> <p>The curious case of neural text degener-ation. arXiv preprint arXiv:1904.09751, 2019.</p> <h3 id=jeremy2018>[Jeremy2018]<a class=headerlink href=#jeremy2018 title="Permanent link">&para;</a></h3> <p>Universal language model fine-tuning for text classification.arXiv preprint arXiv:1801.06146, 2018.</p> <h3 id=hakan2016>[Hakan2016]<a class=headerlink href=#hakan2016 title="Permanent link">&para;</a></h3> <p>Tying word vectors and word classifiers: Aloss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.</p> <h3 id=melvin2017>[Melvin2017]<a class=headerlink href=#melvin2017 title="Permanent link">&para;</a></h3> <p>Googles multilingual neural ´machine translation system: Enabling zero-shot translation. Transactions of the Association forComputational Linguistics, 5:339–351, 2017.</p> <h3 id=mandar2017>[Mandar2017]<a class=headerlink href=#mandar2017 title="Permanent link">&para;</a></h3> <p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.</p> <h3 id=david2017>[David2017]<a class=headerlink href=#david2017 title="Permanent link">&para;</a></h3> <p>Self-censorship is not enough. Nature, 492(7429):345–347,December 2012. doi: 10.1038/492345a.</p> <h3 id=lukasz2017>[Lukasz2017]<a class=headerlink href=#lukasz2017 title="Permanent link">&para;</a></h3> <p>One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.</p> <h3 id=ukasz2018>[Łukasz2018]<a class=headerlink href=#ukasz2018 title="Permanent link">&para;</a></h3> <p>Fast decoding in sequence models using discrete latent variables. arXiv preprintarXiv:1803.03382, 2018.</p> <h3 id=nitish2019>[Nitish2019]<a class=headerlink href=#nitish2019 title="Permanent link">&para;</a></h3> <p>Unifying questionanswering and text classification via span extraction. arXiv preprint arXiv:1904.09286, 2019.</p> <h3 id=diederik2014>[Diederik2014]<a class=headerlink href=#diederik2014 title="Permanent link">&para;</a></h3> <p>Adam: A method for stochastic optimization. arXiv preprintarXiv:1412.6980, 2014.</p> <h3 id=diederik2013>[Diederik2013]<a class=headerlink href=#diederik2013 title="Permanent link">&para;</a></h3> <p>Auto-encoding variational bayes. arXiv preprintarXiv:1312.6114, 2013.</p> <h3 id=ryan2015>[Ryan2015]<a class=headerlink href=#ryan2015 title="Permanent link">&para;</a></h3> <p>Skip-thought vectors. In Advances in neural information processingsystems, pp. 3294–3302, 2015.</p> <h3 id=catherine2016>[Catherine2016]<a class=headerlink href=#catherine2016 title="Permanent link">&para;</a></h3> <p>Senellart. Domain control for neural machine translation.arXiv preprint arXiv:1612.06140, 2016.</p> <h3 id=wojciech2019>[Wojciech2019]<a class=headerlink href=#wojciech2019 title="Permanent link">&para;</a></h3> <p>Neural text summarization: A critical evaluation. arXiv preprint arXiv:1908.08960, 2019.</p> <h3 id=tom2019>[Tom2019]<a class=headerlink href=#tom2019 title="Permanent link">&para;</a></h3> <p>Natural questions: abenchmark for question answering research. Transactions of the Association for ComputationalLinguistics, 7:453–466, 2019.</p> <h3 id=guillaume2019>[Guillaume2019]<a class=headerlink href=#guillaume2019 title="Permanent link">&para;</a></h3> <p>Cross-lingual language model pretraining. arXiv preprintarXiv:1901.07291, 2019.</p> <h3 id=guillaume2019_1>[Guillaume2019]<a class=headerlink href=#guillaume2019_1 title="Permanent link">&para;</a></h3> <p>Large memory layers with product keys. ´ arXiv preprint arXiv:1907.05242, 2019.</p> <h3 id=hector2012>[Hector2012]<a class=headerlink href=#hector2012 title="Permanent link">&para;</a></h3> <p>The winograd schema challenge. In Thir-teenth International Conference on the Principles of Knowledge Representation and Reasoning,2012.</p> <h3 id=patrick2019>[Patrick2019]<a class=headerlink href=#patrick2019 title="Permanent link">&para;</a></h3> <p>Unsupervised question answering by clozetranslation. arXiv preprint arXiv:1906.04980, 2019.</p> <h3 id=minh-thang2015>[Minh-Thang2015]<a class=headerlink href=#minh-thang2015 title="Permanent link">&para;</a></h3> <p>Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.</p> <h3 id=julian2015>[Julian2015]<a class=headerlink href=#julian2015 title="Permanent link">&para;</a></h3> <p>Image-based rec-ommendations on styles and substitutes. In Proceedings of the 38<sup>th</sup> International ACM SIGIRConference on Research and Development in Information Retrieval, pp. 43–52. ACM, 2015.</p> <h3 id=bryan6294>[Bryan6294]<a class=headerlink href=#bryan6294 title="Permanent link">&para;</a></h3> <p>Learned in translation:Contextualized word vectors. In Advances in Neural Information Processing Systems, pp. 6294.</p> <h3 id=bryan2018>[Bryan2018]<a class=headerlink href=#bryan2018 title="Permanent link">&para;</a></h3> <p>The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.</p> <h3 id=15stephen2017>[15Stephen2017]<a class=headerlink href=#15stephen2017 title="Permanent link">&para;</a></h3> <p>Regularizing and optimizing lstm lan-guage models. arXiv preprint arXiv:1708.02182, 2017.</p> <h3 id=tomas2013>[Tomas2013]<a class=headerlink href=#tomas2013 title="Permanent link">&para;</a></h3> <p>Distributed represen-tations of words and phrases and their compositionality. In Advances in neural information pro-cessing systems, pp. 3111–3119, 2013.</p> <h3 id=margaret7596>[Margaret7596]<a class=headerlink href=#margaret7596 title="Permanent link">&para;</a></h3> <p>Model cards for model reporting. InProceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ’19), Jan-uary 2019. doi: 10.1145/3287560.3287596.</p> <h3 id=amit2019>[Amit2019]<a class=headerlink href=#amit2019 title="Permanent link">&para;</a></h3> <p>Filling gender &amp; number gaps in neural ma-chine translation with black-box context injection. arXiv preprint arXiv:1903.03467, 2019.</p> <h3 id=vinod2010>[Vinod2010]<a class=headerlink href=#vinod2010 title="Permanent link">&para;</a></h3> <p>Rectified linear units improve restricted boltzmann machines. InProceedings of the 27<sup>th</sup> International Conference on Machine Learning (ICML-10), pp. 807–814,2010.</p> <h3 id=ramesh2016>[Ramesh2016]<a class=headerlink href=#ramesh2016 title="Permanent link">&para;</a></h3> <p>Abstractive text summarizationusing sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016.</p> <h3 id=matthew2018>[Matthew2018]<a class=headerlink href=#matthew2018 title="Permanent link">&para;</a></h3> <p>Deep contextualized word representations. arXiv preprint arXiv:1802.05365,2018.</p> <h3 id=carol1979>[Carol1979]<a class=headerlink href=#carol1979 title="Permanent link">&para;</a></h3> <p>Constraints on language mixing: intra sentential code-switching and borrowing inspanish/english. Language, pp. 291–318, 1979.</p> <h3 id=shana1980>[Shana1980]<a class=headerlink href=#shana1980 title="Permanent link">&para;</a></h3> <p>Sometimes ill start a sentence in spanish y termino en espanol: toward a typologyof code-switching1. Linguistics, 18(7-8):581–618, 1980.</p> <h3 id=ofir2016>[Ofir2016]<a class=headerlink href=#ofir2016 title="Permanent link">&para;</a></h3> <p>Using the output embedding to improve language models. arXiv preprintarXiv:1608.05859, 2016.</p> <h3 id=alec2018>[Alec2018]<a class=headerlink href=#alec2018 title="Permanent link">&para;</a></h3> <p>Improving language under-standing by generative pre-training. URL <a href=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/language>https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/language</a> understanding paper.pdf, 2018.</p> <h3 id=alec2019>[Alec2019]<a class=headerlink href=#alec2019 title="Permanent link">&para;</a></h3> <p>Language models are unsupervised multitask learners. URLhttps://d4mucfpksywv.cloudfront.net/better-language-models/language models are unsupervised multitask learners.pdf, 2019.</p> <h3 id=nazneen2019>[Nazneen2019]<a class=headerlink href=#nazneen2019 title="Permanent link">&para;</a></h3> <p>Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019.</p> <h3 id=pranav2016>[Pranav2016]<a class=headerlink href=#pranav2016 title="Permanent link">&para;</a></h3> <p>Squad: 100,000+ questionsfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.</p> <h3 id=alexander2015>[Alexander2015]<a class=headerlink href=#alexander2015 title="Permanent link">&para;</a></h3> <p>A neural attention model for abstractivesentence summarization. arXiv preprint arXiv:1509.00685, 2015.</p> <h3 id=evan2008>[Evan2008]<a class=headerlink href=#evan2008 title="Permanent link">&para;</a></h3> <p>The new york times annotated corpus. Linguistic Data Consortium, Philadelphia,6(12):e26752, 2008.</p> <h3 id=thomas2019>[Thomas2019]<a class=headerlink href=#thomas2019 title="Permanent link">&para;</a></h3> <p>Answers unite!unsupervised metrics for reinforced summarization models. arXiv preprint arXiv:1909.01610,2019.</p> <h3 id=abigail2017>[Abigail2017]<a class=headerlink href=#abigail2017 title="Permanent link">&para;</a></h3> <p>Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55<sup>th</sup> Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), volume 1, pp. 1073–1083, 2017.</p> <h3 id=rico2015>[Rico2015]<a class=headerlink href=#rico2015 title="Permanent link">&para;</a></h3> <p>Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.</p> <h3 id=noam2018>[Noam2018]<a class=headerlink href=#noam2018 title="Permanent link">&para;</a></h3> <p>Adafactor: Adaptive learning rates with sublinear memory cost.arXiv preprint arXiv:1804.04235, 2018.</p> <h3 id=jack008>[Jack008]<a class=headerlink href=#jack008 title="Permanent link">&para;</a></h3> <p>Developing a framework for responsible inno-vation. Research Policy, 42(9):1568–1580, November 2013. doi: 10.1016/j.respol.2013.05.008.</p> <h3 id=ilya2014>[Ilya2014]<a class=headerlink href=#ilya2014 title="Permanent link">&para;</a></h3> <p>Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014.</p> <h3 id=trieu2018>[Trieu2018]<a class=headerlink href=#trieu2018 title="Permanent link">&para;</a></h3> <p>A simple method for commonsense reasoning. arXiv preprintarXiv:1806.02847, 2018.</p> <h3 id=adam2016>[Adam2016]<a class=headerlink href=#adam2016 title="Permanent link">&para;</a></h3> <p>A machine comprehension dataset. arXiv preprint arXiv:1611.09830,2016.</p> <h3 id=lav6008>[Lav6008]<a class=headerlink href=#lav6008 title="Permanent link">&para;</a></h3> <p>Pretrained AI models: Performativity,mobility, and change, September 2019. arXiv:1909.03290 [cs.CY].</p> <h3 id=curran2018>[Curran2018]<a class=headerlink href=#curran2018 title="Permanent link">&para;</a></h3> <p>Glue:A multi-task benchmark and analysis platform for natural language understanding. arXiv preprintarXiv:1804.07461, 2018.</p> <h3 id=sean2019>[Sean2019]<a class=headerlink href=#sean2019 title="Permanent link">&para;</a></h3> <p>Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.</p> <h3 id=yonghui2016>[Yonghui2016]<a class=headerlink href=#yonghui2016 title="Permanent link">&para;</a></h3> <p>Google’s neural machine trans-lation system: Bridging the gap between human and machine translation. arXiv preprintarXiv:1609.08144, 2016.</p> <h3 id=stratos2019>[Stratos2019]<a class=headerlink href=#stratos2019 title="Permanent link">&para;</a></h3> <p>Sumqe: a bert-based summary quality estimation model. arXiv preprint arXiv:1909.00578, 2019.</p> <h3 id=zhilin2018>[Zhilin2018]<a class=headerlink href=#zhilin2018 title="Permanent link">&para;</a></h3> <p>Hotpotqa: A dataset for diverse, explainable multi-hop questionanswering. arXiv preprint arXiv:1809.09600, 2018.</p> <h3 id=rowan-2019>[Rowan 2019]<a class=headerlink href=#rowan-2019 title="Permanent link">&para;</a></h3> <p>Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019</p> <h3 id=fangxiaoyu2022>[Fangxiaoyu2022]<a class=headerlink href=#fangxiaoyu2022 title="Permanent link">&para;</a></h3> <p><a href=https://arxiv.org/abs/2007.01852>Language-agnostic BERT Sentence Embedding - LaBSE</a> - BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning - BERT based cross-lingual sentence embeddings is explored in this paper. - It explored combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM) - Introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance - It produces a model that achieves high bi-text retrieval accuracy over 112 languages </p> <h2 id=nlp-papers-available-on-my-google-drive>NLP Papers Available on my Google Drive<a class=headerlink href=#nlp-papers-available-on-my-google-drive title="Permanent link">&para;</a></h2> <p>You can download these papers from <a href="https://drive.google.com/drive/folders/1588WNZVCTRbOnsrSVKousaky7A59mfl9?usp=sharing">link</a></p> <ol> <li>A brief introduction to boosting.pdf</li> <li>A Closer Look at Fermentors and Bioreactors.pdf</li> <li>A Comprehensive Survey on Graph Neural Networks.pdf</li> <li>A Corpus of English-Hindi Code-Mixed Tweets for Sarcasm Detection.pdf</li> <li>A dataset for detecting irony in Hindi-english code-mixed social media text.pdf</li> <li>A Framework for Document Specific Error Detection and Corrections in Indic OCR.pdf</li> <li>A lexicon-based approach for hate speech detection.pdf</li> <li>A method for multi-class sentiment classification based on an improved one-vs-one (OVO) strategy and the support vector machine (.pdf</li> <li>A novel automatic satire and irony detection using ensembled feature selection and data mining.pdf</li> <li>A Pragmatic Analysis Of Humor In Modern Family.pdf</li> <li>A Selective Overview of Deep Learning.pdf</li> <li>A Sentiment Analyzer for Hindi Using Hindi Senti Lexicon.pdf</li> <li>A Survey of Code-switched Speech and Language Processing.pdf</li> <li>A Survey of the State of Explainable AI for Natural Language Processing.pdf</li> <li>A Survey on Explainable Artificial Intelligence (XAI) Toward Medical XAI.pdf</li> <li>A TENGRAM method based part-of-speech tagging of multi-category words in Hindi language.pdf</li> <li>A transformer-based approach to irony and sarcasm detection.pdf</li> <li>A2Text-net A novel deep neural network for sarcasm detection.pdf</li> <li>Adaptive glove and fasttext model for Hindi word embeddings.pdf</li> <li>AI and Ethics - Operationalising Responsible AI-PAPER.pdf</li> <li>AI4Bharat-IndicNLP Corpus Monolingual Corpora and Word Embeddings for Indic Languages.pdf</li> <li>ALBERT A Lite BERT for Self-supervised Learning of Language Representations.pdf</li> <li>an Analysis of Current Trends for Sanskrit As a Computer Programming Language.pdf</li> <li>An empirical, quantitative analysis of the differences between sarcasm and Irony.pdf</li> <li>An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf</li> <li>Analyzing_The_Expressive_Power_Of_Graph.pdf</li> <li>AnnCorra Annotating Corpora Guidelines For POS And Chunk Annotation For Indian Languages.pdf</li> <li>Approaches to Cross-Domain Sentiment Analysis A Systematic Literature Review.pdf</li> <li>Attention is all you need.pdf</li> <li>Automatic sarcasm detection A survey.pdf</li> <li>Automatic satire detection Are you having a laugh.pdf</li> <li>Bag of tricks for efficient text classification.pdf</li> <li>Baselines and bigrams Simple, good sentiment and topic classification.pdf</li> <li>BERT Explained - A list of Frequently Asked Questions.pdf</li> <li>BERT Pre-training of deep bidirectional transformers for language understanding.pdf</li> <li>BHAAV- A Text Corpus for Emotion Analysis from Hindi Stories.pdf</li> <li>Carer Contextualized affect representations for emotion recognition.pdf</li> <li>CASCADE Contextual Sarcasm Detection in Online Discussion Forums.pdf</li> <li>Challenges in Deploying Machine Learning a Survey of Case Studies.pdf</li> <li>Clinical artificial intelligence quality improvement towards continual monitoring and updating of AI algorithms in healthcare.pdf</li> <li>CLUE based load balancing in replicated web server.pdf</li> <li>Clues for detecting irony in user-generated contents Oh...!! it_s so easy -).pdf</li> <li>Code Mixing A Challenge for Language Identification in the Language of Social Media.pdf</li> <li>Context-based Sarcasm Detection in Hindi Tweets.pdf</li> <li>Contextualized sarcasm detection on twitter.pdf</li> <li>Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis.pdf</li> <li>Data governance A conceptual framework, structured review, and research agenda.pdf</li> <li>Deep and Dense Sarcasm Detection.pdf</li> <li>Deep learning based unsupervised POS tagging for Sanskrit.pdf</li> <li>Detailed human avatars from monocular video.pdf</li> <li>Detecting Sarcasm is Extremely Easy -).pdf</li> <li>DIALOGPT Large-Scale Generative Pre-training for Conversational Response Generation.pdf</li> <li>DistilBERT, a distilled version of BERT smaller, faster, cheaper and lighter.pdf</li> <li>DRIFT Deep Reinforcement Learning for Functional Software Testing.pdf</li> <li>Drop A reading comprehension benchmark requiring discrete reasoning over paragraphs.pdf</li> <li>Dynamic routing between capsules.pdf</li> <li>Effect of speech coding on speaker identification.pdf</li> <li>Efficient estimation of word representations in vector space(2).pdf</li> <li>ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators.pdf</li> <li>Embedding Words as Distributions with a Bayesian Skip-gram Model.pdf</li> <li>Enriching Word Vectors with Subword Information.pdf</li> <li>Experience Grounds Language.pdf</li> <li>Exploiting emojis for sarcasm detection.pdf</li> <li>Exploiting Similarities among Languages for Machine Translation.pdf</li> <li>Exploring the fine-grained analysis and automatic detection of irony on Twitter(2).pdf</li> <li>Exploring the fine-grained analysis and automatic detection of irony on Twitter.pdf</li> <li>Exploring the impact of pragmatic phenomena on irony detection in tweets A multilingual corpus study.pdf</li> <li>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf</li> <li>Extensions to HMM-based Statistical Word Alignment Models.pdf</li> <li>Fairness_In_Machine_Learning_A_Survey.pdf</li> <li>Fake news detection of Indian and United States election data using machine learning algorithm.pdf</li> <li>Fake News Detection on Social Media.pdf</li> <li>FakeNewsNet A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social.pdf</li> <li>Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf</li> <li>FastText.zip Compressing text classification models.pdf</li> <li>Figurative messages and affect in Twitter Differences between #irony, #sarcasm and #not.pdf</li> <li>Forecasting COVID-19 Confirmed Cases in Major Indian Cities and Their Connectedness with Mobility and Weather-related Parameters.pdf</li> <li>From English To Foreign Languages Transferring Pre-trained Language Models.pdf</li> <li>FROM Pre-trained Word Embeddings TO Pre-trained Language Models - Focus on BERT.pdf</li> <li>Going deeper with convolutions.pdf</li> <li>Graph Machine Learning NeurIPS 2020 Papers.pdf</li> <li>Grouped Convolutional Neural Networks for Multivariate Time Series.pdf</li> <li>Grouped Functional Time Series Forecasting An Application to Age-Specific Mortality Rates.pdf</li> <li>Handbook of approximation algorithms and metaheuristics.pdf</li> <li>Harnessing context incongruity for sarcasm detection.pdf</li> <li>Harnessing Online News for Sarcasm Detection in Hindi Tweets.pdf</li> <li>Hidden Markov Models.pdf</li> <li>Hidden technical debt in machine learning systems.pdf</li> <li>Hotpotqa A dataset for diverse, explainable multi-hop question answering.pdf</li> <li>How multilingual is multilingual BERT.pdf</li> <li>How to avoid machine learning pitfalls a guide for academic researchers.pdf</li> <li>How to read a paper.pdf</li> <li>HuggingFace_s Transformers State-of-the-art Natural Language Processing.pdf</li> <li>Identifying machine learning techniques for classification of target advertising.pdf</li> <li>Identifying sarcasm in Twitter A closer look.pdf</li> <li>Improving Language Understanding by Generative Pre-Training.pdf</li> <li>Improving the learnability of classifiers for Sanskrit OCR corrections.pdf</li> <li>Indic sentiReview Natural language processing based sentiment analysis on major indian languages.pdf</li> <li>Interactive-and-Visual-Prompt-Engineering-for-adhoc-Task-Adaptation-LLM.pdf</li> <li>Investigations in computational sarcasm.pdf</li> <li>Irony detection in twitter The role of affective content.pdf</li> <li>Irony, Sarcasm and Parody in the American Sitcom Modern Family.pdf</li> <li>iSarcasm A Dataset of Intended Sarcasm.pdf</li> <li>K-means with Three different Distance Metrics.pdf</li> <li>Knowledge Representation in Sanskrit and Artificial Intelligence.pdf</li> <li>Learning Graph Search Heuristics.pdf</li> <li>Learning latent causal graphs via mixture oracles.pdf</li> <li>LearningSys_2015_paper_32.pdf</li> <li>Lexicon-Based Methods for Sentiment Analysis.pdf</li> <li>Lexicon-Based Sentiment Analysis in the Social Web.pdf</li> <li>LightGBM A highly efficient gradient boosting decision tree.pdf</li> <li>Linguistic Inquiry and Word Count LIWC2015.pdf</li> <li>Machine Learning in Automated Text Categorization.pdf</li> <li>Machine Learning within a Graph Database A Case Study on Link Prediction for Scholarly Data.pdf</li> <li>Machine Translation Approaches and Survey for Indian Languages.pdf</li> <li>Machine Translation of Bi-lingual Hindi-English (Hinglish) Text.pdf</li> <li>Merlion A Machine Learning Library for Time Series.pdf</li> <li>Mining of Massive Datasets.pdf</li> <li>MLP-Mixer An all-MLP Architecture for Vision.pdf</li> <li>Multi-modal sarcasm detection in Twitter with hierarchical fusion model.pdf</li> <li>Multi-rule based ensemble feature selection model for sarcasm type detection in Twitter.pdf</li> <li>Multimodal markers of irony and sarcasm.pdf</li> <li>N Atural L Anguage I Nference Over.pdf</li> <li>Natural Language Processing - A Panian Perspective.pdf</li> <li>Natural language processing based features for sarcasm detection An investigation using bilingual social media texts.pdf</li> <li>NeuralProphet Explainable Forecasting at Scale.pdf</li> <li>On State-of-the-art of POS Tagger, Sandhi Splitter, Alankaar Finder and Samaas Finder for IndoAryan and Dravidian Languages.pdf</li> <li>Opinion mining and sentiment analysis.pdf</li> <li>Opinion-Based Entity Ranking (Author_s Draft).pdf</li> <li>Part-of-speech tagging from 97_ to 100_ Is it time for some linguistics.pdf</li> <li>PAVE Lazy-MDP based Ensemble to Improve Recall of Product Attribute Extraction Models.pdf</li> <li>Real-time Sentiment Analysis of Hindi Tweets.pdf</li> <li>Reasoning with sarcasm by reading in-between.pdf</li> <li>Recent trends in deep learning based natural language processing Review Article.pdf</li> <li>RECEPTIVE FIELDS OF SINGLE NEURONES IN THE CAT _ S STRIATE CORTEX.pdf</li> <li>Recognition of consonant-vowel (CV) units under background noise using combined temporal and spectral preprocessing.pdf</li> <li>Representing social media users for sarcasm detection.pdf</li> <li>Retrospective Reader for Machine Reading Comprehension.pdf</li> <li>RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf</li> <li>Robotics , AI , and.pdf</li> <li>ROC graphs Notes and practical considerations for researchers.pdf</li> <li>Sanskrit sandhi splitting using Seq2(Seq)22.pdf</li> <li>Sanskrit word segmentation using character-level recurrent and convolutional neural networks.pdf</li> <li>Sarc-M Sarcasm Detection in Typo-graphic Memes.pdf</li> <li>Sarcasm as contrast between a positive sentiment and negative situation.pdf</li> <li>Sarcasm Detection in Hindi sentences using Support Vector machine.pdf</li> <li>Sarcasm detection in tweets.pdf</li> <li>Sarcasm detection on twitterA behavioral modeling approach.pdf</li> <li>Sarcastic sentiment detection in tweets streamed in real time a big data approach.pdf</li> <li>Scalable linear algebra on a relational database system.pdf</li> <li>Scaling Large Production Clusters with Partitioned Synchronization This paper is included in the Proceedings of the.pdf</li> <li>Semantics-Aware BERT for Language Understanding.pdf</li> <li>Semi-supervised recognition of sarcastic sentences in twitter and Amazon.pdf</li> <li>SentencePiece A simple and language independent subword tokenizer and detokenizer for neural text processing.pdf</li> <li>Sentiment Analysis for Hindi Language.pdf</li> <li>Sentiment Analysis in a Resource Scarce LanguageHindi.pdf</li> <li>Sentiment Analysis In Hindi.pdf</li> <li>Sentiment Analysis in Indian languages o Definition.pdf</li> <li>Sentiment Analysis of Hindi Review based on Negation and Discourse Relation.pdf</li> <li>Sentiment classification using machine learning techniques with syntax features.pdf</li> <li>Skillful writing of an awful research paper.pdf</li> <li>Social media and fake news in the 2016 election.pdf</li> <li>Sound classification using convolutional neural network and tensor deep stacking network.pdf</li> <li>Sparse, contextually informed models for irony detection Exploiting user communities, entities and sentiment.pdf</li> <li>SQuad 100,000 questions for machine comprehension of text.pdf</li> <li>ST4_Method_Random_Forest.pdf</li> <li>Statistical Methods in Natural Language Processing.pdf</li> <li>StructBERT Incorporating Language Structures into Pre-training for Deep Language Understanding.pdf</li> <li>Structural S tudies on S mall A myloid O ligomers RT-6.pdf</li> <li>Superintelligence.pdf</li> <li>Systematic literature review of sentiment analysis on Twitter using soft computing techniques.pdf</li> <li>Text categorization with support vector machines Learning with many relevant features.pdf</li> <li>Text normalization of code mix and sentiment analysis.pdf</li> <li>The Differential Role of Ridicule in Sarcasm and Irony The Differential Role of Ridicule in Sarcasm and Irony.pdf</li> <li>The highest form of intelligence Sarcasm increases creativity for both expressers and recipients.pdf</li> <li>The Modern Mathematics of Deep Learning *.pdf</li> <li>The Paninian approach to natural language processing.pdf</li> <li>The perfect solution for detecting sarcasm in tweets #not.pdf</li> <li>Thumbs Up or Thumbs Down Semantic Orientation Applied to Unsupervised Classification of Reviews.pdf</li> <li>THU_NGN at SemEval-2018 Task 3 Tweet Irony Detection with Densely connected LSTM and Multi-task Learning.pdf</li> <li>TnT - A Statistical Part-of-Speech Tagger.pdf</li> <li>To BLOB or Not To BLOB Large Object Storage in a Database or a Filesystem To BLOB or Not To BLOB Large Object Storage in a Dat.pdf</li> <li>Towards Demystifying Serverless Machine Learning Training.pdf</li> <li>Towards multimodal sarcasm detection (an obviously perfect paper).pdf</li> <li>Towards sub-word level compositions for sentiment analysis of Hindi-English code mixed text.pdf</li> <li>Triple-View Feature Learning for Medical Image Segmentation.pdf</li> <li>Twitter as a corpus for sentiment analysis and opinion mining.pdf</li> <li>Two improved continuous bag-of-word models.pdf</li> <li>Understanding Diffusion Models A Unified Perspective Introduction Generative Models.pdf</li> <li>Universal Sentence Encoder.pdf</li> <li>Unsupervised Irony Detection A Probabilistic Model with Word Embeddings.pdf</li> <li>UR-Funny A multimodal language dataset for understanding humor.pdf</li> <li>Use of Sanskrit for natural language processing.pdf</li> <li>Using TF-IDF to Determine Word Relevance in Document Queries.pdf</li> <li>Using Word Embeddings for Query Translation for Hindi to English Cross Language Information Retrieval.pdf</li> <li>Very deep convolutional networks for large-scale image recognition.pdf</li> <li>We are IntechOpen , the world ' s leading publisher of Open Access books Built by scientists , for scientists TOP 1 _.pdf</li> <li>When BERT Plays the Lottery, All Tickets Are Winning.pdf</li> <li>XGBoost A scalable tree boosting system.pdf</li> <li>XLNet Generalized Autoregressive Pretraining for Language Understanding.pdf</li> </ol> <h2 id=ai-papers-available-on-my-google-drive>AI Papers Available on my Google Drive<a class=headerlink href=#ai-papers-available-on-my-google-drive title="Permanent link">&para;</a></h2> <p>You can download these papers from <a href="https://drive.google.com/drive/folders/1_dMdWy_qytFEDs2JQr4ecSs92T1RG_Ju?usp=sharing">link</a></p> <ol> <li>A Comprehensive Survey on Graph Neural Networks-PAPER.pdf</li> <li>A machine learning approach to predicting psychosis-PAPER.pdf</li> <li>A Selective Overview of Deep Learning-PAPER.pdf</li> <li>A Short introduction to boosting-PAPER.pdf</li> <li>A Survey of the State of Explainable AI for NLP-PAPER.pdf</li> <li>A Survey on Explainable AI (XAI) towards Medical XAI-PAPER.pdf</li> <li>AI and Ethics - Operationalising Responsible AI-PAPER.pdf</li> <li>Analyzing The Expressive Power of Graph Neural Network in a Spectral Perspective-PAPER.pdf</li> <li>Attention-Mechanism-Transformers-BERT-and-GPT-PAPER.pdf</li> <li>Can GPT-4 Perform Neural Architecture Search-PAPER.pdf</li> <li>Challenges in Deploying Machine Learning-PAPER.pdf</li> <li>Clinical AI quality improvement-PAPER.pdf</li> <li>Cramming-Training-a-Language-Model-On-A-Single-GPU-in-one-Day-PAPER.pdf</li> <li>DataGovernance-A conceptual framework, structured review-PAPER.pdf</li> <li>Detailed human avatar-PAPER.pdf</li> <li>DRIFT_26_CameraReadySubmission_NeurIPS_DRL-PAPER.pdf</li> <li>Dynamic Routing Between Capsules-PAPER.pdf</li> <li>Fairness in Machine Learning A Survey-PAPER.pdf</li> <li>Forecasting COVID-19 Confirmed Cases-PAPER.pdf</li> <li>Generalization Beyond Overfitting On Small Datasets-PAPER.pdf</li> <li>GPTrillion-Paper.pdf</li> <li>GPTs-are-GPTs-An-Early-Look-at-the-Labor-Market-Impact-Potential-of-Large-Language-Models-PAPER.pdf</li> <li>GraphMachine Learning NeurIPS 2020-PAPER.pdf</li> <li>Grouped Convolutional Neural Networks for Multivariate Time Series -PAPER.pdf</li> <li>Grouped functional time series forecasting An application to age-specific mortality rates-PAPER.pdf</li> <li>Hidden Technical Debt in Machine Learning Systems-PAPER.pdf</li> <li>Hidden technical debt in machine learning systems.pdf</li> <li>How to avoid machine learning pitfalls-PAPER.pdf</li> <li>How to Read a Paper-ARTC.pdf</li> <li>Identifying machine learning techniques for classification of target advertising-PAPER.pdf</li> <li>Introducing-GPTrillion-PAPER.pdf</li> <li>Large Object Storage in a Database or a Filesystem-PAPER.pdf</li> <li>Learning Graph Heuristic Search-PAPER.pdf</li> <li>Learning latent causal graphs via mixture oracles-PAPER.pdf</li> <li>LightGBM A Highly Efficient Gradient Boosting-PAPER.pdf</li> <li>Machine Learning within a Graph Database- A Case Study on Link Prediction for Scholarly Data-PAPER.pdf</li> <li>Merlion- A Machine Learning Library for Time Series-PAPER.pdf</li> <li>Model Evaluation, Model Selection, and Algorithm Selection-PAPER.pdf</li> <li>NeuralProphet-Explainable Forecasting at Scale-PAPER.pdf</li> <li>PAVE-Lazy-MDP based Ensemble to Improve Recall of Product Attribute Extraction Models-PAPER.pdf</li> <li>Precise Zero-Shot Dense Retrieval without Relevance Labels-PAPER.pdf</li> <li>Randomforest-PAPER.pdf</li> <li>Receptive Fields of Single Neurones in the Cats Striate Cortex-PAPER.pdf</li> <li>Robotics, AI, and Humanity Science-PAPERS.pdf</li> <li>Scalable Linear Algebra on a Relational Database System-PAPER.pdf</li> <li>Scaling Large Production Clusters-PAPER.pdf</li> <li>Skillful writing of an awful research paper-GUIDE.pdf</li> <li>The Modern Mathematics of Deep Learning-PAPER.pdf</li> <li>Towards Demystifying Serverless Machine Learning Training-PAPER.pdf</li> <li>Triple-View Feature Learning for Medical Image Segmentation-PAPER.pdf</li> <li>Understanding Diffusion Models- A Unified Perspective-PAPER.pdf</li> <li>VeML-An-End-to-End-Machine-Learning-Lifecycle-for-Large-Scale-and-High-Dimensional-Data-PAPER.pdf</li> <li>Very Deep Convolutional Networks for Large Scale Image Recognition-PAPER.pdf</li> <li>XGBoost A Scalable Tree Boosting System-PAPER.pdf</li> <li>XGBoost Reliable Large-scale Tree Boosting System-PAPER.pdf</li> </ol> <h2 id=recent-papers>Recent Papers<a class=headerlink href=#recent-papers title="Permanent link">&para;</a></h2> <ol> <li><a href=https://blog.research.google/2022/03/detecting-signs-of-disease-from.html>Detecting Signs of Disease from External Images of the Eye</a>, THURSDAY, MARCH 24, 2022</li> <li><a href=https://www.nature.com/articles/s41551-021-00745-6>Deep-learning models for the detection and incidence prediction of chronic kidney disease and type 2 diabetes from retinal fundus images</a>, 15 June 2021</li> <li><a href=https://www.nature.com/articles/s41551-019-0487-z>Detection of anaemia from retinal fundus images via deep learning</a>, 23 December 2019</li> <li><a href=https://blog.research.google/2018/02/assessing-cardiovascular-risk-factors.html>Assessing Cardiovascular Risk Factors with Computer Vision</a>, MONDAY, FEBRUARY 19, 2018</li> </ol> <p><strong>Author</strong> <br> Dr Hari Thapliyaal <br> dasarpai.com <br> linkedin.com/in/harithapliyal </p> <div class=author-bio style="margin-top: 2rem; display: flex; gap: 1rem;"> <img src=../assets/images/myphotos/Profilephoto1.jpg alt="Hari Thapliyaal" style="border-radius: 50%; width: 80px; height: 80px;"> <div> <strong>Dr. Hari Thapliyaal</strong><br> <small>Dr. Hari Thapliyal is a prolific blogger and seasoned professional with an extensive background in Data Science, Project Management, and Advait-Vedanta Philosophy. He holds a Doctorate in AI/NLP from SSBM, Geneva, along with Master’s degrees in Computers, Business Management, Data Science, and Economics. With over three decades of experience in management and leadership, Hari has extensive expertise in training, consulting, and coaching within the technology sector. His specializations include Data Science, AI, Computer Vision, NLP, and machine learning. Hari is also passionate about meditation and nature, often retreating to secluded places for reflection and peace.</small> </div> </div> <div class=share-buttons style="margin-top: 2rem;"> <strong>Share this article:</strong><br> <a href="https://twitter.com/intent/tweet?text=Important%20AI%20Paper%20List&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/DSR-Important-AI-Paper-List.html" target=_blank>Twitter</a> | <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/DSR-Important-AI-Paper-List.html" target=_blank>Facebook</a> | <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/DSR-Important-AI-Paper-List.html" target=_blank>LinkedIn</a> </div> </article> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.js></script> </body> </html>