<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="John Doe"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Understaning-Working-of-CNN.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Understanding the Working of CNN - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#understanding-the-working-of-cnn class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Understanding the Working of CNN </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-the-meaning-of-convolution-in-neural-network class=md-nav__link> <span class=md-ellipsis> What is the meaning of convolution in neural network? </span> </a> </li> <li class=md-nav__item> <a href=#if-there-is-some-convolution-layer-with-64-kernel-filter-and-filter-size-is-3x3-then-does-the-filter-get-updated-during-training-process class=md-nav__link> <span class=md-ellipsis> If there is some convolution layer with 64 kernel (filter) and filter size is 3x3 then does the filter get updated during training process? </span> </a> </li> <li class=md-nav__item> <a href=#i-heard-filters-has-only-0-and-1-value-depending-upon-what-we-want-to-extract-we-use-the-pattern-of-0-and-1-on-the-filter-like-for-edge-detection-contras-detection-etc class=md-nav__link> <span class=md-ellipsis> I heard filters has only 0 and 1 value. Depending upon what we want to extract we use the pattern of 0 and 1 on the filter, like for edge detection, contras detection etc. </span> </a> </li> <li class=md-nav__item> <a href=#if-a-layer-with-64-filter-has-3x3-filter-then-how-many-weights-are-there class=md-nav__link> <span class=md-ellipsis> If a layer with 64 filter has 3x3 filter then how many weights are there? </span> </a> </li> <li class=md-nav__item> <a href=#there-is-very-famous-1x1-filter-how-many-weights-are-there-if-it-layer-has-64-neuron-why-it-is-more-effective class=md-nav__link> <span class=md-ellipsis> There is very famous 1x1 filter. How many weights are there if it layer has 64 neuron? Why it is more effective? </span> </a> </li> <li class=md-nav__item> <a href=#normally-we-think-channel-means-number-of-layer-in-input-image-rgb-color-how-come-we-can-have-256-channels-in-neural-network class=md-nav__link> <span class=md-ellipsis> Normally we think channel means number of layer in input image (RGB color). How come we can have 256 channels in neural network? </span> </a> </li> <li class=md-nav__item> <a href=#how-to-calculate-output-size-of-convolutional-layer class=md-nav__link> <span class=md-ellipsis> How to calculate output size of convolutional layer? </span> </a> </li> <li class=md-nav__item> <a href=#when-3x3x3-filter-is-applied-to-224x224x3-image-then-how-it-become-224x224 class=md-nav__link> <span class=md-ellipsis> When 3x3x3 filter is applied to 224x224x3 image then how it become 224x224? </span> </a> </li> <li class=md-nav__item> <a href=#earlier-we-discussed-weight-of-each-layer-r-g-b-is-different-when-and-how-these-weights-are-decided class=md-nav__link> <span class=md-ellipsis> Earlier we discussed weight of each layer R, G, B is different? When and how these weights are decided? </span> </a> </li> <li class=md-nav__item> <a href=#where-do-we-learn-features-at-the-level-of-differet-layers-or-different-channels-filter class=md-nav__link> <span class=md-ellipsis> Where do we learn features? At the level of differet layers or different channels (filter) </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/Understaning-Working-of-CNN.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/Understaning-Working-of-CNN.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="Understanding the Working of CNN" src=../assets/images/dspost/dsp6213-Understanding-Working-of-CNN.jpg></p> <h1 id=understanding-the-working-of-cnn>Understanding the Working of CNN<a class=headerlink href=#understanding-the-working-of-cnn title="Permanent link">&para;</a></h1> <p>In this article, we aim to delve deeper into the working of CNNs. This article is intended for readers who have a basic understanding of CNNs and have computation-related questions. If you have any other questions about CNNs, feel free to ask in the comments.</p> <p><strong>Questions we are looking into.</strong> - What is the meaning of convolution in neural network? - If there is some convolution layer with 64 kernel (filter) and filter size is 3x3 then does the filter get updated during training process? - I heard filters has only 0 and 1 value. Depending upon what we want to extract we use the pattern of 0 and 1 on the filter, like for edge detection, contras detection etc. - If a layer with 64 filter has 3x3 filter then how many weights are there? - There is very famous 1x1 filter. How many weights are there if it layer has 64 neuron? Why it is more effective? - Normally we think channel means number of layer in input image (RGB color). How come we can have 256 channels in neural network? - How to calculate output size of convolutional layer? - When 3x3x3 filter is applied to 224x224x3 image then how it become 224x224? - Earlier we discussed weight of each layer R, G, B is different? When and how these weights are decided? - Where do we learn features? At the level of differet layers or different channels (filter)</p> <h2 id=what-is-the-meaning-of-convolution-in-neural-network>What is the meaning of convolution in neural network?<a class=headerlink href=#what-is-the-meaning-of-convolution-in-neural-network title="Permanent link">&para;</a></h2> <p>In the context of neural networks, specifically Convolutional Neural Networks (CNNs), <em>convolution</em> refers to a mathematical operation that combines two functions (or datasets) to produce a third function, typically used to extract features from input data. In simple terms, it’s a way of applying a filter or kernel to an input (like an image) to create a feature map, which highlights important patterns or features such as edges, textures, or shapes.</p> <p>Here’s a breakdown of the process:</p> <ol> <li><strong>Filter/Kernels</strong>: A small matrix (filter or kernel) is applied over the input data (e.g., an image). The filter contains a set of weights.</li> <li><strong>Sliding Window</strong>: The filter slides over the input image (or data) in steps, typically starting from the top-left corner. At each position, the filter performs an element-wise multiplication with the corresponding portion of the input, and the results are summed up to produce a single value.</li> <li><strong>Feature Map</strong>: This process results in a new matrix called the feature map, which represents the features detected in the input image.</li> </ol> <p>The main goal of convolution in CNNs is to reduce the spatial dimensions of the input (through pooling layers) while preserving important features, allowing the network to focus on relevant patterns and improve efficiency for tasks like image classification, object detection, and more.</p> <p>In summary, convolution helps the neural network understand and capture local patterns in the input data, making it particularly powerful for tasks involving visual information.</p> <h2 id=if-there-is-some-convolution-layer-with-64-kernel-filter-and-filter-size-is-3x3-then-does-the-filter-get-updated-during-training-process>If there is some convolution layer with 64 kernel (filter) and filter size is 3x3 then does the filter get updated during training process?<a class=headerlink href=#if-there-is-some-convolution-layer-with-64-kernel-filter-and-filter-size-is-3x3-then-does-the-filter-get-updated-during-training-process title="Permanent link">&para;</a></h2> <p>Yes, the <strong>filters (kernels) in a convolutional layer are updated during training</strong> through <strong>backpropagation</strong> and <strong>gradient descent</strong>. They learn to detect meaningful patterns (e.g., edges, textures, shapes) by adjusting their weights through backpropagation.</p> <ol> <li><strong>Convolution Layer Setup</strong> </li> <li>If you have a convolutional layer with <strong>64 filters</strong> (or neurons), and each filter is of size <strong>3×3</strong>, then each filter has a set of weights (parameters).</li> <li> <p>These filters slide over the input feature maps, performing convolution operations and generating activation maps.</p> </li> <li> <p><strong>Weight Updates During Training</strong></p> </li> <li>Each <strong>filter (3×3)</strong> has <strong>weights</strong> (along with biases).</li> <li>During <strong>forward propagation</strong>, these filters extract features by performing element-wise multiplication with the input and summing up the results.</li> <li>During <strong>backpropagation</strong>, the loss (error) is computed, and gradients of the loss with respect to the filter weights are calculated using the <strong>chain rule</strong>.</li> <li>Using <strong>gradient descent (or any optimization algorithm like Adam, SGD, RMSProp, etc.)</strong>, the filter weights are updated in order to minimize the loss.</li> </ol> <h2 id=i-heard-filters-has-only-0-and-1-value-depending-upon-what-we-want-to-extract-we-use-the-pattern-of-0-and-1-on-the-filter-like-for-edge-detection-contras-detection-etc>I heard filters has only 0 and 1 value. Depending upon what we want to extract we use the pattern of 0 and 1 on the filter, like for edge detection, contras detection etc.<a class=headerlink href=#i-heard-filters-has-only-0-and-1-value-depending-upon-what-we-want-to-extract-we-use-the-pattern-of-0-and-1-on-the-filter-like-for-edge-detection-contras-detection-etc title="Permanent link">&para;</a></h2> <p>That's a common misconception! You're referring to <strong>handcrafted filters</strong> used in traditional image processing, like <strong>Sobel, Prewitt, or Laplacian filters</strong> for edge detection, which have fixed values (e.g., 0s and 1s). </p> <p>However, in <strong>deep learning (CNNs)</strong>, filters (kernels) are <strong>learned automatically during training</strong>, and their values are not just 0s and 1s. Instead, they are real-valued numbers (floating-point weights) that get updated through <strong>backpropagation</strong>. </p> <hr> <p><strong>Key Differences: Traditional Filters vs. CNN Filters</strong></p> <table> <thead> <tr> <th>Feature</th> <th>Traditional Filters (Fixed)</th> <th>CNN Filters (Learned)</th> </tr> </thead> <tbody> <tr> <td>Values</td> <td>Manually set (e.g., 0s, 1s, -1s)</td> <td>Real-valued, learned from data</td> </tr> <tr> <td>Purpose</td> <td>Predefined for edge/contrast detection</td> <td>Learn features specific to the dataset</td> </tr> <tr> <td>Training Needed?</td> <td>No training needed (fixed)</td> <td>Updated using backpropagation</td> </tr> <tr> <td>Examples</td> <td>Sobel, Laplacian, Gabor filters</td> <td>Learned filters in convolutional layers</td> </tr> </tbody> </table> <hr> <p><strong>How CNN Filters Work?</strong> 1. <strong>Initialization:</strong><br> - Filters start with random values (not just 0s and 1s).</p> <ol> <li><strong>Learning Through Training:</strong> </li> <li>CNNs automatically adjust the filter values to detect relevant patterns (edges, textures, objects, etc.).</li> <li> <p>Filters in the first layers detect simple patterns (edges, corners), while deeper layers detect complex structures (faces, objects).</p> </li> <li> <p><strong>Backpropagation Updates Filters:</strong> </p> </li> <li>During training, the loss function calculates the error.</li> <li>Gradients of the loss with respect to each filter are computed.</li> <li>Using an optimizer (e.g., SGD, Adam), the filter values are updated to improve accuracy.</li> </ol> <h2 id=if-a-layer-with-64-filter-has-3x3-filter-then-how-many-weights-are-there>If a layer with 64 filter has 3x3 filter then how many weights are there?<a class=headerlink href=#if-a-layer-with-64-filter-has-3x3-filter-then-how-many-weights-are-there title="Permanent link">&para;</a></h2> <p><strong>Understanding the Parameters in a CNN Layer</strong><br> If a convolutional layer has <strong>64 filters</strong> (or neurons), and each filter is of size <strong>3×3</strong>, the number of trainable weights depends on the <strong>number of input channels</strong> (depth of the input feature map). </p> <p><strong>Case 1: Single-Channel Input (Grayscale)</strong> If the input to this layer is <strong>grayscale</strong> (i.e., it has only 1 channel), each filter has: </p> <p>3 x 3 = 9 (weights per filter) </p> <p>Since there are <strong>64 filters</strong>, the total number of weights is: </p> <p>64 x 9 = 576 Additionally, each filter has <strong>one bias term</strong>, so the total trainable parameters are: </p> <p>64 x (9 + 1) = 640</p> <p><strong>Case 2: Multi-Channel Input (e.g., RGB Image)</strong> If the input has <strong>multiple channels</strong>, such as an <strong>RGB image with 3 channels (R, G, B)</strong>, each filter must process all channels. So, each filter has: </p> <p>3 x 3 x 3 = 27 (weights per filter)}</p> <p>With <strong>64 filters</strong>, the total number of trainable weights is: </p> <p>64 x 27 = 1,728</p> <p>Including the bias terms (one per filter): </p> <p>64 x (27 + 1) = 1,792</p> <p><strong>General Formula for Convolutional Layer Parameters</strong> For a convolutional layer with:<br> - <strong>F</strong> filters (neurons)<br> - <strong>K × K</strong> filter size<br> - <strong>C</strong> input channels<br> - <strong>1 bias per filter</strong> </p> <p>The number of trainable parameters is: </p> <p>F x (K x K x C + 1)</p> <h2 id=there-is-very-famous-1x1-filter-how-many-weights-are-there-if-it-layer-has-64-neuron-why-it-is-more-effective>There is very famous 1x1 filter. How many weights are there if it layer has 64 neuron? Why it is more effective?<a class=headerlink href=#there-is-very-famous-1x1-filter-how-many-weights-are-there-if-it-layer-has-64-neuron-why-it-is-more-effective title="Permanent link">&para;</a></h2> <ul> <li><strong>1×1 convolutions learn cross-channel interactions efficiently.</strong></li> <li>They are <strong>used for dimensionality reduction</strong> (reducing channels) and <strong>feature transformation</strong>.</li> <li>They <strong>do not capture spatial features</strong>, but when combined with <strong>3×3 or 5×5 convolutions</strong>, they improve efficiency dramatically</li> </ul> <p><strong>1×1 Convolution (Pointwise Convolution)</strong> A <strong>1×1 filter</strong> is a convolutional kernel that operates on an entire input channel but <strong>does not consider neighboring spatial information</strong> like larger filters (e.g., 3×3 or 5×5). </p> <p><strong>Number of Weights in a 1×1 Convolution Layer</strong> If a convolutional layer has <strong>64 filters</strong> (neurons), and each filter is <strong>1×1</strong>, the number of trainable weights depends on the <strong>number of input channels (C)</strong>.</p> <p><strong>Formula for Weights in a 1×1 Convolution:</strong></p> <p>F x (K x K x C + 1)</p> <p>where: - <strong>F</strong> = Number of filters (neurons) = <strong>64</strong> - <strong>K × K</strong> = <strong>1×1</strong> filter size - <strong>C</strong> = Number of input channels - <strong>+1</strong> accounts for the bias term per filter</p> <p><strong>Case 1: Single-Channel Input (Grayscale)</strong> For a <strong>grayscale image (1 channel)</strong>: </p> <p>64 x (1 x 1 x 1 + 1) = 64 x 2 = 128 parameters</p> <p><strong>Case 2: Multi-Channel Input (RGB, C=3)</strong> For an <strong>RGB image (3 channels)</strong>: </p> <p>64 x (1 x 1 x 3 + 1) = 64 x 4 = 256 parameters</p> <p><strong>Case 3: Multi-Channel - Interim Layers</strong> For an input with <strong>C=256 channels</strong>, a <strong>1×1 convolution with 64 filters</strong> would have: </p> <p>64 x (1 x 1 x 256 + 1) = 16,448 parameters</p> <hr> <p><strong>Why is 1×1 Convolution Effective?</strong> Despite its simplicity, <strong>1×1 convolutions are extremely powerful</strong> and are widely used in deep learning architectures like <strong>GoogleNet (Inception), MobileNet, and ResNet</strong> for multiple reasons:</p> <ol> <li><strong>Dimensionality Reduction (Bottleneck Layers)</strong></li> <li>A <strong>1×1 convolution reduces the number of channels</strong> (feature maps), thus reducing computational cost.</li> <li>Example: If an input has <strong>256 channels</strong>, applying a <strong>1×1 convolution with 64 filters</strong> reduces it to <strong>64 channels</strong>.</li> <li> <p>This significantly <strong>reduces model size and computation</strong>.</p> </li> <li> <p><strong>Feature Transformation</strong></p> </li> <li>Even though it <strong>doesn’t change spatial dimensions</strong>, it <strong>learns new feature representations</strong> by linearly combining different channels.</li> <li> <p>It acts as a <strong>fully connected layer applied independently to each pixel</strong>.</p> </li> <li> <p><strong>Efficient Depthwise Computation (Depthwise Separable Convolutions)</strong></p> </li> <li>Used in architectures like <strong>MobileNet</strong> to replace expensive 3×3 convolutions.</li> <li> <p>Instead of using a large <strong>3×3×256</strong> kernel (which has <strong>2,304</strong> weights per filter), a <strong>depthwise 3×3 convolution</strong> + <strong>1×1 convolution</strong> achieves the same effect with <strong>far fewer parameters</strong>.</p> </li> <li> <p><strong>Non-Linearity Enhancement (ReLU After 1×1)</strong></p> </li> <li>Typically, after a <strong>1×1 convolution</strong>, a <strong>non-linearity (ReLU)</strong> is applied.</li> <li>This helps the network learn <strong>more complex transformations</strong> while keeping computation low.</li> </ol> <hr> <p><strong>Comparison: 1×1 vs 3×3 Convolutions</strong></p> <table> <thead> <tr> <th>Feature</th> <th>1×1 Convolution</th> <th>3×3 Convolution</th> </tr> </thead> <tbody> <tr> <td>Spatial Context</td> <td>No spatial info</td> <td>Captures local patterns</td> </tr> <tr> <td>Parameters (per filter)</td> <td>C+1</td> <td>9C+1</td> </tr> <tr> <td>Computational Cost</td> <td>Low</td> <td>High</td> </tr> <tr> <td>Feature Mixing</td> <td>Yes (across channels)</td> <td>Yes (across spatial locations)</td> </tr> <tr> <td>Use Case</td> <td>Bottlenecks, depth reduction</td> <td>Feature extraction</td> </tr> </tbody> </table> <h2 id=normally-we-think-channel-means-number-of-layer-in-input-image-rgb-color-how-come-we-can-have-256-channels-in-neural-network>Normally we think channel means number of layer in input image (RGB color). How come we can have 256 channels in neural network?<a class=headerlink href=#normally-we-think-channel-means-number-of-layer-in-input-image-rgb-color-how-come-we-can-have-256-channels-in-neural-network title="Permanent link">&para;</a></h2> <p>The concept of <strong>"channels"</strong> in convolutional layers extends beyond just color channels like <strong>RGB</strong>. Let’s understand this.</p> <p><strong>1. Understanding Channels in CNNs</strong> - In the <strong>input layer</strong>, channels refer to the number of color layers (e.g., <strong>RGB = 3 channels</strong>). - However, <strong>inside convolutional layers</strong>, each filter learns a different feature map, and these become the new "channels" for the next layer.</p> <p><strong>2. How Do We Get 256 Channels?</strong> Each convolutional layer has <strong>multiple filters</strong>, and each filter extracts <strong>different features</strong> from the input. The <strong>number of filters</strong> determines the number of <strong>output channels</strong> in that layer.</p> <p><strong>Example: Expanding from RGB (3 channels)</strong> 1. Suppose we apply <strong>64 filters of size 3×3×3</strong> (since RGB has 3 input channels) in the <strong>first convolutional layer</strong>. - Each filter extracts a feature from all 3 channels. - The output will have <strong>64 feature maps</strong>, meaning <strong>64 channels</strong>. 2. If the <strong>next convolutional layer</strong> applies <strong>256 filters</strong> on those <strong>64-channel feature maps</strong>, the output will have <strong>256 channels</strong>.</p> <p><strong>3. What Do Extra Channels Represent?</strong> Unlike <strong>RGB channels (red, green, blue), which are fixed</strong>, the additional <strong>channels in deeper layers are learned feature maps</strong>. These channels capture: - <strong>Edges, textures (early layers)</strong> - <strong>Shapes, patterns (middle layers)</strong> - <strong>High-level objects (deeper layers)</strong></p> <p>Each <strong>filter extracts a different aspect</strong> of the input, which is why more channels develop as the network gets deeper.</p> <p><strong>4. Example in a CNN Architecture</strong> - <strong>First Convolutional Layer (Processing RGB Image)</strong> - <strong>Input</strong>: 224×224×3 (RGB image) - <strong>Filters</strong>: 64 filters of 3×3×3 - <strong>Output</strong>: 224×224×64 (<strong>64 feature maps = 64 channels</strong>) - <strong>Second Convolutional Layer</strong> - <strong>Input</strong>: 224×224×64 (previous output) - <strong>Filters</strong>: 128 filters of 3×3×64 - <strong>Output</strong>: 224×224×128 (<strong>128 feature maps = 128 channels</strong>) - If a later convolutional layer has <strong>256 filters</strong>, its output will have <strong>256 channels</strong>.</p> <p><strong>5. Why Do CNNs Use So Many Channels?</strong> - <strong>Helps learn complex patterns</strong> → More channels mean the network can capture different types of information. - <strong>Improves accuracy</strong> → More filters = better feature representation. - <strong>Works well with deeper layers</strong> → As images go deeper in the network, low-level edges combine into high-level structures.</p> <h2 id=how-to-calculate-output-size-of-convolutional-layer>How to calculate output size of convolutional layer?<a class=headerlink href=#how-to-calculate-output-size-of-convolutional-layer title="Permanent link">&para;</a></h2> <p><strong>1. Understanding Convolution Layer Output Shape</strong> A convolutional layer <strong>does not reduce the spatial dimensions</strong> (height &amp; width) unless we apply <strong>stride &gt; 1</strong> or <strong>pooling</strong>. Instead, it adds <strong>depth (channels)</strong> based on the number of filters used.</p> <p><strong>Example: First Convolution Layer Processing an RGB Image</strong> - <strong>Input Shape:</strong> 224 x 224 x 3 (Height × Width × Channels) - <strong>Filters:</strong> Suppose we apply <strong>64 filters</strong> of size 3 x 3 - <strong>Stride:</strong> 1 (no downsampling) - <strong>Padding:</strong> "Same" (output size remains unchanged)</p> <p>\text{Output Shape} = 224 x 224 x 64</p> <p>✔ <strong>224 × 224</strong> → The spatial dimensions remain the same (since stride = 1).<br> ✔ <strong>64 channels</strong> → Because we used 64 filters, each one creates a separate feature map. </p> <p><strong>2. Why Isn't the Output Just 64?</strong> You're probably thinking about a <strong>fully connected layer</strong>, where we get a single number per neuron.<br> However, <strong>a convolutional layer works differently</strong>: - Each <strong>filter slides</strong> over the input and computes a <strong>feature map</strong>. - Since we use <strong>64 filters</strong>, we get <strong>64 feature maps</strong>, each of size <strong>224 × 224</strong>. - The output has <strong>spatial structure</strong>, not just 64 numbers.</p> <p>💡 <strong>Think of the output as a "stack" of 64 images (feature maps), each of size 224 × 224.</strong></p> <p><strong>3. General Formula for Convolution Output Size</strong> If an input has size ** H x W x C ** and we apply ** F filters** of size K x K with stride S and padding P , the output shape is:</p> <p>[[[H - K + 2P]/S] + 1 x [[W - K + 2P]/S] + 1 ] x F</p> <p>For our example: - H = 224, W = 224, C = 3 - K = 3, S = 1, P = 1 (same padding ensures output size remains 224 × 224) - F = 64 (number of filters)</p> <p>\text{Output Shape} = 224 x 224 x 64</p> <p><strong>4. When Do We Get Just 64 Numbers?</strong> If we want to reduce the <strong>spatial dimensions</strong>, we use: 1. <strong>Pooling (e.g., MaxPooling 2×2)</strong> → Reduces the size (e.g., from 224×224 to 112×112). 2. <strong>Flattening</strong> → Converts the feature maps into a <strong>1D vector</strong>. 3. <strong>Fully Connected (Dense) Layer</strong> → Outputs a single number per neuron.</p> <p>For example, in <strong>classification</strong>, the CNN eventually flattens feature maps and passes them through <strong>dense layers</strong>, resulting in <strong>64 or fewer numbers</strong> at the end.</p> <hr> <p><strong>Why Is Output 224 × 224 × 64?</strong> ✅ A <strong>convolutional layer outputs feature maps, not just a single number</strong>.<br> ✅ Since we apply <strong>64 filters</strong>, we get <strong>64 feature maps</strong>, <strong>not just 64 values</strong>.<br> ✅ The <strong>spatial size (224 × 224) remains the same</strong> if we use <strong>stride = 1</strong> and <strong>padding = same</strong>. </p> <h2 id=when-3x3x3-filter-is-applied-to-224x224x3-image-then-how-it-become-224x224>When 3x3x3 filter is applied to 224x224x3 image then how it become 224x224?<a class=headerlink href=#when-3x3x3-filter-is-applied-to-224x224x3-image-then-how-it-become-224x224 title="Permanent link">&para;</a></h2> <p>✔ A <strong>3×3×3 filter applied to a 224×224×3 image produces a 224×224×1 feature map</strong>.<br> ✔ When using <strong>64 filters, we get 224×224×64 output</strong>.<br> ✔ The <strong>spatial dimensions remain 224×224</strong> because of <strong>stride=1 and "same" padding</strong>.<br> ✔ The <strong>depth increases from 3 (RGB) to 64</strong> because each filter extracts different features. </p> <p><strong>1. Understanding How a Filter Works in CNNs</strong> In a convolutional layer: - A filter (also called a kernel) slides over the <strong>spatial dimensions</strong> (height &amp; width) of the input image. - It computes the <strong>dot product</strong> between the filter’s weights and the corresponding region of the image. - The depth of the filter <strong>must match the depth (number of channels) of the input</strong>.</p> <p><strong>2. How a 3×3×3 Filter Works</strong> Consider an <strong>RGB image of size 224×224×3</strong> (Height × Width × Channels):</p> <ol> <li>The <strong>filter size is 3×3×3</strong>, meaning:</li> <li>It covers a <strong>3×3</strong> region of the image.</li> <li>It extends across <strong>all 3 channels</strong> (Red, Green, Blue).</li> <li> <p>Each filter has <strong>3×3×3 = 27 weights</strong>.</p> </li> <li> <p>The <strong>convolution operation</strong>:</p> </li> <li>The filter slides over the image <strong>one step (stride = 1) at a time</strong>.</li> <li>At each position, it computes the weighted sum (dot product) of <strong>all 27 values</strong> (from 3×3 pixels across 3 channels).</li> <li>This produces <strong>a single number per position</strong>.</li> <li> <p>The filter moves across the <strong>entire</strong> 224×224 spatial area.</p> </li> <li> <p>Since the filter moves <strong>one step at a time (stride = 1) and we use "same" padding</strong>, the output <strong>retains the same spatial size</strong> but with <strong>only one channel</strong>: Output size = 224 x 224 x 1</p> </li> </ol> <p>(i.e., one feature map).</p> <p><strong>3. What Happens When We Use 64 Filters?</strong> - If we apply <strong>64 different 3×3×3 filters</strong>, each filter extracts a different feature from the image. - Each filter generates <strong>one 224×224 feature map</strong>. - Since we have <strong>64 filters</strong>, we get <strong>64 feature maps</strong>.</p> <p>Thus, the final output shape of this convolutional layer is: 224 x 224 x 64</p> <p><strong>4. General Formula for Convolution Output Size</strong> If an input image has: - <strong>Size:</strong> H x W x C (Height × Width × Channels) - <strong>Filter size:</strong> K x K x C (Kernel size × Kernel size × Same depth as input) - <strong>Stride:</strong> S - <strong>Padding:</strong> P </p> <p>Then the output size is:</p> <div class=arithmatex>\[\left( \frac{H - K + 2P}{S} + 1 \right) x \left( \frac{W - K + 2P}{S} + 1 \right) x F\]</div> <p>where: - <strong>F</strong> = number of filters</p> <hr> <p><strong>5. Example Calculation for Our Case</strong> Given: - <strong>Input:</strong> 224 x 224 x 3 - <strong>Filter:</strong> 3 x 3 x 3 - <strong>Stride:</strong> S = 1 - <strong>Padding:</strong> "Same" (so that output size remains unchanged)</p> <p>Using the formula:</p> <div class=arithmatex>\[\left( \frac{224 - 3 + 2(1)}{1} + 1 \right) x \left( \frac{224 - 3 + 2(1)}{1} + 1 \right) x 64\]</div> <p>= 224 x 224 x 64</p> <p>Thus, the output retains the <strong>same height and width</strong> but increases in <strong>depth (number of filters applied)</strong>.</p> <hr> <p><strong>6. Why Does Depth Increase?</strong> Each filter extracts a different feature from the image: - <strong>Some filters detect edges</strong>. - <strong>Some detect textures</strong>. - <strong>Some detect patterns like curves, shapes, or corners</strong>.</p> <p>Instead of just <strong>3 channels (RGB), now we have 64 feature maps</strong>, allowing the CNN to learn more <strong>complex patterns</strong>.</p> <h2 id=earlier-we-discussed-weight-of-each-layer-r-g-b-is-different-when-and-how-these-weights-are-decided>Earlier we discussed weight of each layer R, G, B is different? When and how these weights are decided?<a class=headerlink href=#earlier-we-discussed-weight-of-each-layer-r-g-b-is-different-when-and-how-these-weights-are-decided title="Permanent link">&para;</a></h2> <p>✔ <strong>Each filter has different weights for each channel (R, G, B).</strong><br> ✔ <strong>Weights are randomly initialized and learned during training using backpropagation.</strong><br> ✔ <strong>The dot product computes the weighted sum of pixel values in all channels.</strong><br> ✔ <strong>Different filters learn to detect different features, from edges to complex objects.</strong></p> <p>The <strong>weights for each channel (R, G, B) are different</strong>, and they are <strong>learned during training</strong>. Let's understand this.</p> <p><strong>1. Understanding Weights in a Convolutional Filter</strong> Each <strong>filter (kernel)</strong> in a convolutional layer has a set of <strong>learnable weights</strong>.<br> For a <strong>3×3×3 filter</strong>, there are: - <strong>9 weights per channel</strong> (since the filter is 3×3). - <strong>3 separate weight matrices</strong> (one for each channel: R, G, B). - <strong>1 bias term</strong> (optional, but usually present).</p> <p>Total trainable parameters for one filter:</p> <p>(3 x 3 x 3) + 1 = 27 + 1 = 28</p> <p>Since <strong>each filter has different weights for R, G, and B channels</strong>, the convolution operation applies <strong>different transformations</strong> to each channel before summing them.</p> <p><strong>2. How Is the Weighted Sum (Dot Product) Computed?</strong> At each position where the filter slides over the image: 1. The <strong>weights of the filter</strong> are multiplied with the corresponding pixel values. 2. The <strong>results for each channel are summed</strong> to get a single value. 3. A <strong>bias term is added</strong> (if used). 4. A <strong>non-linearity (like ReLU)</strong> is applied.</p> <p>Example:</p> <p>If a <strong>3×3×3 filter</strong> has weights: W_R, W_G, W_B</p> <p>and the image patch under the filter has pixel values: I_R, I_G, I_B</p> <p>Then the output at that position is:</p> <p>output = I_R \cdot + I_G \cdot W_G + I_B \cdot W_B + bias</p> <p>This results in <strong>a single number per filter position</strong>.</p> <p><strong>3. When and How Are These Weights Decided?</strong> <strong>(a) Initializing Weights</strong> - At the start of training, weights are <strong>randomly initialized</strong> (using techniques like Xavier or He initialization). - They are <strong>not manually set</strong>—they start as small random values.</p> <p><strong>(b) Learning Weights During Training</strong> Weights are learned using <strong>backpropagation</strong> and <strong>gradient descent</strong>: 1. The <strong>forward pass</strong> computes outputs using the current weights. 2. The <strong>loss function</strong> (e.g., cross-entropy for classification) measures how far the predictions are from the correct labels. 3. <strong>Backpropagation</strong> calculates gradients of the loss <strong>with respect to the weights</strong>. 4. <strong>Gradient descent (or Adam, RMSprop, etc.) updates the weights</strong> to minimize the loss. 5. Steps 1–4 repeat for many iterations (epochs) until the network converges.</p> <p><strong>&copy; Different Filters Learn Different Features</strong> - In <strong>early layers</strong>, filters learn <strong>edges, corners, and simple textures</strong>. - In <strong>deeper layers</strong>, filters learn <strong>complex patterns like shapes, objects, and high-level features</strong>.</p> <p><strong>4. Why Are There Different Weights for R, G, B?</strong> Each filter learns <strong>a different way to combine color information</strong>.<br> - Some filters might focus more on <strong>edges in the red channel</strong>. - Others might emphasize <strong>texture in the green channel</strong>. - Others may combine <strong>all three channels differently</strong> to detect complex patterns.</p> <p>This flexibility allows CNNs to <strong>extract meaningful features</strong> regardless of the color composition of the image.</p> <h2 id=where-do-we-learn-features-at-the-level-of-differet-layers-or-different-channels-filter>Where do we learn features? At the level of differet layers or different channels (filter)<a class=headerlink href=#where-do-we-learn-features-at-the-level-of-differet-layers-or-different-channels-filter title="Permanent link">&para;</a></h2> <p>Earlier we discussed "In early layers, filters learn edges, corners, and simple textures." The we said "If we apply 64 different 3×3×3 filters, each filter extracts a different feature from the image." Isn't contracting? Each filter extracts different feature and is represented by the channel (neuron) or each layer is learning different features?</p> <p><strong>1. Each Filter Extracts a Different Feature (Per Layer)</strong> When we apply <strong>64 different 3×3×3 filters</strong> in a <strong>single convolutional layer</strong>, each filter learns to detect <strong>a different low-level feature</strong>.<br> - One filter might detect <strong>horizontal edges</strong>.<br> - Another might detect <strong>vertical edges</strong>.<br> - Another might respond to <strong>small texture patterns</strong>.<br> - Each of these filters produces a <strong>separate feature map (channel)</strong>. </p> <p>✔ <strong>Each filter learns a different feature in that layer.</strong> </p> <hr> <p><strong>2. Deeper Layers Learn More Complex Features</strong> CNNs are hierarchical:<br> - <strong>Early layers</strong> learn simple features (edges, corners, textures).<br> - <strong>Middle layers</strong> learn more abstract features (patterns, shapes).<br> - <strong>Deeper layers</strong> learn high-level structures (eyes, faces, objects). </p> <p>✔ <strong>Each layer captures different types of features.</strong> </p> <p><strong>3. How Does This Work Together?</strong> - In <strong>Layer 1</strong>, each filter detects a different <strong>low-level feature</strong> (edges, textures).<br> - In <strong>Layer 2</strong>, filters combine these edges/textures into <strong>shapes and patterns</strong>.<br> - In <strong>Layer 3+,</strong> filters detect <strong>high-level structures</strong> like objects. </p> <p>💡 <strong>Each filter within a layer extracts a different feature, and each deeper layer extracts more abstract features.</strong> </p> <p><strong>4. Analogy: Detecting a Face</strong> Imagine detecting a face in an image: - <strong>Layer 1:</strong> Detects edges (nose outline, eye corners).<br> - <strong>Layer 2:</strong> Combines edges into <strong>shapes</strong> (eye, mouth, nose).<br> - <strong>Layer 3:</strong> Recognizes <strong>full facial structures</strong>.<br> - <strong>Final Layers:</strong> Identify specific faces (person A vs. person B). </p> <p><strong>5. Final Answer: No Contradiction!</strong> ✔ <strong>Each filter in a single layer extracts a different feature (edge, texture, shape).</strong><br> ✔ <strong>Each deeper layer extracts progressively more complex features.</strong><br> ✔ <strong>The number of filters = number of feature maps (channels) in that layer.</strong> </p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>