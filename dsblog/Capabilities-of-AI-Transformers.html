<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Capabilities-of-AI-Transformers.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Capabilities of AI Transformers - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#capabilities-of-ai-transformers class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Capabilities of AI Transformers </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#background class=md-nav__link> <span class=md-ellipsis> Background </span> </a> </li> <li class=md-nav__item> <a href=#capabilities-of-ai-transformers_1 class=md-nav__link> <span class=md-ellipsis> Capabilities of AI Transformers </span> </a> </li> <li class=md-nav__item> <a href=#conclusion class=md-nav__link> <span class=md-ellipsis> Conclusion </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/Capabilities-of-AI-Transformers.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/Capabilities-of-AI-Transformers.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="Capabilities of AI Transformers" src=../assets/images/dspost/dsp6067-Capabilities-of-AI-Transformers.jpg> </p> <h1 id=capabilities-of-ai-transformers>Capabilities of AI Transformers<a class=headerlink href=#capabilities-of-ai-transformers title="Permanent link">&para;</a></h1> <h2 id=background>Background<a class=headerlink href=#background title="Permanent link">&para;</a></h2> <p>Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings. </p> <p>Precursors of Transformers were RNN, LSTM, and GRU architecture. Transformers are based on the 2017 research paper <a href=https://arxiv.org/abs/1706.03762>"Attention is All You Need"</a></p> <p>Initially, Transformers were used for NLP-related tasks. Slowly researchers started exploring the power of the Transformer Architectures and as of 2023 these are used for hundreds of tasks in different AI domains of technologies like: - Text Models (NLP, NLU, NLG) - Vision Models (Computer Vision) - Audio Models (Audio Processing, Classification, Audio Generation) - Reinforcement (RL) Models - Time-series Models - Multimodal: OCR (extract information from scanned documents), video classification, visual QA, table data question answering - Graph Models</p> <p>Starting the journey in 2017, as of now (2023) we have approx 200 Transformer based architectures proposed by various researchers for various purposes. Using these architecture and various benchmark datasets thousands of models have been created which give SOTA performance on various tasks. Based on your need you choose which architecture can help you meet your project objective. There are high chances you will get some pre-trained models which you can use without training (Zero-shot) or small finetuning (one-shot or few-shot) efforts. For that you need to explore <a href=huggingface.co>Huggingface</a> and <a href=https://paperswithcode.com/ >PaperWithCode</a></p> <p>This articles list all the major Transformer related researcher paper, their object, and capabilities.</p> <p><strong>Note :</strong> Name starting with * are not Transformers, most of them are pretransformer age architectures. </p> <h2 id=capabilities-of-ai-transformers_1>Capabilities of AI Transformers<a class=headerlink href=#capabilities-of-ai-transformers_1 title="Permanent link">&para;</a></h2> <table> <thead> <tr> <th>Sno</th> <th>Transformer</th> <th>Objective</th> <th>Summary</th> <th>NLP Tasks</th> <th>CV Tasks</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>*AlexNet</td> <td>Image Classification</td> <td>A deep convolutional neural network architecture for image classification tasks.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>2</td> <td>*VGG16</td> <td>Visual Geometry Group Network (16 layers)</td> <td>A deep CNN model with 16 convolutional layers developed by the Visual Geometry Group at Oxford University.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>3</td> <td>*VGG19</td> <td>Visual Geometry Group Network (19 layers)</td> <td>A deep CNN model with 19 convolutional layers, an extended version of VGG16.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>4</td> <td>*ResNet</td> <td>Residual Networks</td> <td>A deep CNN architecture that introduces residual connections to alleviate the vanishing gradient problem.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>5</td> <td>*InceptionResNet</td> <td>Combination of Inception and ResNet</td> <td>A hybrid CNN model that combines the strengths of the Inception and ResNet architectures.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>6</td> <td>*ConvNeXt</td> <td>Improved Convolutional Neural Network</td> <td>A convolutional neural network architecture that aims to capture richer spatial relationships in images.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>7</td> <td>*DenseNet</td> <td>Dense Connections in Convolutional Networks</td> <td>A densely connected convolutional neural network architecture that encourages feature reuse and reduces the number of parameters.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>8</td> <td>*MobileNetV1</td> <td>Mobile-oriented CNN Architecture</td> <td>A lightweight convolutional neural network architecture designed for mobile and embedded devices.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>9</td> <td>*Xception</td> <td>Extreme Inception</td> <td>A deep CNN architecture that replaces the standard Inception modules with depthwise separable convolutions.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>10</td> <td>EncoderDecoder</td> <td>Sequence-to-sequence modeling</td> <td>A transformer-based model architecture that combines encoder and decoder for sequence-to-sequence tasks such as machine translation.</td> <td>Machine Translation, Text Summarization</td> <td>-</td> </tr> <tr> <td>11</td> <td>*MobileNetV2</td> <td>Improved MobileNet Architecture</td> <td>An enhanced version of MobileNet with improved performance and efficiency.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>12</td> <td>Data2Vec</td> <td>Embedding data tables</td> <td>A transformer-based model for embedding and encoding structured data tables.</td> <td>Tabular Data Embedding, Data Table Encoding</td> <td>-</td> </tr> <tr> <td>13</td> <td>GPT</td> <td>Language modeling and text generation</td> <td>A transformer-based model trained on a large corpus to generate coherent and contextually relevant text.</td> <td>Text Generation, Text Completion, Language Modeling</td> <td>-</td> </tr> <tr> <td>14</td> <td>BERT</td> <td>Pre-training and fine-tuning on various NLP tasks</td> <td>A transformer-based model widely used for pre-training and fine-tuning on NLP tasks.</td> <td>Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>15</td> <td>MarianMT</td> <td>Multilingual Neural Machine Translation</td> <td>A multilingual neural machine translation model based on the Marian framework.</td> <td>Machine Translation</td> <td>-</td> </tr> <tr> <td>16</td> <td>BiT</td> <td>Vision transformer for image classification</td> <td>A vision transformer model pre-trained on large-scale datasets for image classification tasks.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>17</td> <td>Transformer-XL</td> <td>Transformer model with extended context</td> <td>A transformer model architecture that extends the context window, enabling longer-range dependencies.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>18</td> <td>XLM</td> <td>Cross-lingual Language Model</td> <td>A transformer-based model for cross-lingual language understanding and machine translation.</td> <td>Cross-lingual Language Understanding, Machine Translation</td> <td>-</td> </tr> <tr> <td>19</td> <td>CTRL</td> <td>Text generation with control codes</td> <td>A transformer-based model that allows fine-grained control over generated text using control codes.</td> <td>Text Generation, Controlled Text Generation</td> <td>-</td> </tr> <tr> <td>20</td> <td>GPT-2</td> <td>Language modeling and text generation</td> <td>A transformer-based model similar to GPT but with a smaller architecture, trained on a large corpus to generate coherent and contextually relevant text.</td> <td>Text Generation, Text Completion, Language Modeling</td> <td>-</td> </tr> <tr> <td>21</td> <td>Funnel Transformer</td> <td>Improving the efficiency and effectiveness of transformers</td> <td>A transformer-based model architecture that reduces the computational cost of transformers while maintaining their effectiveness.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>22</td> <td>*EfficientNet B0</td> <td>Efficient and Scalable CNN</td> <td>A family of convolutional neural network architectures that achieve high accuracy with fewer parameters and computations.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>23</td> <td>ALBERT</td> <td>Improve the efficiency of BERT</td> <td>A lite version of BERT that uses parameter reduction techniques to achieve faster training and lower memory consumption.</td> <td>Classification, Translation, Named Entity Recognition (NER)</td> <td>-</td> </tr> <tr> <td>24</td> <td>EfficientNet</td> <td>Efficient convolutional neural network architecture</td> <td>A convolutional neural network architecture that achieves state-of-the-art performance with significantly fewer parameters.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>25</td> <td>MobileNetV3</td> <td>Efficient Mobile Neural Network for Computer Vision</td> <td>A lightweight and efficient neural network architecture designed for computer vision tasks on mobile devices.</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> <td>-</td> </tr> <tr> <td>26</td> <td>Nezha</td> <td>Neural Encoder for Zero-shot Transfer Learning</td> <td>A transformer-based model that enables zero-shot transfer learning by learning a shared semantic space.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>27</td> <td>BART</td> <td>Text generation and summarization</td> <td>A denoising autoencoder model that can be used for text generation and summarization tasks.</td> <td>Text Generation, Summarization</td> <td>-</td> </tr> <tr> <td>28</td> <td>ERNIE</td> <td>Enhanced representation through knowledge integration</td> <td>A transformer-based model that enhances representation learning by integrating external knowledge sources.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>29</td> <td>ErnieM</td> <td>Enhanced representation through multitask learning</td> <td>A multitask learning framework that enhances representation learning by jointly training multiple downstream NLP tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>30</td> <td>FlauBERT</td> <td>French language representation learning</td> <td>A transformer-based model specifically trained for French language representation learning tasks.</td> <td>French Language Processing, Text Classification</td> <td>-</td> </tr> <tr> <td>31</td> <td>LXMERT</td> <td>Vision and Language Multimodal Transformer</td> <td>A multimodal transformer model that combines vision and language information for various tasks.</td> <td>Visual Question Answering (VQA), Visual Dialog, Image Captioning, Visual Grounding</td> <td>-</td> </tr> <tr> <td>32</td> <td>Pegasus</td> <td>Pre-training with Extracted Gap Sentences for Abstractive Summarization</td> <td>A transformer-based model trained for abstractive text summarization tasks.</td> <td>Text Summarization</td> <td>-</td> </tr> <tr> <td>33</td> <td>XLNet</td> <td>Generalized Autoregressive Pretraining</td> <td>A transformer-based model that leverages permutation-based training to learn bidirectional context.</td> <td>Language Modeling, Text Classification</td> <td>-</td> </tr> <tr> <td>34</td> <td>BioGpt</td> <td>Processing biomedical text</td> <td>A variant of the GPT model specifically designed for processing biomedical text.</td> <td>Biomedical Text Processing, Named Entity Recognition (NER), Clinical Text Understanding</td> <td>-</td> </tr> <tr> <td>35</td> <td>Hubert</td> <td>Automatic speech recognition with transformers</td> <td>A transformer-based model designed for automatic speech recognition tasks.</td> <td>Automatic Speech Recognition</td> <td>-</td> </tr> <tr> <td>36</td> <td>REALM</td> <td>Retrieval-Augmented Language Model</td> <td>A language model augmented with a dense retrieval mechanism to improve performance on text retrieval tasks.</td> <td>Information Retrieval, Text Classification, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>37</td> <td>SpeechToTextTransformer</td> <td>Transformer for Speech-to-Text Conversion</td> <td>A transformer-based model designed specifically for speech-to-text conversion tasks.</td> <td>Speech-to-Text Conversion</td> <td>-</td> </tr> <tr> <td>38</td> <td>XLM-V</td> <td>Cross-lingual Language Understanding</td> <td>A transformer-based model for cross-lingual language understanding, leveraging multilingual embeddings.</td> <td>Cross-lingual Language Understanding</td> <td>-</td> </tr> <tr> <td>39</td> <td>RoBERTa</td> <td>Robustly optimized BERT variant</td> <td>An optimized variant of BERT (Bidirectional Encoder Representations from Transformers) for various NLP tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>40</td> <td>GPT Neo</td> <td>Efficient and scalable variant of GPT</td> <td>A transformer-based model architecture that provides an efficient and scalable variant of GPT for various natural language processing tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>41</td> <td>CamemBERT</td> <td>French language processing and text classification</td> <td>A transformer-based model specifically trained for French language processing and text classification tasks.</td> <td>French Language Processing, Text Classification</td> <td>-</td> </tr> <tr> <td>42</td> <td>DialoGPT</td> <td>Conversational AI chatbot</td> <td>A transformer-based model trained for generating human-like conversational responses.</td> <td>Conversational AI, Chatbot</td> <td>-</td> </tr> <tr> <td>43</td> <td>DistilBERT</td> <td>Distilled version of BERT</td> <td>A smaller and faster version of BERT with a similar performance on various NLP tasks.</td> <td>Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>44</td> <td>LiLT</td> <td>Language learning from transliterated text</td> <td>A transformer-based model for language learning that utilizes transliterated text as training data.</td> <td>Language Learning</td> <td>-</td> </tr> <tr> <td>45</td> <td>LUKE</td> <td>Language Understanding with Knowledge-based Entities</td> <td>A model that integrates knowledge-based entities into transformer-based language understanding tasks.</td> <td>Named Entity Recognition (NER), Relation Extraction, Knowledge Graph Completion</td> <td>-</td> </tr> <tr> <td>46</td> <td>MobileBERT</td> <td>Efficient BERT for Mobile and Edge Devices</td> <td>A compact and efficient version of BERT designed for deployment on mobile and edge devices.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>47</td> <td>MT5</td> <td>Multilingual Text-to-Text Transfer Transformer</td> <td>A transformer-based model capable of multilingual text-to-text transfer learning across various NLP tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>48</td> <td>RAG</td> <td>Retrieval-Augmented Generation</td> <td>A model that combines retrieval and generation methods for open-domain question answering.</td> <td>Open-Domain Question Answering</td> <td>-</td> </tr> <tr> <td>49</td> <td>ConvBERT</td> <td>Text classification and named entity recognition (NER)</td> <td>A transformer-based model for text classification and named entity recognition (NER) tasks.</td> <td>Classification, Named Entity Recognition (NER), Sentiment Analysis</td> <td>-</td> </tr> <tr> <td>50</td> <td>Megatron-GPT2</td> <td>High-performance GPT-2-based language model</td> <td>A high-performance GPT-2-based language model developed using the Megatron framework.</td> <td>Text Generation, Text</td> <td>-</td> </tr> <tr> <td>51</td> <td>PhoBERT</td> <td>Pretrained language model for Vietnamese</td> <td>A pretrained language model specifically designed for the Vietnamese language.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>52</td> <td>RoBERTa-PreLayerNorm</td> <td>RoBERTa with PreLayerNorm</td> <td>A variant of RoBERTa with the PreLayerNorm (PLN) technique, which improves training stability and efficiency.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>53</td> <td>BERTweet</td> <td>Pre-trained BERT models for processing tweets</td> <td>BERT models specifically trained on Twitter data for tweet processing tasks.</td> <td>Classification, Named Entity Recognition (NER), Sentiment Analysis</td> <td>-</td> </tr> <tr> <td>54</td> <td>mBART</td> <td>Multilingual Denoising Autoencoder</td> <td>A multilingual denoising autoencoder based on the BART framework, capable of generating text in multiple languages.</td> <td>Text Generation, Text Completion, Multilingual Language Modeling</td> <td>-</td> </tr> <tr> <td>55</td> <td>Megatron-BERT</td> <td>High-performance BERT-based language model</td> <td>A high-performance BERT-based language model developed using the Megatron framework.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>56</td> <td>SpeechToTextTransformer2</td> <td>Transformer model for Speech-to-Text Conversion</td> <td>Another transformer-based model for speech-to-text conversion, providing an alternative approach.</td> <td>Speech-to-Text Conversion</td> <td>-</td> </tr> <tr> <td>57</td> <td>BERT For Sequence Generation</td> <td>Text generation using BERT-based models</td> <td>Fine-tuned BERT models for sequence generation tasks, such as text generation or summarization.</td> <td>Text Generation, Summarization</td> <td>-</td> </tr> <tr> <td>58</td> <td>ConvNeXT</td> <td>Language modeling and text generation</td> <td>A transformer-based model for language modeling and text generation tasks.</td> <td>Language Modeling, Text Generation</td> <td>-</td> </tr> <tr> <td>59</td> <td>ELECTRA</td> <td>Pre-training method for language representation learning</td> <td>A pre-training method that replaces masked language modeling with a generator-discriminator setup for better language representation.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>60</td> <td>Longformer</td> <td>Long-range sequence modeling with transformers</td> <td>A transformer-based model architecture that extends the standard transformer to handle long-range dependencies.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>61</td> <td>RegNet</td> <td>Regularized Convolutional Neural Network</td> <td>A convolutional neural network architecture with regularization techniques for efficient and scalable training.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>62</td> <td>SqueezeBERT</td> <td>Lightweight BERT model with Squeeze-and-Excitation</td> <td>A lightweight variant of BERT with Squeeze-and-Excitation (SE) blocks for efficient training and inference.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>63</td> <td>LayoutLM</td> <td>Text and layout understanding for document analysis</td> <td>A transformer-based model that combines text and layout information for document understanding tasks.</td> <td>Document Understanding, OCR, Named Entity Recognition (NER)</td> <td>-</td> </tr> <tr> <td>64</td> <td>MPNet</td> <td>Megatron Pretrained Network</td> <td>A model pretrained using the Megatron framework, designed for various NLP tasks with high performance.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>65</td> <td>VisualBERT</td> <td>Integrating Visual Information with BERT</td> <td>A BERT-based model that incorporates visual information for multimodal understanding.</td> <td>-</td> <td>Vision-Language Tasks, Image Captioning, Visual Question Answering (VQA)</td> </tr> <tr> <td>66</td> <td>Conditional DETR</td> <td>Object detection and instance segmentation</td> <td>A transformer-based model for object detection and instance segmentation tasks.</td> <td>-</td> <td>Object Detection, Instance Segmentation</td> </tr> <tr> <td>67</td> <td>GPTBigCode</td> <td>Code generation for programming languages</td> <td>A transformer-based model trained on a large corpus of code to generate code snippets or complete programs for various programming languages.</td> <td>Code Generation, Programming Language Processing</td> <td>-</td> </tr> <tr> <td>68</td> <td>M-CTC-T</td> <td>Music Transcription with Transformer</td> <td>A transformer-based model designed for music transcription, converting audio into musical notation.</td> <td>Music Transcription</td> <td>-</td> </tr> <tr> <td>69</td> <td>Pix2Struct</td> <td>Image-to-Structure Translation</td> <td>A transformer-based model for translating images into structured representations.</td> <td>-</td> <td>Image-to-Structure Translation</td> </tr> <tr> <td>70</td> <td>ProphetNet</td> <td>Pretrained Sequence-to-Sequence Model</td> <td>A sequence-to-sequence model pretrained for various NLP tasks, based on the transformer architecture.</td> <td>Text Generation, Text Completion, Machine Translation, Summarization</td> <td>-</td> </tr> <tr> <td>71</td> <td>SEW</td> <td>Simple and Efficient Word-level language model</td> <td>A word-level language model that is simple and efficient, designed for various NLP tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>72</td> <td>T5</td> <td>Text-to-Text Transfer Transformer</td> <td>A text-to-text transfer transformer model that can be fine-tuned for various NLP tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>73</td> <td>DeBERTa</td> <td>Improving the effectiveness of BERT</td> <td>A transformer-based model that enhances BERT by addressing its limitations and improving performance on various NLP tasks.</td> <td>Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>74</td> <td>Informer</td> <td>Time series forecasting with transformers</td> <td>A transformer-based model for time series forecasting tasks, capturing long-term dependencies in the data.</td> <td>Time Series Forecasting</td> <td>-</td> </tr> <tr> <td>75</td> <td>LED</td> <td>Language model for efficient decoding</td> <td>A transformer-based language model designed for efficient decoding, suitable for constrained environments.</td> <td>Text Generation, Text Completion, Language Modeling</td> <td>-</td> </tr> <tr> <td>76</td> <td>SwitchTransformers</td> <td>Transformers with Dynamic Routing</td> <td>A library that provides implementations of various transformer models with dynamic routing capabilities.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>77</td> <td>Whisper</td> <td>Unsupervised Representation Learning</td> <td>A transformer-based model for unsupervised representation learning on audio data.</td> <td>Speech Representation Learning</td> <td>-</td> </tr> <tr> <td>78</td> <td>XLM-ProphetNet</td> <td>Cross-lingual Language Generation</td> <td>A transformer-based model for cross-lingual language generation, extending the ProphetNet architecture.</td> <td>Cross-lingual Language Generation</td> <td>-</td> </tr> <tr> <td>79</td> <td>XLM-RoBERTa</td> <td>Cross-lingual Language Representation</td> <td>A cross-lingual variant of RoBERTa, providing multilingual representation learning.</td> <td>Cross-lingual Language Representation</td> <td>-</td> </tr> <tr> <td>80</td> <td>Deformable DETR</td> <td>Object detection and instance segmentation with deformable attention</td> <td>A transformer-based model for object detection and instance segmentation tasks, incorporating deformable attention mechanisms.</td> <td>-</td> <td>Object Detection, Instance Segmentation</td> </tr> <tr> <td>81</td> <td>FNet</td> <td>Image generation with Fourier features</td> <td>A transformer-based model that generates images using Fourier features instead of traditional positional encodings.</td> <td>-</td> <td>Image Generation</td> </tr> <tr> <td>82</td> <td>GPTSAN-japanese</td> <td>Japanese language variant of GPT for sentiment analysis</td> <td>A version of GPT specifically designed and trained for sentiment analysis tasks in the Japanese language.</td> <td>Japanese Language</td> <td></td> </tr> <tr> <td>83</td> <td>SEW-D</td> <td>Deep version of Simple and Efficient Word-level language model</td> <td>A deep variant of SEW for improved performance on NLP tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>84</td> <td>CPM</td> <td>Chinese language processing and text generation</td> <td>A transformer-based model specifically designed for Chinese language processing and text generation tasks.</td> <td>Chinese Language Processing, Text Generation</td> <td>-</td> </tr> <tr> <td>85</td> <td>GIT</td> <td>Generating informative text from structured data</td> <td>A transformer-based model that generates informative text, such as explanations or summaries, from structured data inputs.</td> <td>Data-to-Text Generation, Structured Data Processing</td> <td>-</td> </tr> <tr> <td>86</td> <td>LayoutXLM</td> <td>Multilingual document understanding with transformers</td> <td>A transformer-based model for multilingual document understanding, incorporating text and layout information.</td> <td>Multilingual Document Understanding, OCR, Named Entity Recognition (NER)</td> <td>-</td> </tr> <tr> <td>87</td> <td>DETR</td> <td>Object detection and instance segmentation</td> <td>A transformer-based model for object detection and instance segmentation tasks.</td> <td>-</td> <td>Object Detection, Instance Segmentation</td> </tr> <tr> <td>88</td> <td>GPT NeoX</td> <td>Further improved version of GPT Neo</td> <td>An advanced version of GPT Neo that incorporates additional enhancements and optimizations for natural language processing tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>89</td> <td>RemBERT</td> <td>Transformer model for code</td> <td>A transformer-based model specifically designed for code-related tasks, such as code generation and understanding.</td> <td>Code Generation, Code Understanding</td> <td>-</td> </tr> <tr> <td>90</td> <td>RoCBert</td> <td>Robustly optimized Chinese BERT variant</td> <td>A Chinese language variant of RoBERTa, optimized for various NLP tasks in Chinese text.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>91</td> <td>TAPAS</td> <td>Table Parsing via Transformer</td> <td>A transformer-based model designed for table parsing, enabling natural language queries over tabular data.</td> <td>Table Parsing, Question Answering (QA) over Tabular Data</td> <td>-</td> </tr> <tr> <td>92</td> <td>UPerNet</td> <td>Unified Perceptual Parsing Network</td> <td>A unified perceptual parsing network based on the transformer model, designed for image segmentation tasks.</td> <td>Semantic Segmentation, Image Parsing</td> <td>-</td> </tr> <tr> <td>93</td> <td>Vision Transformer (ViT)</td> <td>Transformer-based model for image classification</td> <td>A transformer-based model designed for image classification tasks, replacing convolutional layers with self-attention.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>94</td> <td>Wav2Vec2</td> <td>Self-supervised Audio Representation Learning</td> <td>A transformer-based model for self-supervised audio representation learning, capturing phonetic information.</td> <td>Speech Recognition, Speech Representation Learning</td> <td>-</td> </tr> <tr> <td>95</td> <td>PLBart</td> <td>Pre-trained Language model for BART</td> <td>A pre-trained variant of BART (Bidirectional and AutoRegressive Transformers) for various NLP tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>96</td> <td>DiT</td> <td>Vision transformer for image classification</td> <td>A transformer-based model for image classification tasks that applies vision transformers to process image data.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>97</td> <td>DPR</td> <td>Dense Passage Retrieval</td> <td>A transformer-based model for dense passage retrieval, enabling efficient and accurate retrieval of relevant passages.</td> <td>Passage Retrieval, Document Ranking</td> <td>-</td> </tr> <tr> <td>98</td> <td>GLPN</td> <td>Learning global-local patterns in natural language processing</td> <td>A transformer-based model that captures both global and local patterns in text for various natural language processing tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>99</td> <td>LeViT</td> <td>Vision transformer with less computations</td> <td>A vision transformer model that reduces computational requirements by using fewer computations.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>100</td> <td>NAT</td> <td>Neural Architecture Transformer</td> <td>A transformer-based model that learns to design neural architectures for various tasks.</td> <td>Neural Architecture Search, AutoML</td> <td>-</td> </tr> <tr> <td>101</td> <td>TAPEX</td> <td>Transformer model for text and program execution</td> <td>A transformer-based model capable of executing programs described in natural language text.</td> <td>Text-to-Program Execution, Natural Language Processing</td> <td>-</td> </tr> <tr> <td>102</td> <td>VideoMAE</td> <td>Video Motion Analysis Encoder</td> <td>A transformer-based model for video motion analysis tasks, encoding motion information in videos.</td> <td>-</td> <td>Video Motion Analysis, Action Recognition, Video Understanding</td> </tr> <tr> <td>103</td> <td>Wav2Vec2-Conformer</td> <td>Conformer-based variant of Wav2Vec2</td> <td>A variant of Wav2Vec2 that incorporates Conformer architecture, improving its performance on speech-related tasks.</td> <td>Speech Recognition, Speech Representation Learning</td> <td>-</td> </tr> <tr> <td>104</td> <td>CLIP</td> <td>Image-text matching and zero-shot learning</td> <td>A transformer-based model that learns to match images and text, enabling zero-shot learning capabilities.</td> <td>-</td> <td>Image-Text Matching, Zero-Shot Learning</td> </tr> <tr> <td>105</td> <td>XLS-R</td> <td>Cross-lingual Speech Recognition</td> <td>A transformer-based model for cross-lingual speech recognition, trained on multilingual speech data.</td> <td>Cross-lingual Speech Recognition</td> <td>-</td> </tr> <tr> <td>106</td> <td>Audio Spectrogram Transformer</td> <td>Processing audio spectrograms</td> <td>A transformer model specifically designed for processing audio spectrograms.</td> <td>Automatic Speech Recognition (ASR), Sound Classification</td> <td>-</td> </tr> <tr> <td>107</td> <td>M2M100</td> <td>Multilingual Multimodal Transformer</td> <td>A transformer-based model capable of multilingual and multimodal tasks, trained on 100 different languages.</td> <td>Machine Translation, Multilingual Text Classification, Multimodal Tasks</td> <td>-</td> </tr> <tr> <td>108</td> <td>MEGA</td> <td>Multilingual Language Generation with Transformers</td> <td>A transformer-based model for multilingual language generation tasks, capable of producing text in multiple languages.</td> <td>Text Generation, Text Completion, Multilingual Language Modeling</td> <td>-</td> </tr> <tr> <td>109</td> <td>BEiT</td> <td>Vision transformer for image classification</td> <td>Combines concepts from CNNs and transformers for image classification tasks.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>110</td> <td>BigBird-Pegasus</td> <td>Text generation and summarization</td> <td>A variant of the Pegasus model that incorporates the BigBird sparse attention mechanism.</td> <td>Text Generation, Summarization</td> <td>-</td> </tr> <tr> <td>111</td> <td>BigBird-RoBERTa</td> <td>Classification and named entity recognition</td> <td>A variant of the RoBERTa model that incorporates the BigBird sparse attention mechanism.</td> <td>Classification, Named Entity Recognition (NER)</td> <td>-</td> </tr> <tr> <td>112</td> <td>CLIPSeg</td> <td>Image segmentation</td> <td>A transformer-based model for image segmentation tasks.</td> <td>-</td> <td>Image Segmentation</td> </tr> <tr> <td>113</td> <td>DPT</td> <td>Object detection and instance segmentation with deformable attention</td> <td>A transformer-based model for object detection and instance segmentation tasks, incorporating deformable attention mechanisms.</td> <td>-</td> <td>Object Detection, Instance Segmentation</td> </tr> <tr> <td>114</td> <td>Perceiver IO</td> <td>Perceiver with Input/output processing</td> <td>A transformer model architecture that handles input and output processing jointly, enabling cross-modal tasks.</td> <td>Multimodal Tasks</td> <td>-</td> </tr> <tr> <td>115</td> <td>Reformer</td> <td>Memory-efficient Transformer</td> <td>A transformer model variant designed to be more memory-efficient by using reversible layers.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>116</td> <td>RoFormer</td> <td>Robustly optimized Transformer variant for images</td> <td>A transformer-based model specifically designed for image-related tasks, leveraging self-attention mechanisms.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>117</td> <td>Swin Transformer</td> <td>Shifted Window Transformer</td> <td>A transformer model that uses shifted windows to capture long-range dependencies in images.</td> <td>-</td> <td>Image Classification, Object Detection</td> </tr> <tr> <td>118</td> <td>TrOCR</td> <td>Transformer-based OCR model</td> <td>A transformer-based model designed for Optical Character Recognition (OCR) tasks, converting images to text.</td> <td>Optical Character Recognition (OCR)</td> <td>-</td> </tr> <tr> <td>119</td> <td>Wav2Vec2Phoneme</td> <td>Phoneme-level variants of Wav2Vec2</td> <td>Phoneme-level variants of Wav2Vec2 designed for speech recognition tasks at the phoneme level.</td> <td>Phoneme-level Speech Recognition</td> <td>-</td> </tr> <tr> <td>120</td> <td>X-CLIP</td> <td>Cross-modal Learning with CLIP</td> <td>A transformer-based model for cross-modal learning, incorporating the CLIP framework.</td> <td>-</td> <td>Vision-Language Tasks, Cross-modal Learning</td> </tr> <tr> <td>121</td> <td>XLSR-Wav2Vec2</td> <td>Cross-lingual Speech Representation</td> <td>A variant of Wav2Vec2 trained for cross-lingual speech representation learning.</td> <td>Cross-lingual Speech Representation</td> <td>-</td> </tr> <tr> <td>122</td> <td>Blenderbot</td> <td>Conversational AI chatbot</td> <td>A chatbot model designed for multi-turn conversations that combines language and dialogue understanding.</td> <td>-</td> <td>-</td> </tr> <tr> <td>123</td> <td>BlenderbotSmall</td> <td>Conversational AI chatbot</td> <td>A smaller version of Blenderbot, designed for multi-turn conversations with language and dialogue understanding capabilities.</td> <td>-</td> <td>-</td> </tr> <tr> <td>124</td> <td>BLIP</td> <td>Image classification and image captioning</td> <td>A transformer-based model for image classification and image captioning tasks.</td> <td>-</td> <td>Image Classification, Image Captioning</td> </tr> <tr> <td>125</td> <td>ByT5</td> <td>Text translation, classification, and question answering</td> <td>A transformer-based model trained on T5 architecture, suitable for text translation, classification, and question answering tasks.</td> <td>Translation, Text Classification, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>126</td> <td>CvT</td> <td>Cross Vision and Transformer</td> <td>A transformer-based model that combines vision and language understanding, enabling cross-modal tasks in computer vision.</td> <td>-</td> <td>Image-Text Matching, Vision-Language Tasks</td> </tr> <tr> <td>127</td> <td>DeBERTa-v2</td> <td>Improved version of DeBERTa</td> <td>An updated version of DeBERTa with improved performance and compatibility for various NLP tasks.</td> <td>Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>128</td> <td>DeiT</td> <td>Vision transformer for image classification</td> <td>A vision transformer model designed for image classification tasks.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>129</td> <td>GroupViT</td> <td>Vision transformer with group-based operations</td> <td>A vision transformer model that incorporates group-based operations to enhance its representation capacity.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>130</td> <td>LayoutLMv2</td> <td>Improved version of LayoutLM for document analysis</td> <td>An enhanced version of LayoutLM with improved performance and additional capabilities for document analysis.</td> <td>Document Understanding, OCR, Named Entity Recognition (NER)</td> <td>-</td> </tr> <tr> <td>131</td> <td>MaskFormer</td> <td>Masked Language Modeling with Transformers</td> <td>A transformer-based model architecture for masked language modeling tasks, such as pretraining BERT.</td> <td>Language Modeling, Pretraining BERT</td> <td>-</td> </tr> <tr> <td>132</td> <td>SegFormer</td> <td>Segmentation Transformer for computer vision</td> <td>A transformer-based model designed for image segmentation tasks in computer vision.</td> <td>Semantic Segmentation, Object Detection</td> <td>-</td> </tr> <tr> <td>133</td> <td>Time Series Transformer</td> <td>Transformer model for time series data</td> <td>A transformer-based model designed specifically for time series data analysis and forecasting tasks.</td> <td>Time Series Forecasting, Anomaly Detection, Sequence Modeling</td> <td>-</td> </tr> <tr> <td>134</td> <td>TimeSformer</td> <td>Time Series Transformer for video analysis</td> <td>A transformer-based model for video analysis and action recognition tasks, leveraging temporal information.</td> <td>-</td> <td>Video Action Recognition, Temporal Modeling</td> </tr> <tr> <td>135</td> <td>Trajectory Transformer</td> <td>Transformer model for trajectory forecasting</td> <td>A transformer-based model designed for trajectory forecasting tasks, such as predicting object movement.</td> <td>Trajectory Forecasting, Object Movement Prediction</td> <td>-</td> </tr> <tr> <td>136</td> <td>UniSpeech</td> <td>Unified Speech Recognition and Synthesis Transformer</td> <td>A unified transformer-based model for both speech recognition and speech synthesis tasks.</td> <td>Speech Recognition, Text-to-Speech Synthesis</td> <td>-</td> </tr> <tr> <td>137</td> <td>UniSpeechSat</td> <td>Self-supervised pre-training for UniSpeech</td> <td>A self-supervised pre-training method for UniSpeech, improving its performance on speech-related tasks.</td> <td>Speech Recognition, Text-to-Speech Synthesis</td> <td>-</td> </tr> <tr> <td>138</td> <td>ALIGN</td> <td>Joint representation learning for textual and tabular data</td> <td>Enables joint representation learning by aligning textual and tabular data.</td> <td>Text-Tabular Alignment, Joint Representation Learning</td> <td>-</td> </tr> <tr> <td>139</td> <td>BORT</td> <td>Language modeling and reinforcement learning</td> <td>A transformer-based model for language modeling and reinforcement learning tasks.</td> <td>Language Modeling, Text Generation</td> <td>-</td> </tr> <tr> <td>140</td> <td>DePlot</td> <td>Data visualization</td> <td>A transformer-based model that generates interactive and informative visualizations from data.</td> <td>Data Visualization</td> <td>-</td> </tr> <tr> <td>141</td> <td>DETA</td> <td>Document extraction and text analysis</td> <td>A transformer-based model for document extraction, information retrieval, and text analysis tasks.</td> <td>Document Extraction, Information Retrieval, Text Analysis</td> <td>-</td> </tr> <tr> <td>142</td> <td>DiNAT</td> <td>Network traffic anomaly detection</td> <td>A transformer-based model for network traffic anomaly detection, specifically designed for cybersecurity applications.</td> <td>Network Traffic Analysis, Anomaly Detection</td> <td>-</td> </tr> <tr> <td>143</td> <td>Jukebox</td> <td>Music generation with transformers</td> <td>A transformer-based model architecture for generating music with various styles and genres.</td> <td>Music Generation</td> <td>-</td> </tr> <tr> <td>144</td> <td>mBART-50</td> <td>Compact version of mBART for resource-constrained</td> <td>A compact version of mBART with reduced parameters and computational requirements.</td> <td>Text Generation, Text Completion, Multilingual Language Modeling</td> <td>-</td> </tr> <tr> <td>145</td> <td>Nystrmformer</td> <td>Approximating Full Transformers with Nystrm</td> <td>A transformer variant that approximates full self-attention using the Nystrm method for efficiency.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>146</td> <td>ViT Hybrid</td> <td>Hybrid Architecture of Vision Transformer</td> <td>A hybrid architecture that combines vision transformer with convolutional neural networks for image understanding.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>147</td> <td>X-MOD</td> <td>Cross-modal Language Modeling</td> <td>A transformer-based model for cross-modal language modeling, integrating vision and text.</td> <td>-</td> <td>Vision-Language Tasks, Cross-modal Language Modeling</td> </tr> <tr> <td>148</td> <td>BARTpho</td> <td>Text-to-speech synthesis</td> <td>A variant of BART model for text-to-speech synthesis tasks.</td> <td>Text-to-Speech Synthesis</td> <td>-</td> </tr> <tr> <td>149</td> <td>BridgeTower</td> <td>Language modeling and text generation</td> <td>A transformer-based model for language modeling and text generation tasks.</td> <td>Text Generation, Language Modeling</td> <td>-</td> </tr> <tr> <td>150</td> <td>CodeGen</td> <td>Code generation</td> <td>A transformer-based model for generating code.</td> <td>Code Generation</td> <td>-</td> </tr> <tr> <td>151</td> <td>GPT-J</td> <td>Japanese language variant of GPT-2</td> <td>A version of GPT-2 specifically designed and trained for Japanese language understanding and generation tasks.</td> <td>Japanese Language Processing, Text Generation</td> <td>-</td> </tr> <tr> <td>152</td> <td>LLaMA</td> <td>Label-agnostic learning with transformers</td> <td>A transformer-based model that learns to perform tasks without explicit labels, leveraging self-supervision.</td> <td>Self-Supervised Learning, Representation Learning, Clustering</td> <td>-</td> </tr> <tr> <td>153</td> <td>MarkupLM</td> <td>Transformer for document structure understanding</td> <td>A transformer-based model for understanding document structure and semantic relationships in text.</td> <td>Document Structure Understanding, Semantic Analysis</td> <td>-</td> </tr> <tr> <td>154</td> <td>PoolFormer</td> <td>Pooling-based Vision Transformer</td> <td>A vision transformer model that incorporates pooling operations for handling images of varying sizes.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>155</td> <td>QDQBert</td> <td>Query-Doc Bidirectional Transformer</td> <td>A transformer model specifically designed for query-document ranking and retrieval tasks.</td> <td>Information Retrieval, Question Answering, Document Ranking</td> <td>-</td> </tr> <tr> <td>156</td> <td>ViLT</td> <td>Vision-and-Language Transformer</td> <td>A transformer-based model that combines vision and language understanding for multimodal tasks.</td> <td>-</td> <td>Vision-Language Tasks, Image Captioning, Visual Question Answering (VQA)</td> </tr> <tr> <td>157</td> <td>BARThez</td> <td>Text generation and summarization</td> <td>A variant of BART model trained specifically for the French language.</td> <td>Text Generation, Summarization</td> <td>-</td> </tr> <tr> <td>158</td> <td>Donut</td> <td>Anomaly detection in time series data</td> <td>A transformer-based model for detecting anomalies in time series data, suitable for various applications such as monitoring systems.</td> <td>-</td> <td>Anomaly Detection, Time Series Analysis</td> </tr> <tr> <td>159</td> <td>ImageGPT</td> <td>Image generation with transformers</td> <td>A transformer-based model architecture for generating images based on text prompts.</td> <td>-</td> <td>Image Generation</td> </tr> <tr> <td>160</td> <td>OPT</td> <td>Optimization Pretraining Transformer</td> <td>A transformer model pre-trained for optimization tasks, such as combinatorial optimization and planning.</td> <td>Combinatorial Optimization, Planning</td> <td>-</td> </tr> <tr> <td>161</td> <td>Splinter</td> <td>Speech and Language Integrated Transformer</td> <td>A transformer-based model designed for integrating speech and language tasks.</td> <td>Speech-to-Text Conversion, Speech Recognition, Natural Language Processing</td> <td>-</td> </tr> <tr> <td>162</td> <td>XGLM</td> <td>Cross-lingual Language Modeling</td> <td>A transformer-based model for cross-lingual language modeling, learning representations across languages.</td> <td>Cross-lingual Language Modeling</td> <td>-</td> </tr> <tr> <td>163</td> <td>YOSO</td> <td>You Only Speak Once</td> <td>A transformer-based model for low-resource machine translation, using only monolingual data.</td> <td>Low-resource Machine Translation</td> <td>-</td> </tr> <tr> <td>164</td> <td>EfficientFormer</td> <td>Efficient transformer architecture for sequence modeling</td> <td>A transformer-based model architecture designed to improve efficiency and performance for sequence modeling tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Machine Translation</td> <td>-</td> </tr> <tr> <td>165</td> <td>ESM</td> <td>Protein structure prediction</td> <td>A transformer-based model for predicting the 3D structure of proteins from their amino acid sequences.</td> <td>Protein Structure Prediction, Bioinformatics</td> <td>-</td> </tr> <tr> <td>166</td> <td>Mask2Former</td> <td>Transformer-based masked image inpainting</td> <td>A transformer-based model for masked image inpainting, reconstructing missing parts of an image.</td> <td>-</td> <td>Image Inpainting</td> </tr> <tr> <td>167</td> <td>MGP-STR</td> <td>Music Generation with Pre-trained Model</td> <td>A pre-trained model for generating music, leveraging a transformer-based architecture.</td> <td>Music Generation</td> <td>-</td> </tr> <tr> <td>168</td> <td>NLLB</td> <td>Natural Language Logic Board</td> <td>A model that combines natural language understanding and symbolic logic reasoning for language understanding.</td> <td>Natural Language Understanding, Logic Reasoning</td> <td>-</td> </tr> <tr> <td>169</td> <td>T5v1.1</td> <td>Version 1.1 of the Text-to-Text Transfer Transformer</td> <td>An updated version of the T5 model with improvements and enhancements for better performance.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>170</td> <td>TVLT</td> <td>Tiny Vision-Language Transformer</td> <td>A compact vision-language transformer model designed for efficient processing of vision and language inputs.</td> <td>-</td> <td>Vision-Language Tasks, Image Captioning, Visual Question Answering (VQA)</td> </tr> <tr> <td>171</td> <td>WavLM</td> <td>Language Modeling for Speech</td> <td>A transformer-based model for language modeling on speech data.</td> <td>Speech Language Modeling</td> <td>-</td> </tr> <tr> <td>172</td> <td>XLM-RoBERTa-XL</td> <td>Cross-lingual Language Representation</td> <td>A larger variant of XLM-RoBERTa for cross-lingual language representation learning.</td> <td>Cross-lingual Language Representation</td> <td>-</td> </tr> <tr> <td>173</td> <td>Chinese-CLIP</td> <td>Chinese language processing and image-text matching</td> <td>A transformer-based model designed for Chinese language processing and image-text matching tasks.</td> <td>Chinese Language Processing, Image-Text Matching</td> <td>-</td> </tr> <tr> <td>174</td> <td>CLAP</td> <td>Image-text representation learning</td> <td>A transformer-based model for learning joint image-text representations.</td> <td>-</td> <td>Image-Text Representation Learning</td> </tr> <tr> <td>175</td> <td>Decision Transformer</td> <td>Decision-making tasks</td> <td>A transformer-based model designed for decision-making tasks that require complex reasoning and inference.</td> <td>Decision-Making, Reasoning, Inference</td> <td>-</td> </tr> <tr> <td>176</td> <td>BLIP-2</td> <td>Image classification</td> <td>An updated version of BLIP, specializing in image classification tasks.</td> <td>-</td> <td>Image Classification</td> </tr> <tr> <td>177</td> <td>CANINE</td> <td>Document classification</td> <td>A transformer-based model for document classification tasks.</td> <td>Document Classification</td> <td>-</td> </tr> <tr> <td>178</td> <td>Graphormer</td> <td>Graph representation learning with transformers</td> <td>A transformer-based model architecture specifically designed for graph representation learning.</td> <td>Graph Representation Learning, Node Classification, Graph Classification, Graph Generation</td> <td>-</td> </tr> <tr> <td>179</td> <td>I-BERT</td> <td>Incremental learning with transformers</td> <td>A transformer-based model architecture that supports incremental learning, allowing continual model updates.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>180</td> <td>MatCha</td> <td>Matching Challenge Transformer</td> <td>A transformer-based model for solving matching challenge tasks, such as natural language inference.</td> <td>Natural Language Inference, Textual Entailment</td> <td>-</td> </tr> <tr> <td>181</td> <td>mLUKE</td> <td>Multilingual Language Understanding with Knowledge</td> <td>A multilingual model that incorporates knowledge-based entities for language understanding tasks.</td> <td>Named Entity Recognition (NER), Relation Extraction, Knowledge Graph Completion</td> <td>-</td> </tr> <tr> <td>182</td> <td>MobileViT</td> <td>Vision Transformer for Mobile and Edge Devices</td> <td>A mobile-friendly version of Vision Transformer, optimized for efficient deployment on mobile and edge devices.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>183</td> <td>OWL-ViT</td> <td>Object-Wide Learning Vision Transformer</td> <td>A vision transformer model designed for object detection and recognition tasks in computer vision.</td> <td>Object Detection, Object Recognition</td> <td>-</td> </tr> <tr> <td>184</td> <td>SpeechT5</td> <td>T5-based model for Speech-to-Text</td> <td>A transformer-based model trained for speech-to-text conversion tasks using the T5 architecture.</td> <td>Speech-to-Text Conversion</td> <td>-</td> </tr> <tr> <td>185</td> <td>Swin Transformer V2</td> <td>Advanced version of Swin Transformer</td> <td>An advanced version of the Swin Transformer model, incorporating improvements for better performance in vision tasks.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>186</td> <td>ViTMAE</td> <td>Vision Transformer for Multi-label Image Classification</td> <td>A vision transformer model designed specifically for multi-label image classification tasks.</td> <td>-</td> <td>Multi-label Image Classification</td> </tr> <tr> <td>187</td> <td>BLOOM</td> <td>Language modeling and text generation</td> <td>A transformer-based model designed for language modeling and text generation tasks.</td> <td>Text Generation, Language Modeling</td> <td>-</td> </tr> <tr> <td>188</td> <td>ConvNeXTV2</td> <td>Language modeling and text generation</td> <td>An improved version of ConvNeXT for language modeling and text generation tasks.</td> <td>Language Modeling, Text Generation</td> <td>-</td> </tr> <tr> <td>189</td> <td>CPM-Ant</td> <td>Chinese language processing and text generation</td> <td>An enhanced version of CPM with better performance and compatibility for Chinese language processing and text generation tasks.</td> <td>Chinese Language Processing, Text Generation</td> <td>-</td> </tr> <tr> <td>190</td> <td>GPT-Sw3</td> <td>Swedish language variant of GPT</td> <td>A version of GPT specifically designed and trained for Swedish language understanding and generation tasks.</td> <td>Swedish Language Processing, Text Generation</td> <td>-</td> </tr> <tr> <td>191</td> <td>LongT5</td> <td>Text-to-Text Transfer Transformer</td> <td>A transformer-based model for text-to-text transfer learning, capable of performing various NLP tasks.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>192</td> <td>OneFormer</td> <td>Transformer for Text-to-Text Transfer Learning</td> <td>A transformer-based model designed for text-to-text transfer learning tasks across multiple languages.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA)</td> <td>-</td> </tr> <tr> <td>193</td> <td>Table Transformer</td> <td>Transformer model for table-related tasks</td> <td>A transformer-based model specifically designed for table-related tasks, such as table understanding and extraction.</td> <td>Table Understanding, Table Extraction</td> <td>-</td> </tr> <tr> <td>194</td> <td>VAN</td> <td>Vision-Adaptive Transformer for Video Analysis</td> <td>A transformer model designed specifically for video analysis tasks, adapting to the dynamic visual context.</td> <td>-</td> <td>Video Classification, Action Recognition, Video Understanding</td> </tr> <tr> <td>195</td> <td>AltCLIP</td> <td>Predicting the relationship between two images</td> <td>A transformer-based model that learns to predict the relationship between two images.</td> <td>-</td> <td>Image-Text Matching, Vision-Language Tasks</td> </tr> <tr> <td>196</td> <td>MVP</td> <td>Multimodal Variational Pretraining</td> <td>A multimodal pretraining framework that combines text and image modalities for various downstream tasks.</td> <td>Multimodal Tasks</td> <td>-</td> </tr> <tr> <td>197</td> <td>NLLB-MOE</td> <td>Natural Language Logic Board with MOE</td> <td>An enhanced version of NLLB that incorporates Mixture of Experts (MOE) for improved performance.</td> <td>Natural Language Understanding, Logic Reasoning</td> <td>-</td> </tr> <tr> <td>198</td> <td>PEGASUS-X</td> <td>Large-Scale Pre-training for Abstractive Summarization</td> <td>A variant of Pegasus with larger model capacity, trained on a large-scale corpus for abstractive summarization.</td> <td>Text Summarization</td> <td>-</td> </tr> <tr> <td>199</td> <td>Swin2SR</td> <td>Swin Transformer for Super-Resolution</td> <td>A variant of the Swin Transformer model specifically designed for super-resolution tasks in computer vision.</td> <td>-</td> <td>Super-Resolution Image Reconstruction</td> </tr> <tr> <td>200</td> <td>UL2</td> <td>Unsupervised Language Learning</td> <td>A transformer-based model designed for unsupervised language learning tasks, leveraging self-supervised learning techniques.</td> <td>Language Modeling, Text Representation Learning</td> <td>-</td> </tr> <tr> <td>201</td> <td>ViTMSN</td> <td>Vision Transformer with Masked Spatial Neurons</td> <td>A vision transformer model with masked spatial neurons, enabling better spatial representation learning.</td> <td>-</td> <td>Image Classification, Object Detection, Semantic Segmentation</td> </tr> <tr> <td>202</td> <td>YOLOS</td> <td>You Only Learn One Sentence</td> <td>A transformer-based model that learns sentence representations for zero-shot classification.</td> <td>Zero-shot Text Classification</td> <td>-</td> </tr> <tr> <td>203</td> <td>FLAN-T5</td> <td>Fast and lightweight adapter-based transformers for T5</td> <td>A transformer-based model architecture that enables efficient and lightweight adaptation of T5 models.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Machine Translation</td> <td>-</td> </tr> <tr> <td>204</td> <td>GPT NeoX Japanese</td> <td>Japanese language variant of GPT NeoX</td> <td>A version of GPT NeoX specifically designed and trained for Japanese language understanding and generation tasks.</td> <td>Japanese Language Processing, Text Generation</td> <td>-</td> </tr> <tr> <td>205</td> <td>LayoutLMv3</td> <td>Further improved version of LayoutLM for documents</td> <td>An advanced version of LayoutLM that incorporates additional enhancements and optimizations.</td> <td>Document Understanding, OCR, Named Entity Recognition (NER)</td> <td>-</td> </tr> <tr> <td>206</td> <td>FLAN-UL2</td> <td>Fast and lightweight adapter-based transformers for UL2</td> <td>A transformer-based model architecture that enables efficient and lightweight adaptation of UL2 models.</td> <td>Text Classification, Named Entity Recognition (NER), Sentiment Analysis, Machine Translation</td> <td>-</td> </tr> <tr> <td>207</td> <td>FLAVA</td> <td>Fluency and acceptability evaluation for machine translation</td> <td>A transformer-based model that evaluates the fluency and acceptability of machine translations.</td> <td>Machine Translation Evaluation</td> <td>-</td> </tr> </tbody> </table> <h2 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">&para;</a></h2> <p>The purpose of this article is to give you a general understanding of the capabilities of the Transformer architecture. It is now up to you to decide which architecture is most suitable to your needs based on the task you have in front of you. Afterwards, you can use hugginface or tfhub to see if there are already models that have been trained using these architectures. The chances are that you will be able to complete your work using zero-shot transfer learning are high.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>