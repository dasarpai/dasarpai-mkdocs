<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Machine-Learning-Key-Concepts.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Machine Learning Key Concepts - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta name=author content="Hari Thapliyaal"><meta name=description content="A comprehensive guide to fundamental machine learning concepts, covering essential topics from model evaluation and cross-validation to advanced techniques like ensemble methods and explainable AI. Learn about hyperparameter tuning, feature engineering, regularization, dimensionality reduction, and best practices for building robust machine learning models. Includes detailed explanations of key algorithms, evaluation metrics, and practical implementation strategies."><meta name=keywords content="m, a, c, h, i, n, e,  , l, e, a, r, n, i, n, g,  , c, o, n, c, e, p, t, s, ,,  , m, o, d, e, l,  , e, v, a, l, u, a, t, i, o, n, ,,  , c, r, o, s, s,  , v, a, l, i, d, a, t, i, o, n, ,,  , h, y, p, e, r, p, a, r, a, m, e, t, e, r,  , t, u, n, i, n, g, ,,  , g, r, i, d,  , s, e, a, r, c, h, ,,  , f, e, a, t, u, r, e,  , e, n, g, i, n, e, e, r, i, n, g, ,,  , r, e, g, u, l, a, r, i, z, a, t, i, o, n,  , t, e, c, h, n, i, q, u, e, s, ,,  , b, i, a, s, -, v, a, r, i, a, n, c, e,  , t, r, a, d, e, o, f, f, ,,  , e, n, s, e, m, b, l, e,  , m, e, t, h, o, d, s, ,,  , d, i, m, e, n, s, i, o, n, a, l, i, t, y,  , r, e, d, u, c, t, i, o, n, ,,  , m, o, d, e, l,  , o, p, t, i, m, i, z, a, t, i, o, n, ,,  , k, e, r, n, e, l,  , m, e, t, h, o, d, s, ,,  , c, l, u, s, t, e, r, i, n, g,  , a, l, g, o, r, i, t, h, m, s, ,,  , n, e, u, r, a, l,  , n, e, t, w, o, r, k, s, ,,  , e, x, p, l, a, i, n, a, b, l, e,  , A, I, ,,  , u, n, c, e, r, t, a, i, n, t, y,  , q, u, a, n, t, i, f, i, c, a, t, i, o, n, ,,  , c, o, n, t, i, n, u, a, l,  , l, e, a, r, n, i, n, g, ,,  , m, o, d, e, l,  , p, e, r, f, o, r, m, a, n, c, e,  , m, e, t, r, i, c, s, ,,  , m, a, c, h, i, n, e,  , l, e, a, r, n, i, n, g,  , b, e, s, t,  , p, r, a, c, t, i, c, e, s, ,,  , d, e, e, p,  , l, e, a, r, n, i, n, g,  , c, o, n, c, e, p, t, s"><meta property=og:type content=article><meta property=og:locale content=en_US><meta property=og:site_name content=DasarpAI><meta property=og:title content="Machine Learning Key Concepts"><meta property=og:description content="A comprehensive guide to fundamental machine learning concepts, covering essential topics from model evaluation and cross-validation to advanced techniques like ensemble methods and explainable AI. Learn about hyperparameter tuning, feature engineering, regularization, dimensionality reduction, and best practices for building robust machine learning models. Includes detailed explanations of key algorithms, evaluation metrics, and practical implementation strategies."><meta property=og:url content=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Machine-Learning-Key-Concepts.html><meta property=og:image content=../../assets/images/dspost/dsp6152-Machine-Learning-Key-Concepts.jpg><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta name=twitter:card content=summary_large_image><meta name=twitter:site content=@dasarpai><meta name=twitter:title content="Machine Learning Key Concepts"><meta name=twitter:description content="A comprehensive guide to fundamental machine learning concepts, covering essential topics from model evaluation and cross-validation to advanced techniques like ensemble methods and explainable AI. Learn about hyperparameter tuning, feature engineering, regularization, dimensionality reduction, and best practices for building robust machine learning models. Includes detailed explanations of key algorithms, evaluation metrics, and practical implementation strategies."><meta name=twitter:image content=../../assets/images/dspost/dsp6152-Machine-Learning-Key-Concepts.jpg><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Machine-Learning-Key-Concepts.html><link rel=stylesheet href=../assets/stylesheets/custom.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#machine-learning-key-concepts class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@harithapliyal</strong> on <a rel=me href=https://linkedin.com/in/harithapliyal> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/dasarpai> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Machine Learning Key Concepts </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#is-this-article-for-me class=md-nav__link> <span class=md-ellipsis> Is this article for me? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cross-validation class=md-nav__link> <span class=md-ellipsis> What is Cross-validation? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-advantages-of-cross-validation class=md-nav__link> <span class=md-ellipsis> What is Advantages of Cross-Validation? </span> </a> </li> <li class=md-nav__item> <a href=#lets-assume-we-have-dataset-of-1000-sample-i-decide-to-to-5-fold-cross-validation-then-how-many-models-will-be-build class=md-nav__link> <span class=md-ellipsis> Let's assume we have dataset of 1000 sample, I decide to to 5-fold cross validation then how many models will be build? </span> </a> </li> <li class=md-nav__item> <a href=#these-5-models-will-have-different-performance-and-based-on-the-selected-metrics-we-finally-select-one-model-is-this-correct class=md-nav__link> <span class=md-ellipsis> These 5 models will have different performance and based on the selected metrics we finally select one model, is this correct? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-the-use-of-the-averaging-the-performance-of-5-models-how-does-it-help-in-final-model-creation-because-hyperparameter-will-remain-same-the-only-change-we-have-instead-of-4-fold-we-have-all-5-folds-for-model-training class=md-nav__link> <span class=md-ellipsis> What is the use of the averaging the performance of 5 models? How does it help in final model creation, because hyperparameter will remain same. The only change we have instead of 4 fold, we have all 5 folds for model training. </span> </a> <nav class=md-nav aria-label="What is the use of the averaging the performance of 5 models? How does it help in final model creation, because hyperparameter will remain same. The only change we have instead of 4 fold, we have all 5 folds for model training."> <ul class=md-nav__list> <li class=md-nav__item> <a href=#why-averaging-the-performance-of-cross-validation-models-matters class=md-nav__link> <span class=md-ellipsis> Why Averaging the Performance of Cross-Validation Models Matters: </span> </a> </li> <li class=md-nav__item> <a href=#how-does-cross-validation-help-in-final-model-creation class=md-nav__link> <span class=md-ellipsis> How Does Cross-Validation Help in Final Model Creation? </span> </a> </li> <li class=md-nav__item> <a href=#why-not-just-train-on-the-full-data-from-the-beginning class=md-nav__link> <span class=md-ellipsis> Why Not Just Train on the Full Data from the Beginning? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#when-should-i-use-cross-validation class=md-nav__link> <span class=md-ellipsis> When should I use Cross-Validation? </span> </a> <nav class=md-nav aria-label="When should I use Cross-Validation?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-when-dataset-is-small class=md-nav__link> <span class=md-ellipsis> 1. When dataset is Small: </span> </a> </li> <li class=md-nav__item> <a href=#2-when-you-want-to-tune-hyperparameter class=md-nav__link> <span class=md-ellipsis> 2. When you want to tune Hyperparameter: </span> </a> </li> <li class=md-nav__item> <a href=#3-it-helps-avoiding-model-overfitting class=md-nav__link> <span class=md-ellipsis> 3. It helps Avoiding model Overfitting: </span> </a> </li> <li class=md-nav__item> <a href=#4-when-the-dataset-is-imbalanced class=md-nav__link> <span class=md-ellipsis> 4. When the Dataset is Imbalanced: </span> </a> </li> <li class=md-nav__item> <a href=#5-when-you-need-to-select-a-model class=md-nav__link> <span class=md-ellipsis> 5. When you need to Select a Model: </span> </a> </li> <li class=md-nav__item> <a href=#6-when-we-have-time-constraints-for-creating-a-train-test-split class=md-nav__link> <span class=md-ellipsis> 6. When we have Time Constraints for Creating a Train-Test Split: </span> </a> </li> <li class=md-nav__item> <a href=#7-when-you-have-no-separate-test-data class=md-nav__link> <span class=md-ellipsis> 7. When You Have No Separate Test Data: </span> </a> </li> <li class=md-nav__item> <a href=#8-time-series-data-with-rolling-windows class=md-nav__link> <span class=md-ellipsis> 8. Time Series Data (with Rolling Windows): </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#when-cross-validation-might-not-be-ideal class=md-nav__link> <span class=md-ellipsis> When Cross-Validation Might Not Be Ideal? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-grid-search class=md-nav__link> <span class=md-ellipsis> What is Grid Search? </span> </a> <nav class=md-nav aria-label="What is Grid Search?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#how-grid-search-works class=md-nav__link> <span class=md-ellipsis> How Grid Search Works? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-is-random-search class=md-nav__link> <span class=md-ellipsis> What is random search? </span> </a> <nav class=md-nav aria-label="What is random search?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#how-random-search-works class=md-nav__link> <span class=md-ellipsis> How Random Search Works? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#when-to-use-grid-search-vs-random-search class=md-nav__link> <span class=md-ellipsis> When to Use Grid Search vs. Random Search? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-popular-hyperparameter-optimization-techniques class=md-nav__link> <span class=md-ellipsis> What are popular hyperparameter optimization techniques? </span> </a> <nav class=md-nav aria-label="What are popular hyperparameter optimization techniques?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-grid-search class=md-nav__link> <span class=md-ellipsis> 1. Grid Search: </span> </a> </li> <li class=md-nav__item> <a href=#2-random-search class=md-nav__link> <span class=md-ellipsis> 2. Random Search: </span> </a> </li> <li class=md-nav__item> <a href=#3-bayesian-optimization class=md-nav__link> <span class=md-ellipsis> 3. Bayesian Optimization: </span> </a> </li> <li class=md-nav__item> <a href=#4-hyperband class=md-nav__link> <span class=md-ellipsis> 4. Hyperband: </span> </a> </li> <li class=md-nav__item> <a href=#5-tree-structured-parzen-estimator-tpe class=md-nav__link> <span class=md-ellipsis> 5. Tree-structured Parzen Estimator (TPE): </span> </a> </li> <li class=md-nav__item> <a href=#6-sequential-model-optimization-smo class=md-nav__link> <span class=md-ellipsis> 6. Sequential Model Optimization (SMO): </span> </a> </li> <li class=md-nav__item> <a href=#7-evolutionary-algorithms class=md-nav__link> <span class=md-ellipsis> 7. Evolutionary Algorithms: </span> </a> </li> <li class=md-nav__item> <a href=#8-simulated-annealing class=md-nav__link> <span class=md-ellipsis> 8. Simulated Annealing: </span> </a> </li> <li class=md-nav__item> <a href=#9-genetic-algorithms class=md-nav__link> <span class=md-ellipsis> 9. Genetic Algorithms: </span> </a> </li> <li class=md-nav__item> <a href=#10-cross-validation class=md-nav__link> <span class=md-ellipsis> 10. Cross-Validation: </span> </a> </li> <li class=md-nav__item> <a href=#11-automated-machine-learning-automl class=md-nav__link> <span class=md-ellipsis> 11. Automated Machine Learning (AutoML): </span> </a> </li> <li class=md-nav__item> <a href=#12-gradient-based-optimization class=md-nav__link> <span class=md-ellipsis> 12. Gradient-Based Optimization: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-is-feature-engineering class=md-nav__link> <span class=md-ellipsis> What is Feature Engineering? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-regularization class=md-nav__link> <span class=md-ellipsis> What is Regularization? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-different-types-of-regularization-techniques-in-ml class=md-nav__link> <span class=md-ellipsis> What are different types of regularization techniques in ML? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-bias-variance-tradeoff class=md-nav__link> <span class=md-ellipsis> What is Bias-Variance Tradeoff? </span> </a> </li> <li class=md-nav__item> <a href=#how-to-handle-bias-variance-problem class=md-nav__link> <span class=md-ellipsis> How to handle Bias-Variance problem? </span> </a> </li> <li class=md-nav__item> <a href=#how-to-evaluate-a-models-goodnessfitnessrobustness class=md-nav__link> <span class=md-ellipsis> How to evaluate a model's goodness/fitness/robustness? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-ensemble-learning class=md-nav__link> <span class=md-ellipsis> What is Ensemble Learning? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-different-ensemble-learning-techniques class=md-nav__link> <span class=md-ellipsis> What are different ensemble learning techniques? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-dimensionality-reduction class=md-nav__link> <span class=md-ellipsis> What is Dimensionality Reduction? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-popular-dimensionality-reduction-techniques class=md-nav__link> <span class=md-ellipsis> What are popular Dimensionality Reduction Techniques? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-kernel-trick-can-you-explain-with-simple-example class=md-nav__link> <span class=md-ellipsis> What is kernel trick, can you explain with simple example? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-clustering class=md-nav__link> <span class=md-ellipsis> What is Clustering? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-popular-clustering-algorithms class=md-nav__link> <span class=md-ellipsis> What are popular clustering algorithms? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-deep-learning-and-neural-networks class=md-nav__link> <span class=md-ellipsis> What is Deep Learning and Neural Networks? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-self-supervised-learning-ssl class=md-nav__link> <span class=md-ellipsis> What is Self-Supervised Learning (SSL)? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-meta-learning-learning-to-learn class=md-nav__link> <span class=md-ellipsis> what is Meta-Learning (Learning to Learn)? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-reinforcement-learning-rl class=md-nav__link> <span class=md-ellipsis> What is Reinforcement Learning (RL)? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-generative-model class=md-nav__link> <span class=md-ellipsis> What is Generative Model? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-different-generative-models class=md-nav__link> <span class=md-ellipsis> What are different Generative Models? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-federated-learning class=md-nav__link> <span class=md-ellipsis> What is Federated Learning? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-causal-inference class=md-nav__link> <span class=md-ellipsis> What is Causal Inference? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-neural-architecture-search-nas class=md-nav__link> <span class=md-ellipsis> What is Neural Architecture Search (NAS)? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-transformers-and-attention-mechanisms class=md-nav__link> <span class=md-ellipsis> What are Transformers and Attention Mechanisms? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-explainable-ai-xai class=md-nav__link> <span class=md-ellipsis> What is Explainable AI (XAI)? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-popular-xai-methods class=md-nav__link> <span class=md-ellipsis> What are popular XAI methods? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-uncertainty-quantification class=md-nav__link> <span class=md-ellipsis> What is Uncertainty Quantification? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-continual-learning-lifelong-learning class=md-nav__link> <span class=md-ellipsis> What is Continual Learning (Lifelong Learning)? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-adversarial-machine-learning class=md-nav__link> <span class=md-ellipsis> What is Adversarial Machine Learning? </span> </a> </li> <li class=md-nav__item> <a href=#hastags class=md-nav__link> <span class=md-ellipsis> Hastags </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <article class="md-content__inner md-typeset"> <p><img alt="Exploring Docker and VS Code Integration" src=../assets/images/dspost/dsp6152-Machine-Learning-Key-Concepts.jpg></p> <h1 id=machine-learning-key-concepts>Machine Learning Key Concepts<a class=headerlink href=#machine-learning-key-concepts title="Permanent link">&para;</a></h1> <p>In this article Essential Machine Learning Techniques/Concepts are Explained, some of them are are Cross-Validation, Hyperparameter Optimization, Machine learning types and much More.</p> <h2 id=is-this-article-for-me>Is this article for me?<a class=headerlink href=#is-this-article-for-me title="Permanent link">&para;</a></h2> <p>If you are looking for the answer to any of the following questions, then the answer is 'Yes.' 1. What is Cross-validation? 1. What is Advantages of Cross-Validation? 1. In cross-validation what is the use of the averaging the performance of 5 models? 1. Why Averaging the Performance of Cross-Validation Models Matters: 1. How Does Cross-Validation Help in Final Model Creation? 1. Why Not Just Train on the Full Data from the Beginning? 1. When should I use Cross-Validation? 1. What is Feature Engineering? 1. What is Regularization? 1. What are different types of regularization techniques in ML? 1. What is Bias-Variance Tradeoff? 1. How to handle Bias-Variance problem? 1. How to evaluate a model's goodness/fitness/robustness? 1. What is Ensemble Learning? 1. What are different ensemble learning techniques? 1. What is Dimensionality Reduction? 1. What is kernel trick, can you explain with simple example? 1. What are popular Dimensionality Reduction Techniques? 1. What is Clustering? 1. What are popular clustering algorithms? 1. What is Deep Learning and Neural Networks? 1. What is Self-Supervised Learning (SSL)? 1. what is Meta-Learning (Learning to Learn)? 1. What is Reinforcement Learning (RL)? 1. What is Generative Model? 1. What are different Generative Models? 1. What is Federated Learning? 1. What is Causal Inference? 1. What is Neural Architecture Search (NAS)? 1. What are Transformers and Attention Mechanisms? 1. What is Explainable AI (XAI)? 1. What are popular XAI methods? 1. What is Uncertainty Quantification? 1. What is Continual Learning (Lifelong Learning)? 1. What is Adversarial Machine Learning?</p> <h2 id=what-is-cross-validation>What is Cross-validation?<a class=headerlink href=#what-is-cross-validation title="Permanent link">&para;</a></h2> <p>In machine learning, <strong>cross-validation</strong> is a technique used to evaluate the performance of a model by partitioning the dataset into subsets, training the model on some of these subsets, and then testing it on the remaining subsets. The goal is to assess how well the model generalizes to unseen data, thus preventing issues like overfitting or underfitting.</p> <p><strong>Key Concepts in Cross-Validation</strong> </p> <ol> <li><strong>Training and Testing Data Split</strong>:</li> <li> <p>In simple terms, the dataset is split into two parts: training data (used to train the model) and testing data (used to evaluate it). However, this can lead to unreliable results if the test set is not representative of the entire data.</p> </li> <li> <p><strong>K-Fold Cross-Validation</strong>:</p> </li> <li>The dataset is divided into <strong>K</strong> equal-sized "folds" or subsets. The model is trained on <strong>K-1</strong> folds and tested on the remaining one. This process is repeated <strong>K</strong> times, with each fold serving as the test set once. The performance is averaged across all folds to get a reliable estimate.</li> <li> <p>Example: For <strong>5-Fold Cross-Validation</strong>, the dataset is split into 5 folds, and the model is trained and tested 5 times (each time with a different fold as the test set).</p> </li> <li> <p><strong>Stratified K-Fold Cross-Validation</strong>:</p> </li> <li> <p>A variation of K-fold cross-validation that ensures each fold has roughly the same proportion of each class (in classification problems). This is especially useful in imbalanced datasets where one class may dominate.</p> </li> <li> <p><strong>Leave-One-Out Cross-Validation (LOOCV)</strong>:</p> </li> <li> <p>A special case of K-fold where <strong>K</strong> equals the number of data points. The model is trained on all but one data point, and the process is repeated so that each data point serves as a test set once. This method can be computationally expensive but provides an almost unbiased estimate.</p> </li> <li> <p><strong>Time Series Cross-Validation</strong>:</p> </li> <li>For time series data, traditional cross-validation isn't applicable since the temporal ordering of data must be preserved. Instead, methods like <strong>walk-forward validation</strong> or <strong>rolling window</strong> are used, where the model is trained on past data and validated on future data.</li> </ol> <h2 id=what-is-advantages-of-cross-validation>What is Advantages of Cross-Validation?<a class=headerlink href=#what-is-advantages-of-cross-validation title="Permanent link">&para;</a></h2> <ul> <li><strong>Reduces overfitting</strong>: By training and testing on different data points multiple times, the model's performance generalizes better.</li> <li><strong>More reliable performance metrics</strong>: Averaging the results over several splits gives a better estimate of the model’s true performance.</li> <li><strong>Efficient use of data</strong>: All data points are used for both training and testing.</li> </ul> <h2 id=lets-assume-we-have-dataset-of-1000-sample-i-decide-to-to-5-fold-cross-validation-then-how-many-models-will-be-build>Let's assume we have dataset of 1000 sample, I decide to to 5-fold cross validation then how many models will be build?<a class=headerlink href=#lets-assume-we-have-dataset-of-1000-sample-i-decide-to-to-5-fold-cross-validation-then-how-many-models-will-be-build title="Permanent link">&para;</a></h2> <p>If you perform <strong>5-fold cross-validation</strong> on a dataset with 1,000 samples, you will build <strong>5 models</strong>.</p> <p>In <strong>5-fold cross-validation</strong>, the dataset is split into 5 equal parts (or folds). In each iteration: - 4 folds (80% of the data) are used for training the model. - 1 fold (20% of the data) is used for testing the model.</p> <p>This process is repeated 5 times, with a different fold being used as the test set each time, while the remaining 4 folds are used for training.</p> <p>So, in total, <strong>5 models</strong> will be built—one for each iteration of training and testing on different subsets of the data. </p> <p>Steps in 5-Fold Cross-Validation: <br> 1. Split data into 5 folds. 2. In the 1<sup>st</sup> iteration, use folds 1-4 for training and fold 5 for testing (build the 1<sup>st</sup> model). 3. In the 2<sup>nd</sup> iteration, use folds 1-3 and 5 for training and fold 4 for testing (build the 2<sup>nd</sup> model). 4. Continue this process until all folds have been used as the test set once.</p> <p>The performance of these 5 models is then averaged to give the final performance estimate.</p> <h2 id=these-5-models-will-have-different-performance-and-based-on-the-selected-metrics-we-finally-select-one-model-is-this-correct>These 5 models will have different performance and based on the selected metrics we finally select one model, is this correct?<a class=headerlink href=#these-5-models-will-have-different-performance-and-based-on-the-selected-metrics-we-finally-select-one-model-is-this-correct title="Permanent link">&para;</a></h2> <p>Not exactly! In <strong>5-fold cross-validation</strong>, while you train 5 models (one for each fold), you <strong>don't select one specific model</strong> from these 5. Instead, the goal is to evaluate how well a single model would generalize to unseen data by averaging the performance of the 5 models trained on different subsets of the data.</p> <p>Here's the key idea:</p> <ol> <li> <p><strong>Performance Estimation</strong>: The different models trained in each fold will indeed have slightly different performances because each model sees a different subset of the data. However, the <strong>final performance metric</strong> (e.g., accuracy, precision, recall, etc.) is the <strong>average</strong> performance of all 5 models across the 5 folds. This average gives you a more reliable estimate of how well your model is expected to perform on unseen data.</p> </li> <li> <p><strong>Final Model</strong>: After cross-validation is completed and the model's performance has been evaluated, <strong>a new model is trained on the entire dataset</strong> using the chosen hyperparameters. This final model is the one you'll actually deploy or use in practice. It's trained on all 1,000 samples, not just one of the folds.</p> </li> </ol> <p>Process Recap: <br> - <strong>5-Fold Cross-Validation</strong>: - Train 5 models on different subsets of the data. - Get 5 different performance metrics. - Average these performance metrics to estimate the generalization ability of the model.</p> <ul> <li><strong>Final Model Selection</strong>:</li> <li>After the cross-validation, you don’t choose one of the 5 models.</li> <li>You train a new model on the <strong>entire dataset</strong> (all 1,000 samples), using the insights gained from cross-validation (e.g., hyperparameter tuning or model selection).</li> </ul> <p>The 5 models during cross-validation are only temporary models used for evaluating performance. The final model used for predictions will be trained on the complete dataset.</p> <h2 id=what-is-the-use-of-the-averaging-the-performance-of-5-models-how-does-it-help-in-final-model-creation-because-hyperparameter-will-remain-same-the-only-change-we-have-instead-of-4-fold-we-have-all-5-folds-for-model-training>What is the use of the averaging the performance of 5 models? How does it help in final model creation, because hyperparameter will remain same. The only change we have instead of 4 fold, we have all 5 folds for model training.<a class=headerlink href=#what-is-the-use-of-the-averaging-the-performance-of-5-models-how-does-it-help-in-final-model-creation-because-hyperparameter-will-remain-same-the-only-change-we-have-instead-of-4-fold-we-have-all-5-folds-for-model-training title="Permanent link">&para;</a></h2> <p>Averaging the performance across the 5 models in cross-validation is a crucial step because it helps provide a <strong>reliable estimate</strong> of how well your final model will perform on unseen data. Let's break this down and explain why this averaging is useful and how it helps in the final model creation:</p> <h3 id=why-averaging-the-performance-of-cross-validation-models-matters><strong>Why Averaging the Performance of Cross-Validation Models Matters</strong>:<a class=headerlink href=#why-averaging-the-performance-of-cross-validation-models-matters title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Reduces Variability</strong>: Data is often noisy and can vary between different subsets. By training 5 models on different data splits, cross-validation provides multiple estimates of model performance. Averaging the performance across these folds reduces the influence of a particular "lucky" or "unlucky" data split (e.g., if one fold happens to be easier or harder to predict).</p> </li> <li> <p><strong>More Reliable Generalization Estimate</strong>: The goal of cross-validation is to assess how well your model, with a specific set of hyperparameters, generalizes to new, unseen data. The average performance over multiple splits gives a more <strong>robust estimate</strong> of this generalization ability, as opposed to relying on a single train-test split, which might give a misleading result.</p> </li> <li> <p><strong>Prevents Overfitting to a Particular Split</strong>: If you only trained and evaluated on one data split, there’s a risk that the model may have overfitted to that particular split, leading to an over-optimistic or overly pessimistic estimate of performance. By averaging across folds, you get a broader view of the model's behavior.</p> </li> </ul> <h3 id=how-does-cross-validation-help-in-final-model-creation><strong>How Does Cross-Validation Help in Final Model Creation?</strong><a class=headerlink href=#how-does-cross-validation-help-in-final-model-creation title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Model Performance Benchmark</strong>: After cross-validation, the average performance (e.g., accuracy, precision, etc.) tells you how well the model is expected to perform on new data. This helps you make informed decisions about whether the model is good enough to use in production or whether you need further improvements, such as hyperparameter tuning or feature engineering.</p> </li> <li> <p><strong>Confidence in the Chosen Hyperparameters</strong>: While hyperparameters remain the same throughout the cross-validation process, averaging helps confirm that those hyperparameters perform well across different data splits. If the average performance is strong, it suggests that the chosen hyperparameters are effective and can be used confidently in the final model.</p> </li> <li> <p><strong>Final Model Trained on Full Data</strong>: Once cross-validation is complete and the performance is evaluated, a <strong>final model</strong> is trained on the <strong>entire dataset</strong> (all 5 folds combined). This final model will be used for deployment or prediction in the real world. Although the cross-validation process doesn't directly select one of the 5 temporary models, it ensures that the final model is trained using a reliable set of hyperparameters.</p> </li> </ul> <p><strong>Example:</strong><br> Assume you’re doing 5-fold cross-validation on a dataset with 1,000 samples. If the performance (accuracy) of each fold is: - Fold 1: 90% - Fold 2: 88% - Fold 3: 92% - Fold 4: 89% - Fold 5: 91%</p> <p>The average accuracy is <strong>90%</strong>. This 90% accuracy is your <strong>best estimate</strong> of how well the final model will perform on new data.</p> <p>Now, when you train the final model on all 1,000 samples using the same hyperparameters (since cross-validation confirmed they work well), you expect its performance to be around 90%, because that’s what cross-validation told you to expect.</p> <h3 id=why-not-just-train-on-the-full-data-from-the-beginning><strong>Why Not Just Train on the Full Data from the Beginning?</strong><a class=headerlink href=#why-not-just-train-on-the-full-data-from-the-beginning title="Permanent link">&para;</a></h3> <p>If you train on the entire dataset without cross-validation, you won't have any reliable way of estimating the model’s performance on unseen data. Cross-validation acts as a <strong>proxy</strong> for unseen data by creating several test sets, which helps to: - Ensure the model’s performance generalizes well. - Avoid overfitting by testing on multiple different subsets of data. - Give you confidence that the final model will perform as expected on real-world data.</p> <h2 id=when-should-i-use-cross-validation>When should I use Cross-Validation?<a class=headerlink href=#when-should-i-use-cross-validation title="Permanent link">&para;</a></h2> <p><strong>Cross-validation</strong> is particularly useful in several situations within machine learning, especially when you want to evaluate the generalization ability of a model on unseen data. It helps ensure that your model performs well across different subsets of data and isn’t overly dependent on a specific train-test split. Here are the key situations where cross-validation makes sense:</p> <h3 id=1-when-dataset-is-small>1. <strong>When dataset is Small</strong>:<a class=headerlink href=#1-when-dataset-is-small title="Permanent link">&para;</a></h3> <p>When you have a small dataset, the amount of data available for training and testing is limited. Using a single train-test split might result in a model that is not properly evaluated because the test set may not represent the underlying distribution of the data. By using <strong>K-fold cross-validation</strong>, the model is trained and tested multiple times on different subsets of the data, which ensures that every data point is used for both training and testing, leading to a more reliable estimate of performance.</p> <p>A medical dataset with only 100 samples. You can't afford to set aside 30-40% of the data for testing in one split, so cross-validation allows you to test on different parts while still training on the majority of the data.</p> <h3 id=2-when-you-want-to-tune-hyperparameter>2. <strong>When you want to tune Hyperparameter</strong>:<a class=headerlink href=#2-when-you-want-to-tune-hyperparameter title="Permanent link">&para;</a></h3> <p>When you're fine-tuning hyperparameters using techniques like grid search or random search, using a single train-test split can give a biased result that might lead to overfitting or underfitting. A model may appear to perform well on one test set due to the specific data split. Cross-validation ensures that the model's hyperparameters are evaluated across multiple data splits, which gives a better sense of how the chosen hyperparameters will perform across different datasets. This leads to a more generalizable model configuration.</p> <p>Tuning the learning rate and max depth of a decision tree or a neural network. Instead of relying on a single split, cross-validation ensures the best hyperparameter values are robust across different folds.</p> <h3 id=3-it-helps-avoiding-model-overfitting>3. <strong>It helps Avoiding model Overfitting</strong>:<a class=headerlink href=#3-it-helps-avoiding-model-overfitting title="Permanent link">&para;</a></h3> <p>Overfitting occurs when a model performs very well on the training data but fails to generalize to unseen data. Evaluating the model on a single test set might not catch overfitting, especially if the test data is "easy" for the model. By evaluating the model on multiple different subsets of the data, cross-validation helps detect overfitting by showing how the model performs on different test sets. If the model overfits, performance will vary significantly across the folds, indicating the need for regularization or simpler model design.</p> <p>Complex models like deep neural networks, decision trees, or polynomial regression tend to overfit, and cross-validation can reveal whether they perform consistently across different folds or just on specific splits.</p> <h3 id=4-when-the-dataset-is-imbalanced>4. <strong>When the Dataset is Imbalanced</strong>:<a class=headerlink href=#4-when-the-dataset-is-imbalanced title="Permanent link">&para;</a></h3> <p>In imbalanced datasets (where one class is more frequent than the other), a single train-test split might not capture the true distribution of the classes. For example, the test set might have too few examples of the minority class, leading to unreliable performance metrics. Techniques like <strong>stratified K-fold cross-validation</strong> ensure that each fold has a similar distribution of classes, leading to a more reliable estimate of performance, especially for classification tasks where class imbalance is a concern.</p> <p>In fraud detection, churn prediction, or rare disease diagnosis, cross-validation helps ensure that models are evaluated on balanced subsets of data that reflect the real-world distribution.</p> <h3 id=5-when-you-need-to-select-a-model>5. <strong>When you need to Select a Model</strong>:<a class=headerlink href=#5-when-you-need-to-select-a-model title="Permanent link">&para;</a></h3> <p>When you're comparing different models (e.g., decision tree, random forest, support vector machine), evaluating them on a single train-test split might give an inaccurate sense of which model is better. The performance could vary significantly depending on the data split. Cross-validation gives a more reliable comparison of models since each model is evaluated on multiple folds. By averaging the performance across these folds, you can choose the best model that generalizes well.</p> <p>You have multiple models (e.g., linear regression, decision tree, and random forest), and you want to see which model works best. Cross-validation helps avoid choosing a model that performs well only due to a "lucky" split.</p> <h3 id=6-when-we-have-time-constraints-for-creating-a-train-test-split>6. <strong>When we have Time Constraints for Creating a Train-Test Split</strong>:<a class=headerlink href=#6-when-we-have-time-constraints-for-creating-a-train-test-split title="Permanent link">&para;</a></h3> <p>In some cases, creating a well-balanced and representative train-test split is challenging or time-consuming, especially when the dataset is heterogeneous or includes rare events. Cross-validation simplifies the process by automating multiple splits and providing a more thorough evaluation of model performance without needing to manually curate the train-test split.</p> <p>In environments like competitions (e.g., Kaggle) or real-world applications where creating multiple random train-test splits manually isn't practical, cross-validation is an efficient way to test model performance.</p> <h3 id=7-when-you-have-no-separate-test-data>7. <strong>When You Have No Separate Test Data</strong>:<a class=headerlink href=#7-when-you-have-no-separate-test-data title="Permanent link">&para;</a></h3> <p>Sometimes, it's not feasible to set aside a separate test set, especially if you have limited data. In this case, cross-validation serves as a substitute for a separate test set, allowing you to still evaluate model performance in a more reliable way than using the training data alone.</p> <p>A research project where data collection is expensive or limited. Cross-validation provides a way to evaluate model performance without needing a separate test set.</p> <h3 id=8-time-series-data-with-rolling-windows>8. <strong>Time Series Data (with Rolling Windows)</strong>:<a class=headerlink href=#8-time-series-data-with-rolling-windows title="Permanent link">&para;</a></h3> <p>Time series data often requires careful handling because the temporal order of data points matters, and you can't randomly shuffle the data. Techniques like <strong>rolling window cross-validation</strong> (or walk-forward validation) allow you to train the model on past data and validate it on future data while preserving the temporal structure, which helps in reliable performance evaluation for time-dependent problems.</p> <p>In stock market prediction, weather forecasting, or sales forecasting, where the model must predict future values based on past trends.</p> <h2 id=when-cross-validation-might-not-be-ideal>When Cross-Validation Might <strong>Not</strong> Be Ideal?<a class=headerlink href=#when-cross-validation-might-not-be-ideal title="Permanent link">&para;</a></h2> <ul> <li><strong>Large Datasets</strong>: If you have a large enough dataset, a simple train-test split may suffice because there’s enough data to give a reliable estimate of model performance. In such cases, cross-validation might be unnecessarily computationally expensive.</li> <li><strong>Extremely Computationally Expensive Models</strong>: For very large models or datasets, cross-validation might be too time-consuming, and a simpler validation strategy (like a single train-test split) might be more practical.</li> </ul> <h2 id=what-is-grid-search>What is Grid Search?<a class=headerlink href=#what-is-grid-search title="Permanent link">&para;</a></h2> <p><strong>Grid search</strong> is a techniques for <strong>hyperparameter tuning</strong> in machine learning. Hyperparameter tuning is the process of finding the optimal set of hyperparameters that maximize a model's performance on a given task.</p> <p>Grid search is a <strong>systematic, exhaustive</strong> method of hyperparameter tuning, where you define a grid of possible values for each hyperparameter, and the algorithm trains and evaluates the model using <strong>every possible combination</strong> of these values.</p> <h3 id=how-grid-search-works>How Grid Search Works?<a class=headerlink href=#how-grid-search-works title="Permanent link">&para;</a></h3> <ul> <li>You define a set of hyperparameters and their possible values.</li> <li>The grid search algorithm will create a "grid" of all possible combinations of these hyperparameters.</li> <li>For each combination, the model is trained and evaluated (usually using cross-validation).</li> <li>The combination of hyperparameters that gives the best performance (based on a chosen metric, e.g., accuracy, F1-score) is selected as the optimal set.</li> </ul> <p><strong>Example of Grid Search:</strong><br> Let’s say you're tuning a Support Vector Machine (SVM) model, and you want to tune two hyperparameters: 1. <strong>C</strong> (regularization strength) = [0.1, 1, 10] 2. <strong>Kernel</strong> (type of kernel) = ['linear', 'rbf']</p> <p>The grid search will evaluate all possible combinations of these hyperparameters: - (C=0.1, Kernel=linear) - (C=0.1, Kernel=rbf) - (C=1, Kernel=linear) - (C=1, Kernel=rbf) - (C=10, Kernel=linear) - (C=10, Kernel=rbf)</p> <p>In this case, you would evaluate 6 combinations of hyperparameters, and the grid search will select the combination that performs best.</p> <p>It is Simple and easy to understand and it guarantees that all combinations will be tested, so it won't miss the best solution.</p> <p>But it is Computationally expensive and inefficient when you have a large number of hyperparameters or a wide range of values. The number of combinations grows exponentially. It might evaluate combinations that are not important or promising.</p> <h2 id=what-is-random-search>What is random search?<a class=headerlink href=#what-is-random-search title="Permanent link">&para;</a></h2> <p><strong>Random search</strong> is anoth techniques for <strong>hyperparameter tuning</strong> in machine learning. Hyperparameter tuning is the process of finding the optimal set of hyperparameters that maximize a model's performance on a given task.</p> <p>Random search is a <strong>more efficient, stochastic</strong> method of hyperparameter tuning, where instead of trying every combination of hyperparameters, it randomly selects combinations from the predefined range of hyperparameter values and evaluates the model.</p> <h3 id=how-random-search-works>How Random Search Works?<a class=headerlink href=#how-random-search-works title="Permanent link">&para;</a></h3> <ul> <li>You define a range of values for each hyperparameter.</li> <li>Instead of trying every combination, random search samples random combinations of these hyperparameters.</li> <li>The algorithm evaluates the model for a fixed number of random combinations.</li> <li>The combination that gives the best performance is selected as the optimal set of hyperparameters.</li> </ul> <p><strong>Example of Random Search:</strong><br> Using the same example of an SVM: 1. <strong>C</strong> = [0.1, 1, 10] 2. <strong>Kernel</strong> = ['linear', 'rbf']</p> <p>Instead of evaluating all 6 possible combinations, random search might try: - (C=10, Kernel=linear) - (C=1, Kernel=rbf) - (C=0.1, Kernel=linear)</p> <p>The number of combinations to evaluate is typically predefined, and random search will try a limited set of random hyperparameter combinations rather than all of them.</p> <p>It avoids evaluating every possible combination and focuses on exploring the space in a less exhaustive but often effective way. In high-dimensional hyperparameter spaces, it is shown to find good solutions faster than grid search. It is useful when you don't know which hyperparameters are most important and want a quick estimate of good hyperparameters.</p> <p>But, since it doesn’t try every combination, it may miss the optimal combination. If the search space is very large, you might need many random samples to find good hyperparameters.</p> <h2 id=when-to-use-grid-search-vs-random-search>When to Use Grid Search vs. Random Search?<a class=headerlink href=#when-to-use-grid-search-vs-random-search title="Permanent link">&para;</a></h2> <p><strong>Grid Search</strong> is preferred when: - You have a small number of hyperparameters or a limited range of possible values. - You want to <strong>ensure</strong> that you explore all possible combinations and get the best one. - Computational resources are not a major concern.</p> <p><strong>Random Search</strong> is preferred when: - You have a large number of hyperparameters or a wide range of values, making grid search computationally expensive. - You want to explore the hyperparameter space more quickly and efficiently. - You're okay with finding a good (but not necessarily the best) set of hyperparameters.</p> <h2 id=what-are-popular-hyperparameter-optimization-techniques>What are popular hyperparameter optimization techniques?<a class=headerlink href=#what-are-popular-hyperparameter-optimization-techniques title="Permanent link">&para;</a></h2> <p>Optimizing hyperparameters is crucial for improving the performance of machine learning models. The choice of hyperparameter optimization technique depends on factors such as the complexity of the model, the size of the search space, available computational resources, and the nature of the hyperparameters (continuous vs. discrete). Combining these methods can often yield the best results.</p> <p>Here are several popular techniques used for hyperparameter optimization:</p> <h3 id=1-grid-search>1. <strong>Grid Search</strong>:<a class=headerlink href=#1-grid-search title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: An exhaustive search method that evaluates all possible combinations of a predefined set of hyperparameters.</li> <li><strong>Use Case</strong>: Suitable for small search spaces; provides a comprehensive view of hyperparameter interactions.</li> </ul> <h3 id=2-random-search>2. <strong>Random Search</strong>:<a class=headerlink href=#2-random-search title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Randomly samples combinations of hyperparameters from defined ranges, evaluating only a subset of possible combinations.</li> <li><strong>Use Case</strong>: More efficient than grid search in high-dimensional spaces, as it can find good hyperparameter settings more quickly.</li> </ul> <h3 id=3-bayesian-optimization>3. <strong>Bayesian Optimization</strong>:<a class=headerlink href=#3-bayesian-optimization title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Uses probabilistic models to find the minimum of a function (e.g., validation error) by balancing exploration and exploitation.</li> <li><strong>Use Case</strong>: Effective for expensive function evaluations, as it learns from past evaluations to make informed guesses about the next hyperparameters to try.</li> </ul> <h3 id=4-hyperband>4. <strong>Hyperband</strong>:<a class=headerlink href=#4-hyperband title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: An adaptive method that allocates resources to configurations based on their performance, discarding poor-performing ones early.</li> <li><strong>Use Case</strong>: Efficient for resource-constrained environments, allowing for rapid testing of a large number of configurations.</li> </ul> <h3 id=5-tree-structured-parzen-estimator-tpe>5. <strong>Tree-structured Parzen Estimator (TPE)</strong>:<a class=headerlink href=#5-tree-structured-parzen-estimator-tpe title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: A Bayesian optimization method that models the distribution of good and bad hyperparameter configurations and uses these models to sample new configurations.</li> <li><strong>Use Case</strong>: Suitable for high-dimensional search spaces; implemented in libraries like Optuna and Hyperopt.</li> </ul> <h3 id=6-sequential-model-optimization-smo>6. <strong>Sequential Model Optimization (SMO)</strong>:<a class=headerlink href=#6-sequential-model-optimization-smo title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Sequentially builds and refines a surrogate model for the objective function, using it to explore the hyperparameter space.</li> <li><strong>Use Case</strong>: Effective for continuous optimization problems with fewer iterations, leveraging past evaluations.</li> </ul> <h3 id=7-evolutionary-algorithms>7. <strong>Evolutionary Algorithms</strong>:<a class=headerlink href=#7-evolutionary-algorithms title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Uses concepts from evolutionary biology (selection, mutation, crossover) to evolve a population of hyperparameter settings over generations.</li> <li><strong>Use Case</strong>: Suitable for complex optimization problems where the search space is large and poorly understood.</li> </ul> <h3 id=8-simulated-annealing>8. <strong>Simulated Annealing</strong>:<a class=headerlink href=#8-simulated-annealing title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: A probabilistic technique that explores the hyperparameter space by accepting worse configurations initially to escape local minima, gradually reducing this allowance.</li> <li><strong>Use Case</strong>: Effective for optimization problems with many local minima, balancing exploration and exploitation.</li> </ul> <h3 id=9-genetic-algorithms>9. <strong>Genetic Algorithms</strong>:<a class=headerlink href=#9-genetic-algorithms title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Mimics natural selection to optimize hyperparameters by encoding them into chromosomes and evolving the population over generations.</li> <li><strong>Use Case</strong>: Useful for complex optimization problems where traditional gradient-based methods may struggle.</li> </ul> <h3 id=10-cross-validation>10. <strong>Cross-Validation</strong>:<a class=headerlink href=#10-cross-validation title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Though primarily a model evaluation technique, using cross-validation to assess hyperparameter configurations ensures robust performance metrics.</li> <li><strong>Use Case</strong>: Provides a reliable estimate of model performance by evaluating on multiple subsets of the training data.</li> </ul> <h3 id=11-automated-machine-learning-automl>11. <strong>Automated Machine Learning (AutoML)</strong>:<a class=headerlink href=#11-automated-machine-learning-automl title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Frameworks that automate the selection of models and hyperparameter tuning, integrating various optimization techniques.</li> <li><strong>Use Case</strong>: Simplifies the process of model development for non-experts and speeds up the modeling process.</li> </ul> <h3 id=12-gradient-based-optimization>12. <strong>Gradient-Based Optimization</strong>:<a class=headerlink href=#12-gradient-based-optimization title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Uses gradients to optimize hyperparameters by treating them as part of the loss function.</li> <li><strong>Use Case</strong>: Effective when hyperparameters are continuous, allowing for efficient optimization using techniques like Adam or SGD.</li> </ul> <h2 id=what-is-feature-engineering>What is <strong>Feature Engineering</strong>?<a class=headerlink href=#what-is-feature-engineering title="Permanent link">&para;</a></h2> <p>Feature engineering involves selecting, modifying, or creating new features (input variables) to improve the performance of a machine learning model. Good features often have a bigger impact on model performance than the choice of algorithm. Well-engineered features help the model better capture underlying patterns in the data.</p> <p><strong>Examples of Feature Engineering</strong><br> - <strong>Scaling</strong>: Normalizing or standardizing numerical features so that they are on a similar scale. - <strong>Binning</strong>: Grouping continuous features into discrete bins (e.g., age into age groups). - <strong>Encoding</strong>: Converting categorical features into numerical representations (e.g., one-hot encoding). - <strong>Combining</strong>: Combining two or more existing features and creating a third feature. (e.g. creating BMI from Mass and Height features from the dataset)</p> <h2 id=what-is-regularization>What is <strong>Regularization</strong>?<a class=headerlink href=#what-is-regularization title="Permanent link">&para;</a></h2> <p>Regularization techniques are used to prevent overfitting by discouraging the model from fitting too closely to the training data. Overfitting happens when a model captures noise or irrelevant patterns in the training data, leading to poor performance on unseen data.</p> <h2 id=what-are-different-types-of-regularization-techniques-in-ml>What are different types of regularization techniques in ML?<a class=headerlink href=#what-are-different-types-of-regularization-techniques-in-ml title="Permanent link">&para;</a></h2> <p>Regularization techniques are crucial in machine learning to prevent overfitting by adding a penalty to the loss function. Here are some common types of regularization techniques:</p> <ol> <li><strong>L1 Regularization (Lasso Regularization)</strong>:</li> <li><strong>Definition</strong>: Adds the absolute value of the coefficients (weights) as a penalty term to the loss function.</li> <li><strong>Loss Function</strong>: $$ L = L_0 + \lambda \sum_{i=1}^{n} |w_i| $$ where <span class=arithmatex>\(<span class=arithmatex>\(L_0\)</span>\)</span> is the original loss, <span class=arithmatex>\(<span class=arithmatex>\(w_i\)</span>\)</span> are the coefficients, and <span class=arithmatex>\(<span class=arithmatex>\(\lambda\)</span>\)</span> is the regularization parameter.</li> <li> <p><strong>Effect</strong>: Encourages sparsity in the model (i.e., drives some coefficients to exactly zero), which can result in simpler models and feature selection.</p> </li> <li> <p><strong>L2 Regularization (Ridge Regularization)</strong>:</p> </li> <li><strong>Definition</strong>: Adds the squared value of the coefficients as a penalty term to the loss function.</li> <li><strong>Loss Function</strong>: $$ L = L_0 + \lambda \sum_{i=1}^{n} w_i^2 $$</li> <li> <p><strong>Effect</strong>: Penalizes large coefficients, resulting in smaller, more evenly distributed weights. Unlike L1, L2 regularization does not necessarily drive coefficients to zero but helps prevent overfitting.</p> </li> <li> <p><strong>Elastic Net Regularization</strong>:</p> </li> <li><strong>Definition</strong>: Combines both L1 and L2 regularization penalties.</li> <li><strong>Loss Function</strong>: $$ L = L_0 + \lambda_1 \sum_{i=1}^{n} |w_i| + \lambda_2 \sum_{i=1}^{n} w_i^2 $$</li> <li> <p><strong>Effect</strong>: This technique is particularly useful when there are many correlated features, as it can select one of the correlated features while shrinking the others.</p> </li> <li> <p><strong>Dropout</strong>:</p> </li> <li><strong>Definition</strong>: A regularization technique used primarily in neural networks where randomly selected neurons are ignored (dropped out) during training.</li> <li> <p><strong>Effect</strong>: Prevents neurons from co-adapting too much, forcing the network to learn more robust features that are not reliant on any single neuron.</p> </li> <li> <p><strong>Early Stopping</strong>:</p> </li> <li><strong>Definition</strong>: A technique where training is halted as soon as the model’s performance on a validation dataset begins to degrade.</li> <li> <p><strong>Effect</strong>: This prevents the model from overfitting to the training data, as it stops before the model has had the chance to learn the noise in the training set.</p> </li> <li> <p><strong>Data Augmentation</strong>:</p> </li> <li><strong>Definition</strong>: A technique used to increase the diversity of training data by applying random transformations (such as rotation, flipping, scaling, etc.) to existing data.</li> <li> <p><strong>Effect</strong>: Helps the model generalize better by providing it with a more comprehensive view of the data distribution, effectively serving as a form of regularization.</p> </li> <li> <p><strong>Weight Decay</strong>:</p> </li> <li><strong>Definition</strong>: A regularization technique that modifies the optimization algorithm to reduce the weights' magnitudes over time. It is essentially equivalent to L2 regularization.</li> <li> <p><strong>Effect</strong>: Encourages smaller weights, which helps mitigate overfitting.</p> </li> <li> <p><strong>Batch Normalization</strong>:</p> </li> <li><strong>Definition</strong>: A technique that normalizes the inputs of each layer in the neural network to have zero mean and unit variance. It can be thought of as a regularization technique as well.</li> <li> <p><strong>Effect</strong>: It can improve the training speed and stability of the model, reducing the likelihood of overfitting.</p> </li> <li> <p><strong>Feature Selection</strong>:</p> </li> <li><strong>Definition</strong>: While not a traditional form of regularization, selecting a subset of relevant features can help reduce the model complexity and mitigate overfitting.</li> <li> <p><strong>Techniques</strong>: Methods like recursive feature elimination, tree-based feature importance, and regularization-based selection (like Lasso) can be used.</p> </li> <li> <p><strong>Adding Noise</strong>:</p> </li> <li><strong>Definition</strong>: Introducing noise to the input data or to the model parameters during training can act as a regularization method.</li> <li><strong>Effect</strong>: Helps the model generalize better by making it robust to variations in input.</li> </ol> <h2 id=what-is-bias-variance-tradeoff>What is Bias-Variance Tradeoff?<a class=headerlink href=#what-is-bias-variance-tradeoff title="Permanent link">&para;</a></h2> <p>This refers to the tradeoff between how well a model fits the training data (<strong>bias</strong>) and how much the model’s predictions vary for different training datasets (<strong>variance</strong>). A model with high bias oversimplifies the data (underfitting), while a model with high variance overfits the training data (overfitting). The goal is to find the right balance between bias and variance to achieve good generalization.</p> <h2 id=how-to-handle-bias-variance-problem>How to handle Bias-Variance problem?<a class=headerlink href=#how-to-handle-bias-variance-problem title="Permanent link">&para;</a></h2> <ul> <li><strong>High bias (underfitting)</strong>: Use more complex models, add more features, or reduce regularization.</li> <li><strong>High variance (overfitting)</strong>: Use simpler models, increase regularization, or gather more training data.</li> </ul> <h2 id=how-to-evaluate-a-models-goodnessfitnessrobustness>How to evaluate a model's goodness/fitness/robustness?<a class=headerlink href=#how-to-evaluate-a-models-goodnessfitnessrobustness title="Permanent link">&para;</a></h2> <p>The performance of a machine learning model can be evaluated using appropriate metrics. Different tasks require different metrics to ensure whether a model is performing well. The choice of metric depends on the problem you are solving (e.g., classification vs. regression).</p> <p><strong>Common evaluation metrics are</strong> <br> - <strong>Accuracy</strong>: Proportion of correct predictions (good for balanced datasets). - <strong>Precision/Recall/F1-Score</strong>: Used in classification problems with imbalanced classes. - <strong>AUC-ROC Curve</strong>: Measures the tradeoff between true positive rate and false positive rate. - <strong>Mean Squared Error (MSE)/Mean Absolute Error (MAE)</strong>: For regression problems. - <strong>Confusion Matrix</strong>: Provides detailed insights into model predictions by showing the true positives, false positives, true negatives, and false negatives.</p> <h2 id=what-is-ensemble-learning>What is <strong>Ensemble Learning</strong>?<a class=headerlink href=#what-is-ensemble-learning title="Permanent link">&para;</a></h2> <p>Ensemble learning combines multiple models to improve the overall performance by reducing variance, bias, or improving predictions. Ensemble methods often outperform individual models by leveraging the strengths of multiple models while mitigating their weaknesses.</p> <h2 id=what-are-different-ensemble-learning-techniques>What are different ensemble learning techniques?<a class=headerlink href=#what-are-different-ensemble-learning-techniques title="Permanent link">&para;</a></h2> <p>Ensemble learning is a powerful technique in machine learning that combines multiple models to improve overall performance, reduce variance, and enhance robustness. These techniques can significantly improve model performance by leveraging the strengths of multiple algorithms while mitigating their individual weaknesses. The choice of ensemble method depends on the problem, the type of models being used, and the desired outcome.</p> <p>Here are some common ensemble learning techniques:</p> <ol> <li><strong>Bagging (Bootstrap Aggregating)</strong>: Involves training multiple models (usually of the same type) on different subsets of the training data, created by randomly sampling with replacement. Each model is trained independently, and their predictions are combined (usually by averaging for regression or majority voting for classification).</li> </ol> <p><strong>Example</strong>: <strong>Random Forest</strong> is a popular bagging algorithm that builds multiple decision trees and aggregates their results.</p> <ol> <li><strong>Boosting</strong>: A sequential ensemble technique where models are trained one after another, each trying to correct the errors of its predecessor. Models focus more on the errors made by previous models, often assigning higher weights to misclassified instances. The final prediction is typically a weighted sum of individual model predictions.</li> </ol> <p>Examples: - <strong>Example1: AdaBoost (Adaptive Boosting)</strong>: Adjusts the weights of misclassified instances and combines weak classifiers into a strong classifier. - <strong>Example2: Gradient Boosting</strong>: Builds models iteratively, with each new model trying to minimize the loss function of the combined previous models. - <strong>Example3: XGBoost</strong>: An optimized implementation of gradient boosting that is faster and more efficient, often used in competitions.</p> <ol> <li><strong>Stacking (Stacked Generalization)</strong>: Involves training multiple models (base learners) and combining their predictions using a meta-model (or blender). Base models are trained on the training data, and their predictions are used as features to train the meta-model. This can help leverage the strengths of different algorithms.</li> </ol> <p><strong>Example</strong>: Using a logistic regression model as a meta-learner to combine predictions from decision trees, support vector machines, and neural networks.</p> <ol> <li><strong>Voting</strong>: Combines predictions from multiple models by majority voting for classification or averaging for regression. There are two types of voting. In <strong>Hard Voting</strong>, Each model casts a vote for a class, and the class with the majority votes is selected. In <strong>Soft Voting</strong>, Averages the predicted probabilities from each model, and the class with the highest average probability is selected.</li> </ol> <p><strong>Example</strong>: Using different classifiers (e.g., logistic regression, decision trees, and SVMs) and voting on their predictions.</p> <ol> <li> <p><strong>Blending</strong>: A simpler version of stacking where the predictions from base models are combined using a hold-out validation set to train the meta-model. Base models are trained on the training data, and their predictions on a validation set are used to train a second-level model. How Blending is different from Stacking? Blending typically uses a simpler approach without the cross-validation mechanism, as used in stacking.</p> </li> <li> <p><strong>Voting Classifier / Regressor</strong>: A specific implementation of voting where different classifiers or regressors are combined into a single ensemble model. It can be implemented using scikit-learn's <code>VotingClassifier</code> or <code>VotingRegressor</code>, which allows you to easily combine different models.</p> </li> <li> <p><strong>Negative Correlation Ensemble</strong>: This technique aims to combine models that are negatively correlated to each other, which can lead to better overall performance. In this the Models are trained such that their errors do not correlate. The final prediction is based on the weighted average of their outputs.</p> </li> <li> <p><strong>Cascading</strong>: A technique where a sequence of models is used, and each subsequent model is trained on the errors made by the previous model. It is often used in object detection tasks where a series of models filters out non-object candidates progressively.</p> </li> </ol> <h2 id=what-is-dimensionality-reduction>What is Dimensionality Reduction?<a class=headerlink href=#what-is-dimensionality-reduction title="Permanent link">&para;</a></h2> <p>The process of reducing the number of input features or dimensions in a dataset while retaining as much information as possible. High-dimensional data can lead to overfitting, longer training times, and higher computational costs. Dimensionality reduction helps simplify the model, improve interpretability, and avoid the "curse of dimensionality."</p> <h2 id=what-are-popular-dimensionality-reduction-techniques>What are popular Dimensionality Reduction Techniques?<a class=headerlink href=#what-are-popular-dimensionality-reduction-techniques title="Permanent link">&para;</a></h2> <p>The choice of dimensionality reduction technique depends on the nature of the data (e.g., linear vs. non-linear), the task (e.g., visualization, classification, clustering), and the computational resources available. Some techniques like PCA and LDA are linear, while others like t-SNE, UMAP, and Autoencoders can capture complex non-linear structures in data.</p> <p>Here are some popular dimensionality reduction techniques. You can explore github repos/pypi repo of these for implementation:</p> <ol> <li><strong>Principal Component Analysis (PCA)</strong>:</li> <li><strong>Description</strong>: Transforms the original features into a set of orthogonal (uncorrelated) components ordered by the amount of variance they capture.</li> <li> <p><strong>Use Case</strong>: Widely used for feature extraction and visualization of high-dimensional data, especially in unsupervised learning.</p> </li> <li> <p><strong>Linear Discriminant Analysis (LDA)</strong>:</p> </li> <li><strong>Description</strong>: Projects data onto a lower-dimensional space that maximizes the separation between different classes and minimize the variance between the samples in a class.</li> <li> <p><strong>Use Case</strong>: Primarily used for supervised classification tasks where the goal is to maximize class separability.</p> </li> <li> <p><strong>t-Distributed Stochastic Neighbor Embedding (t-SNE)</strong>:</p> </li> <li><strong>Description</strong>: A non-linear dimensionality reduction technique that preserves local structure of the data and is useful for visualizing high-dimensional data in 2D or 3D.</li> <li> <p><strong>Use Case</strong>: Popular for data visualization, particularly for tasks involving image and text data.</p> </li> <li> <p><strong>Independent Component Analysis (ICA)</strong>:</p> </li> <li><strong>Description</strong>: Decomposes multivariate data into independent, non-Gaussian components.</li> <li> <p><strong>Use Case</strong>: Useful for signal separation tasks, such as separating mixed audio signals (e.g., "cocktail party problem").</p> </li> <li> <p><strong>Autoencoders</strong>:</p> </li> <li><strong>Description</strong>: Neural network-based unsupervised learning technique where the model learns to compress data into a lower-dimensional space (encoder) and then reconstruct the original data (decoder).</li> <li> <p><strong>Use Case</strong>: Commonly used for complex non-linear dimensionality reduction and feature learning in deep learning tasks.</p> </li> <li> <p><strong>Factor Analysis</strong>:</p> </li> <li><strong>Description</strong>: Reduces dimensionality by modeling observed variables as linear combinations of a small number of unobserved (latent) variables called factors.</li> <li> <p><strong>Use Case</strong>: Used for understanding underlying relationships in the data, often in social sciences and psychometrics.</p> </li> <li> <p><strong>Non-Negative Matrix Factorization (NMF)</strong>:</p> </li> <li><strong>Description</strong>: Factorizes a data matrix into two lower-dimensional matrices with non-negative elements, preserving part-based representation.</li> <li> <p><strong>Use Case</strong>: Common in text mining and image processing where features are non-negative (e.g., topic modeling).</p> </li> <li> <p><strong>Kernel PCA</strong>:</p> </li> <li><strong>Description</strong>: An extension of PCA that applies a kernel trick to project data into higher dimensions before performing PCA, capturing non-linear relationships.</li> <li> <p><strong>Use Case</strong>: Suitable for non-linear dimensionality reduction, often used in image recognition and pattern analysis.</p> </li> <li> <p><strong>Multidimensional Scaling (MDS)</strong>:</p> </li> <li><strong>Description</strong>: Aims to project high-dimensional data onto lower dimensions while preserving the pairwise distances between data points.</li> <li> <p><strong>Use Case</strong>: Used in cases where the geometry of the data is important, such as visualization of distance or similarity matrices.</p> </li> <li> <p><strong>Locally Linear Embedding (LLE)</strong>:</p> </li> <li><strong>Description</strong>: A non-linear technique that preserves local distances between neighboring points when mapping high-dimensional data to a lower-dimensional space.</li> <li> <p><strong>Use Case</strong>: Often used for manifold learning and non-linear dimensionality reduction in datasets where local neighborhood structure is important.</p> </li> <li> <p><strong>Isomap</strong>:</p> </li> <li><strong>Description</strong>: Extends MDS by preserving geodesic distances (distances along the manifold) rather than Euclidean distances, making it suitable for non-linear dimensionality reduction.</li> <li> <p><strong>Use Case</strong>: Used in cases where data lie on a curved manifold, such as in image or speech processing.</p> </li> <li> <p><strong>Uniform Manifold Approximation and Projection (UMAP)</strong>:</p> </li> <li><strong>Description</strong>: A non-linear technique similar to t-SNE but faster and more scalable, with better preservation of both local and global structure.</li> <li> <p><strong>Use Case</strong>: Widely used for visualization of large high-dimensional datasets in areas like genomics and natural language processing.</p> </li> <li> <p><strong>Truncated SVD (Singular Value Decomposition)</strong>:</p> </li> <li><strong>Description</strong>: A linear dimensionality reduction technique that factorizes a matrix into singular values, keeping only the top components.</li> <li> <p><strong>Use Case</strong>: Commonly used in text mining and natural language processing (e.g., Latent Semantic Analysis) when data is sparse, such as document-term matrices.</p> </li> <li> <p><strong>Self-Organizing Maps (SOMs)</strong>:</p> </li> <li><strong>Description</strong>: A type of neural network used to map high-dimensional data onto a two-dimensional grid, preserving the topology of the data.</li> <li> <p><strong>Use Case</strong>: Useful for visualizing and clustering high-dimensional data, especially in unsupervised learning.</p> </li> <li> <p><strong>Feature Agglomeration</strong>:</p> </li> <li><strong>Description</strong>: A hierarchical clustering method applied to the features of a dataset, reducing dimensionality by merging similar features.</li> <li><strong>Use Case</strong>: Useful when the feature space is large and some features are correlated or redundant.</li> </ol> <h2 id=what-is-kernel-trick-can-you-explain-with-simple-example>What is kernel trick, can you explain with simple example?<a class=headerlink href=#what-is-kernel-trick-can-you-explain-with-simple-example title="Permanent link">&para;</a></h2> <p>The <strong>kernel trick</strong> is a method used to apply linear algorithms to non-linear data by implicitly mapping the data into a higher-dimensional space without explicitly performing the transformation. This allows linear models, like Support Vector Machines (SVMs) or Principal Component Analysis (PCA), to solve problems where the data is not linearly separable in its original space. The idea behind is some data that is non-linearly separable in its original, low-dimensional space can become linearly separable when projected into a higher-dimensional space.</p> <p>Let’s say we have the following 1D dataset, where <code>x</code> represents the input feature, and the two classes are labeled as <code>red</code> (Class 1) and <code>blue</code> (Class 2):</p> <table> <thead> <tr> <th style="text-align: center;">Input Feature (x)</th> <th style="text-align: center;">Class</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;">-2</td> <td style="text-align: center;">Red</td> </tr> <tr> <td style="text-align: center;">-1</td> <td style="text-align: center;">Red</td> </tr> <tr> <td style="text-align: center;">0</td> <td style="text-align: center;">Blue</td> </tr> <tr> <td style="text-align: center;">1</td> <td style="text-align: center;">Blue</td> </tr> <tr> <td style="text-align: center;">2</td> <td style="text-align: center;">Blue</td> </tr> </tbody> </table> <p>In this simple example, the two classes overlap, and it's not possible to draw a straight line (in 1D) to separate the red and blue points.</p> <p>Non-Linear Problem in 1D: In the original 1D space, the data is <strong>non-linearly separable</strong> because the red points are close to the blue points, and no straight line can fully separate them.</p> <p>Solution Using the Kernel Trick: Instead of trying to separate the data in the original 1D space, we can <strong>map</strong> the data into a <strong>higher-dimensional space</strong>, say 2D, where the separation might become easier.</p> <p>For simplicity, let’s apply a mapping function <code>Φ(x) = (x, x²)</code> that transforms the 1D data into a 2D space, where the first dimension is <code>x</code> and the second dimension is <code>x²</code>.</p> <p>After applying this transformation, our data points become:</p> <table> <thead> <tr> <th style="text-align: center;">Original Feature (x)</th> <th style="text-align: center;">Transformed Features (Φ(x)) = (x, x²)</th> <th style="text-align: center;">Class</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;">-2</td> <td style="text-align: center;">(-2, 4)</td> <td style="text-align: center;">Red</td> </tr> <tr> <td style="text-align: center;">-1</td> <td style="text-align: center;">(-1, 1)</td> <td style="text-align: center;">Red</td> </tr> <tr> <td style="text-align: center;">0</td> <td style="text-align: center;">(0, 0)</td> <td style="text-align: center;">Blue</td> </tr> <tr> <td style="text-align: center;">1</td> <td style="text-align: center;">(1, 1)</td> <td style="text-align: center;">Blue</td> </tr> <tr> <td style="text-align: center;">2</td> <td style="text-align: center;">(2, 4)</td> <td style="text-align: center;">Blue</td> </tr> </tbody> </table> <p>Now, in the transformed 2D space (x, x²), we can draw a linear boundary (a straight line) that separates the red and blue classes. In this case, the classes become <strong>linearly separable</strong> in the higher-dimensional space.</p> <p>Popular Kernel Functions: 1. <strong>Linear Kernel</strong>: Simply the dot product in the original space (no transformation). - $$ K(x_i, x_j) = x_i^T x_j $$</p> <ol> <li><strong>Polynomial Kernel</strong>: Applies a polynomial transformation.</li> <li> <div class=arithmatex>\[ K(x_i, x_j) = (x_i^T x_j + c)^d \]</div> </li> <li> <p><strong>Radial Basis Function (RBF) or Gaussian Kernel</strong>: Maps data to an infinite-dimensional space.</p> </li> <li> <div class=arithmatex>\[ K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2) \]</div> </li> </ol> <p>Let’s say we use a <strong>polynomial kernel</strong> for the above example. Instead of transforming <code>x</code> into <code>x²</code> manually, the kernel function computes the dot product in the transformed space for us, directly operating on the original values of <code>x</code>.</p> <p>By using the kernel function, algorithms like SVM can create a linear decision boundary in the higher-dimensional space, which corresponds to a non-linear boundary in the original space.</p> <p>The kernel trick enables algorithms that rely on dot products (like SVM) to efficiently operate in high-dimensional spaces without explicitly transforming the data. This makes it easier to classify or separate non-linear data using linear techniques.</p> <h2 id=what-is-clustering>What is <strong>Clustering</strong>?<a class=headerlink href=#what-is-clustering title="Permanent link">&para;</a></h2> <p>A type of unsupervised learning where the goal is to group similar data points into clusters without predefined labels. Clustering can help discover patterns and structure in data, particularly when dealing with unlabelled data or when performing exploratory data analysis. These clustering algorithms have various applications across different domains, including customer segmentation, image analysis, anomaly detection, and more.</p> <h2 id=what-are-popular-clustering-algorithms>What are popular clustering algorithms?<a class=headerlink href=#what-are-popular-clustering-algorithms title="Permanent link">&para;</a></h2> <p>There are many clustering algorithms and the choice of algorithm often depends on the nature of the data and the specific requirements of the task at hand.</p> <ol> <li><strong>K-Means Clustering</strong>:</li> <li><strong>Description</strong>: Partitions data into K clusters by minimizing the variance within each cluster.</li> <li> <p><strong>Use Case</strong>: Suitable for large datasets and when the number of clusters is known a priori.</p> </li> <li> <p><strong>Hierarchical Clustering</strong>:</p> </li> <li><strong>Description</strong>: Builds a tree of clusters (dendrogram) either by agglomerative (bottom-up) or divisive (top-down) methods.</li> <li> <p><strong>Use Case</strong>: Useful for small datasets and when a hierarchy of clusters is needed.</p> </li> <li> <p><strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong>:</p> </li> <li><strong>Description</strong>: Groups together points that are closely packed together, marking points in low-density regions as outliers.</li> <li> <p><strong>Use Case</strong>: Effective for spatial data and identifying clusters of varying shapes and sizes.</p> </li> <li> <p><strong>Mean Shift</strong>:</p> </li> <li><strong>Description</strong>: A non-parametric clustering algorithm that shifts points towards the mode of the data distribution iteratively.</li> <li> <p><strong>Use Case</strong>: Good for finding clusters in complex data distributions without assuming the number of clusters.</p> </li> <li> <p><strong>Gaussian Mixture Models (GMM)</strong>:</p> </li> <li><strong>Description</strong>: Represents the data as a mixture of multiple Gaussian distributions and uses the Expectation-Maximization algorithm for parameter estimation.</li> <li> <p><strong>Use Case</strong>: Suitable for soft clustering where data points can belong to multiple clusters with different probabilities.</p> </li> <li> <p><strong>Agglomerative Clustering</strong>:</p> </li> <li><strong>Description</strong>: A type of hierarchical clustering that starts with individual points and merges them into clusters based on distance.</li> <li> <p><strong>Use Case</strong>: Useful for determining the number of clusters dynamically and visualizing data hierarchically.</p> </li> <li> <p><strong>Spectral Clustering</strong>:</p> </li> <li><strong>Description</strong>: Uses the eigenvalues of a similarity matrix to reduce dimensionality before applying a clustering algorithm like K-means.</li> <li> <p><strong>Use Case</strong>: Effective for clustering non-convex shapes and complex data structures.</p> </li> <li> <p><strong>Affinity Propagation</strong>:</p> </li> <li><strong>Description</strong>: Clusters data by sending messages between pairs of points, identifying exemplars that represent clusters.</li> <li> <p><strong>Use Case</strong>: Useful for clustering with unknown numbers of clusters and can handle large datasets efficiently.</p> </li> <li> <p><strong>BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)</strong>:</p> </li> <li><strong>Description</strong>: Incrementally builds a tree structure to represent clusters and refines them iteratively.</li> <li> <p><strong>Use Case</strong>: Suitable for large datasets and online clustering applications.</p> </li> <li> <p><strong>OPTICS (Ordering Points To Identify the Clustering Structure)</strong>:</p> </li> <li><strong>Description</strong>: Extends DBSCAN by creating an ordering of points that captures the clustering structure, allowing for varying density clusters.</li> <li> <p><strong>Use Case</strong>: Effective for data with varying density, enabling a more comprehensive analysis of cluster structures.</p> </li> <li> <p><strong>Fuzzy C-Means (FCM)</strong>:</p> </li> <li><strong>Description</strong>: A clustering method where each point can belong to multiple clusters with varying degrees of membership.</li> <li> <p><strong>Use Case</strong>: Suitable for problems where data points are not easily separable into distinct clusters.</p> </li> <li> <p><strong>K-Medoids (PAM)</strong>:</p> </li> <li><strong>Description</strong>: Similar to K-means but selects actual data points as cluster centers (medoids), making it more robust to outliers.</li> <li> <p><strong>Use Case</strong>: Effective for datasets with noise or when interpretability is crucial.</p> </li> <li> <p><strong>Clustering by Committee (CBC)</strong>:</p> </li> <li><strong>Description</strong>: An ensemble approach where multiple clustering results are combined to form a final clustering output.</li> <li> <p><strong>Use Case</strong>: Useful when different clustering algorithms yield varied results, providing a more robust solution.</p> </li> <li> <p><strong>Self-Organizing Maps (SOM)</strong>:</p> </li> <li><strong>Description</strong>: A type of neural network that reduces dimensions and visualizes data by organizing similar data points on a grid.</li> <li> <p><strong>Use Case</strong>: Useful for visualizing high-dimensional data in a lower-dimensional space while maintaining the topological properties.</p> </li> <li> <p><strong>Density-Based Clustering (like HDBSCAN)</strong>:</p> </li> <li><strong>Description</strong>: An extension of DBSCAN that finds clusters of varying densities and scales while identifying outliers.</li> <li><strong>Use Case</strong>: Effective for large datasets with varying density clusters and noise handling.</li> </ol> <h2 id=what-is-deep-learning-and-neural-networks>What is Deep Learning and Neural Networks?<a class=headerlink href=#what-is-deep-learning-and-neural-networks title="Permanent link">&para;</a></h2> <p>Deep learning is a subset of machine learning based on artificial neural networks with many layers (deep neural networks). Deep learning excels at tasks such as image recognition, natural language processing, and speech recognition by automatically learning complex patterns from raw data.</p> <h2 id=what-is-self-supervised-learning-ssl>What is Self-Supervised Learning (SSL)?<a class=headerlink href=#what-is-self-supervised-learning-ssl title="Permanent link">&para;</a></h2> <p>A type of learning where the model learns useful representations from unlabelled data by generating pseudo-labels from the data itself. In traditional supervised learning, vast amounts of labeled data are needed, which is often costly and time-consuming to gather. Self-supervised learning allows models to learn from unlabeled data, reducing the reliance on manual annotation. It has been widely successful in domains like <strong>natural language processing (NLP)</strong> (e.g., GPT models) and <strong>computer vision</strong> (e.g., contrastive learning with models like SimCLR and MoCo).</p> <h2 id=what-is-meta-learning-learning-to-learn>what is Meta-Learning (Learning to Learn)?<a class=headerlink href=#what-is-meta-learning-learning-to-learn title="Permanent link">&para;</a></h2> <p>Meta-learning focuses on creating models that can learn new tasks with very few examples, often referred to as <strong>few-shot learning</strong> or <strong>transfer of learning</strong>. It mimics human-like learning, where we can generalize knowledge across tasks. In many real-world scenarios, it's impractical to have large datasets for every new problem. Meta-learning enables models to quickly adapt to new tasks with minimal data. MAML (Model-Agnostic Meta-Learning), where a model is trained in such a way that it can quickly adapt to new tasks with few training steps.</p> <h2 id=what-is-reinforcement-learning-rl>What is Reinforcement Learning (RL)?<a class=headerlink href=#what-is-reinforcement-learning-rl title="Permanent link">&para;</a></h2> <p>A type of learning where an agent learns to make decisions by interacting with an environment, receiving rewards or penalties based on its actions.Reinforcement learning is used for tasks that require long-term strategy and decision-making, such as game playing (e.g., AlphaGo, OpenAI's Dota 2 bot), robotics, autonomous vehicles, and recommendation systems.</p> <h2 id=what-is-generative-model>What is Generative Model?<a class=headerlink href=#what-is-generative-model title="Permanent link">&para;</a></h2> <p>These models learn the underlying data distribution and generate new data points that resemble the training data. They fall under <strong>unsupervised learning</strong> and are used for tasks like image generation, text generation, and data synthesis. Generative models can create synthetic data, simulate environments, or generate realistic media, contributing to areas like <strong>AI art</strong>, game development, and <strong>data augmentation</strong>.</p> <h2 id=what-are-different-generative-models>What are different Generative Models?<a class=headerlink href=#what-are-different-generative-models title="Permanent link">&para;</a></h2> <p>Generative models are a class of statistical models that aim to generate new data samples from a learned distribution based on existing data. Here’s a list of different types of generative models:</p> <ol> <li><strong>Gaussian Mixture Models (GMM)</strong>:</li> <li><strong>Description</strong>: A probabilistic model that represents a mixture of multiple Gaussian distributions, used to model data with multiple underlying groups.</li> <li> <p><strong>Use Case</strong>: Useful for clustering, density estimation, and scenarios where data points can belong to different subpopulations.</p> </li> <li> <p><strong>Variational Autoencoders (VAEs)</strong>:</p> </li> <li><strong>Description</strong>: A type of autoencoder that learns a probabilistic representation of the data by maximizing a lower bound on the likelihood of the data.</li> <li> <p><strong>Use Case</strong>: Effective for generating new data samples similar to the training set, commonly used in image generation and anomaly detection.</p> </li> <li> <p><strong>Generative Adversarial Networks (GANs)</strong>:</p> </li> <li><strong>Description</strong>: Comprises two neural networks (generator and discriminator) that compete against each other, where the generator creates fake data and the discriminator tries to distinguish between real and fake data.</li> <li> <p><strong>Use Case</strong>: Widely used in image generation, video synthesis, and generating high-quality samples from complex distributions.</p> </li> <li> <p><strong>Deep Belief Networks (DBNs)</strong>:</p> </li> <li><strong>Description</strong>: A stack of Restricted Boltzmann Machines (RBMs) where each layer can learn to represent features of the input data hierarchically.</li> <li> <p><strong>Use Case</strong>: Used in dimensionality reduction, feature learning, and generative tasks.</p> </li> <li> <p><strong>Restricted Boltzmann Machines (RBMs)</strong>:</p> </li> <li><strong>Description</strong>: A type of stochastic neural network that can learn a probability distribution over its set of inputs.</li> <li> <p><strong>Use Case</strong>: Commonly used for collaborative filtering, dimensionality reduction, and as building blocks for deep learning models.</p> </li> <li> <p><strong>Autoregressive Models</strong>:</p> </li> <li><strong>Description</strong>: Models that predict the next data point in a sequence based on previous points (e.g., PixelCNN, WaveNet).</li> <li> <p><strong>Use Case</strong>: Effective in generating sequential data such as text, audio, and images by modeling the joint distribution of the data.</p> </li> <li> <p><strong>Normalizing Flows</strong>:</p> </li> <li><strong>Description</strong>: A family of generative models that transform a simple distribution (like a Gaussian) into a more complex distribution through a series of invertible transformations.</li> <li> <p><strong>Use Case</strong>: Useful for high-dimensional data and for estimating complex densities while allowing exact likelihood computation.</p> </li> <li> <p><strong>Conditional Generative Models</strong>:</p> </li> <li><strong>Description</strong>: Generative models that condition on additional information, such as labels or features (e.g., Conditional GANs, Conditional VAEs).</li> <li> <p><strong>Use Case</strong>: Effective in tasks like image-to-image translation and text-to-image synthesis, where specific conditions must be met in the generated data.</p> </li> <li> <p><strong>Score-Based Generative Models</strong>:</p> </li> <li><strong>Description</strong>: A class of models that utilize the score function of the data distribution to generate samples, often through Langevin dynamics or diffusion processes.</li> <li> <p><strong>Use Case</strong>: Used for high-fidelity image generation and sampling from complex distributions.</p> </li> <li> <p><strong>Transformers for Generation</strong>:</p> </li> <li><strong>Description</strong>: Transformer architectures can be adapted for generative tasks, generating sequences by predicting the next element based on previous ones (e.g., GPT models).</li> <li> <p><strong>Use Case</strong>: Widely used for natural language processing tasks, text generation, and machine translation.</p> </li> <li> <p><strong>Diffusion Models</strong>:</p> </li> <li><strong>Description</strong>: Models that iteratively denoise a sample from a simple distribution (like Gaussian) to generate new samples from a target distribution.</li> <li><strong>Use Case</strong>: Effective for image generation and denoising tasks, gaining popularity in recent research.</li> </ol> <h2 id=what-is-federated-learning>What is Federated Learning?<a class=headerlink href=#what-is-federated-learning title="Permanent link">&para;</a></h2> <p>A decentralized form of machine learning where models are trained across multiple devices or servers without sharing the raw data. Instead, model updates are shared and aggregated. Federated learning is crucial for privacy-sensitive applications where data cannot leave the local device, such as healthcare, personal mobile devices, and financial services. It allows for collaborative model building while maintaining user privacy. For example, Google uses federated learning to improve its predictive text features without accessing individual users’ data.</p> <h2 id=what-is-causal-inference>What is Causal Inference?<a class=headerlink href=#what-is-causal-inference title="Permanent link">&para;</a></h2> <p>Causal inference goes beyond correlation to determine <strong>cause-and-effect relationships</strong> between variables. Traditional machine learning focuses on predicting outcomes, while causal inference attempts to understand the underlying mechanism causing those outcomes. Understanding causality allows us to make better decisions and predictions, especially in fields like healthcare, economics, and policy-making. It helps answer questions like "If we intervene on variable X, how will it affect Y? Techniques like Directed Acyclic Graphs (DAGs), <strong>Instrumental Variables</strong>, and <strong>Counterfactual Analysis</strong> are used for causal inference.</p> <h2 id=what-is-neural-architecture-search-nas>What is Neural Architecture Search (NAS)?<a class=headerlink href=#what-is-neural-architecture-search-nas title="Permanent link">&para;</a></h2> <p>Neural Architecture Search is an automated process that designs optimal neural network architectures without human intervention. It uses algorithms to explore various configurations of layers, neurons, and connections. We know, designing neural networks often requires expert knowledge and manual trial-and-error. NAS automates this process, making deep learning more accessible and efficient.</p> <h2 id=what-are-transformers-and-attention-mechanisms>What are Transformers and Attention Mechanisms?<a class=headerlink href=#what-are-transformers-and-attention-mechanisms title="Permanent link">&para;</a></h2> <p>Transformers are a type of deep learning architecture, initially designed for natural language processing, that leverage <strong>attention mechanisms</strong> to weigh the importance of different parts of the input sequence. Transformers, like GPT, BERT, and T5, have revolutionized NLP and are now expanding into other domains like computer vision, protein folding (AlphaFold), and reinforcement learning. <strong>Self-Attention</strong>: Allows the model to focus on different parts of the input data, understanding relationships between distant parts of the sequence.</p> <h2 id=what-is-explainable-ai-xai>What is <strong>Explainable AI (XAI)</strong>?<a class=headerlink href=#what-is-explainable-ai-xai title="Permanent link">&para;</a></h2> <p>Explainable AI focuses on making machine learning models interpretable and transparent, particularly for complex models like neural networks. As machine learning is increasingly used in critical areas like healthcare, finance, and criminal justice, it's essential to understand how models make decisions to build trust and ensure fairness.</p> <h2 id=what-are-popular-xai-methods>What are popular XAI methods?<a class=headerlink href=#what-are-popular-xai-methods title="Permanent link">&para;</a></h2> <ol> <li><strong>LIME (Local Interpretable Model-agnostic Explanations)</strong>:</li> <li><strong>Description</strong>: Analyzes the predictions of any classifier by perturbing the input data and observing changes in predictions, generating locally interpretable explanations.</li> <li> <p><strong>Use Case</strong>: Useful for understanding individual predictions by providing insights into which features are driving specific outcomes.</p> </li> <li> <p><strong>SHAP (SHapley Additive exPlanations)</strong>:</p> </li> <li><strong>Description</strong>: Based on game theory, SHAP assigns each feature an importance value for a particular prediction, explaining how features contribute to the model's output.</li> <li> <p><strong>Use Case</strong>: Provides consistent and interpretable feature attributions across different models and is effective for both local and global explanations.</p> </li> <li> <p><strong>Feature Importance</strong>:</p> </li> <li><strong>Description</strong>: Measures the impact of individual features on model predictions, often calculated using techniques like permutation importance or tree-based feature importance.</li> <li> <p><strong>Use Case</strong>: Helps in identifying which features are most influential in the model’s decisions.</p> </li> <li> <p><strong>Partial Dependence Plots (PDP)</strong>:</p> </li> <li><strong>Description</strong>: Visualizes the relationship between a feature and the predicted outcome while marginalizing over the other features.</li> <li> <p><strong>Use Case</strong>: Provides insights into how changes in a specific feature affect predictions, making it easier to understand model behavior.</p> </li> <li> <p><strong>Individual Conditional Expectation (ICE) Plots</strong>:</p> </li> <li><strong>Description</strong>: Similar to PDPs but show how the predicted outcome varies for each instance as a specific feature changes.</li> <li> <p><strong>Use Case</strong>: Helps to visualize heterogeneous effects of a feature across different instances.</p> </li> <li> <p><strong>Counterfactual Explanations</strong>:</p> </li> <li><strong>Description</strong>: Provides explanations by showing how minimal changes to the input features could change the model’s prediction.</li> <li> <p><strong>Use Case</strong>: Useful in decision-making contexts where understanding "what-if" scenarios is critical.</p> </li> <li> <p><strong>Saliency Maps</strong>:</p> </li> <li><strong>Description</strong>: Visualizes the most influential pixels or regions in input images for a given prediction, often used in convolutional neural networks (CNNs).</li> <li> <p><strong>Use Case</strong>: Helps to interpret model decisions in image classification tasks by highlighting important features in the input image.</p> </li> <li> <p><strong>Grad-CAM (Gradient-weighted Class Activation Mapping)</strong>:</p> </li> <li><strong>Description</strong>: A visualization technique for CNNs that uses gradients to produce a heatmap of important regions in an image for a particular class.</li> <li> <p><strong>Use Case</strong>: Useful for visualizing which parts of an image contribute most to a particular prediction in image classification tasks.</p> </li> <li> <p><strong>Rule-Based Explanations</strong>:</p> </li> <li><strong>Description</strong>: Extracts human-readable rules from complex models (like decision trees or ensembles) to explain predictions.</li> <li> <p><strong>Use Case</strong>: Provides interpretable explanations for model decisions in a format that is easily understandable by non-experts.</p> </li> <li> <p><strong>Model Distillation</strong>:</p> </li> <li><strong>Description</strong>: Involves creating a simpler, interpretable model that approximates the behavior of a more complex model.</li> <li> <p><strong>Use Case</strong>: Useful when a trade-off between accuracy and interpretability is needed, enabling stakeholders to understand the decision-making process.</p> </li> <li> <p><strong>Anchors</strong>:</p> </li> <li><strong>Description</strong>: Provides high-precision, locally interpretable explanations by identifying a subset of features that sufficiently explains a prediction.</li> <li> <p><strong>Use Case</strong>: Offers robust explanations for individual predictions, highlighting key features that influence the outcome.</p> </li> <li> <p><strong>Textual Explanations</strong>:</p> </li> <li><strong>Description</strong>: Generates natural language explanations for model predictions, often using techniques like sequence-to-sequence models.</li> <li> <p><strong>Use Case</strong>: Enhances interpretability for non-technical stakeholders by providing explanations in an accessible format.</p> </li> <li> <p><strong>Transparency in Model Design</strong>:</p> </li> <li><strong>Description</strong>: Involves using inherently interpretable models (like linear regression or decision trees) to ensure explanations are built into the model's architecture.</li> <li><strong>Use Case</strong>: Simplifies the explanation process by using models that are easy to understand and communicate.</li> </ol> <h2 id=what-is-uncertainty-quantification>What is Uncertainty Quantification?<a class=headerlink href=#what-is-uncertainty-quantification title="Permanent link">&para;</a></h2> <p>Uncertainty quantification in machine learning helps measure the confidence of a model's predictions. Instead of making hard predictions, models can output probability distributions or intervals that indicate the uncertainty in the prediction. In many critical applications (like healthcare diagnostics or autonomous driving), it's essential to know how certain the model is about its predictions. This helps in decision-making, particularly when model predictions might have serious consequences. Techniques like <strong>Bayesian Neural Networks</strong>, Introduce uncertainty by placing distributions over the model parameters. <strong>Dropout as Bayesian Approximation</strong> is another technique where dropout layers (originally used to prevent overfitting) are used at inference time to generate a distribution of predictions, estimating uncertainty.</p> <h2 id=what-is-continual-learning-lifelong-learning>What is Continual Learning (Lifelong Learning)?<a class=headerlink href=#what-is-continual-learning-lifelong-learning title="Permanent link">&para;</a></h2> <p>Continual learning refers to the ability of a model to learn new tasks sequentially while retaining knowledge from previous tasks, avoiding the issue of <strong>catastrophic forgetting</strong> (where the model forgets previous knowledge). This is important for real-world scenarios where new data is constantly generated, and models need to adapt to new tasks without retraining from scratch. It is used in Robotics, personal AI assistants, autonomous systems that need to continuously adapt to changing environments.</p> <h2 id=what-is-adversarial-machine-learning>What is Adversarial Machine Learning?<a class=headerlink href=#what-is-adversarial-machine-learning title="Permanent link">&para;</a></h2> <p>Adversarial machine learning studies how machine learning models can be fooled or attacked by adversarial inputs, which are carefully crafted to cause the model to make incorrect predictions. Ensuring the robustness and security of machine learning models is crucial, particularly in sensitive areas like autonomous driving, cybersecurity, and facial recognition. In computer vision, slight perturbations to an image (imperceptible to humans) can cause a neural network to misclassify the image, but we don't it to happen. So machine should be able learn there is adversarial attach on this image and actually image is ABC or something else.</p> <h2 id=hastags>Hastags<a class=headerlink href=#hastags title="Permanent link">&para;</a></h2> <h1 id=machinelearning>MachineLearning<a class=headerlink href=#machinelearning title="Permanent link">&para;</a></h1> <h1 id=datascience>DataScience<a class=headerlink href=#datascience title="Permanent link">&para;</a></h1> <h1 id=crossvalidation>CrossValidation<a class=headerlink href=#crossvalidation title="Permanent link">&para;</a></h1> <h1 id=hyperparametertuning>HyperparameterTuning<a class=headerlink href=#hyperparametertuning title="Permanent link">&para;</a></h1> <h1 id=ensemblelearning>EnsembleLearning<a class=headerlink href=#ensemblelearning title="Permanent link">&para;</a></h1> <h1 id=dimensionalityreduction>DimensionalityReduction<a class=headerlink href=#dimensionalityreduction title="Permanent link">&para;</a></h1> <h1 id=clustering>Clustering<a class=headerlink href=#clustering title="Permanent link">&para;</a></h1> <h1 id=deeplearning>DeepLearning<a class=headerlink href=#deeplearning title="Permanent link">&para;</a></h1> <h1 id=explainableai>ExplainableAI<a class=headerlink href=#explainableai title="Permanent link">&para;</a></h1> <h1 id=generativemodels>GenerativeModels<a class=headerlink href=#generativemodels title="Permanent link">&para;</a></h1> <h1 id=airesearch>AIResearch<a class=headerlink href=#airesearch title="Permanent link">&para;</a></h1> <h1 id=featureengineering>FeatureEngineering<a class=headerlink href=#featureengineering title="Permanent link">&para;</a></h1> <h1 id=biasvariance>BiasVariance<a class=headerlink href=#biasvariance title="Permanent link">&para;</a></h1> <h1 id=selfsupervisedlearning>SelfSupervisedLearning<a class=headerlink href=#selfsupervisedlearning title="Permanent link">&para;</a></h1> <h1 id=federatedlearning>FederatedLearning<a class=headerlink href=#federatedlearning title="Permanent link">&para;</a></h1> <h1 id=neuralnetworks>NeuralNetworks<a class=headerlink href=#neuralnetworks title="Permanent link">&para;</a></h1> <div class=author-bio style="margin-top: 2rem; display: flex; gap: 1rem;"> <img src=../assets/images/myphotos/Profilephoto1.jpg alt="Hari Thapliyaal" style="border-radius: 50%; width: 80px; height: 80px;"> <div> <strong>Dr. Hari Thapliyaal</strong><br> <small>Dr. Hari Thapliyal is a prolific blogger and seasoned professional with an extensive background in Data Science, Project Management, and Advait-Vedanta Philosophy. He holds a Doctorate in AI/NLP from SSBM, Geneva, along with Master’s degrees in Computers, Business Management, Data Science, and Economics. With over three decades of experience in management and leadership, Hari has extensive expertise in training, consulting, and coaching within the technology sector. His specializations include Data Science, AI, Computer Vision, NLP, and machine learning. Hari is also passionate about meditation and nature, often retreating to secluded places for reflection and peace.</small> </div> </div> <div class=share-buttons style="margin-top: 2rem;"> <strong>Share this article:</strong><br> <a href="https://twitter.com/intent/tweet?text=Machine%20Learning%20Key%20Concepts&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/Machine-Learning-Key-Concepts.html" target=_blank>Twitter</a> | <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/Machine-Learning-Key-Concepts.html" target=_blank>Facebook</a> | <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/Machine-Learning-Key-Concepts.html" target=_blank>LinkedIn</a> </div> <div id=comments style="margin-top: 3rem;"> <script src=https://giscus.app/client.js data-repo=dasarpai/dasarpai-comments data-repo-id=R_kgDOOGVFpA data-category=General data-category-id=DIC_kwDOOGVFpM4CnzHR data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async>
        </script> </div> </article> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.js></script> </body> </html>