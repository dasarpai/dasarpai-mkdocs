<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/LLM-Architecture-and-Training.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>LLM Architecture and Training - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta name=author content="Hari Thapliyaal"><meta name=description content="Dive deep into the architecture and training process of Large Language Models. Understand the components, training methodologies, and optimization techniques used in building advanced AI models."><meta name=keywords content="LLM Training, Model Architecture, Neural Network Design, AI Model Development, Deep Learning Training, Language Model Architecture, Transformer Networks, Model Optimization"><meta property=og:type content=article><meta property=og:locale content=en_US><meta property=og:site_name content=DasarpAI><meta property=og:title content="LLM Architecture and Training"><meta property=og:description content="Dive deep into the architecture and training process of Large Language Models. Understand the components, training methodologies, and optimization techniques used in building advanced AI models."><meta property=og:url content=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/LLM-Architecture-and-Training.html><meta property=og:image content=../../assets/images/dspost/dsp6129-LLM-Architecture-and-Training.jpg><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta name=twitter:card content=summary_large_image><meta name=twitter:site content=@dasarpai><meta name=twitter:title content="LLM Architecture and Training"><meta name=twitter:description content="Dive deep into the architecture and training process of Large Language Models. Understand the components, training methodologies, and optimization techniques used in building advanced AI models."><meta name=twitter:image content=../../assets/images/dspost/dsp6129-LLM-Architecture-and-Training.jpg><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/LLM-Architecture-and-Training.html><link rel=stylesheet href=../assets/stylesheets/custom.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#understanding-llm-architectures-and-model-training class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@harithapliyal</strong> on <a rel=me href=https://linkedin.com/in/harithapliyal> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/dasarpai> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> LLM Architecture and Training </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-introduction-to-large-language-models-llms class=md-nav__link> <span class=md-ellipsis> 1. Introduction to Large Language Models (LLMs) </span> </a> <nav class=md-nav aria-label="1. Introduction to Large Language Models (LLMs)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#definition-and-importance-of-llms class=md-nav__link> <span class=md-ellipsis> Definition and Importance of LLMs </span> </a> </li> <li class=md-nav__item> <a href=#evolution-and-milestones-in-llm-development class=md-nav__link> <span class=md-ellipsis> Evolution and Milestones in LLM Development </span> </a> </li> <li class=md-nav__item> <a href=#significance-of-llms-in-modern-ai-applications class=md-nav__link> <span class=md-ellipsis> Significance of LLMs in Modern AI Applications </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-core-architectures-of-llms class=md-nav__link> <span class=md-ellipsis> 2. Core Architectures of LLMs </span> </a> <nav class=md-nav aria-label="2. Core Architectures of LLMs"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#transformers-the-foundation-of-llms class=md-nav__link> <span class=md-ellipsis> Transformers: The Foundation of LLMs </span> </a> </li> <li class=md-nav__item> <a href=#key-models-and-differences-bert-gpt-and-t5 class=md-nav__link> <span class=md-ellipsis> Key Models and Differences (BERT, GPT, and T5) </span> </a> </li> <li class=md-nav__item> <a href=#attention-mechanisms class=md-nav__link> <span class=md-ellipsis> Attention Mechanisms </span> </a> </li> <li class=md-nav__item> <a href=#encoder-decoder-architectures-vs-autoregressive-models class=md-nav__link> <span class=md-ellipsis> Encoder-Decoder Architectures vs. Autoregressive Models </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3-components-of-llm-training class=md-nav__link> <span class=md-ellipsis> 3. Components of LLM Training </span> </a> <nav class=md-nav aria-label="3. Components of LLM Training"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#data-collection-and-preprocessing class=md-nav__link> <span class=md-ellipsis> Data Collection and Preprocessing </span> </a> </li> <li class=md-nav__item> <a href=#tokenization-vocabulary-choices-and-byte-pair-encoding class=md-nav__link> <span class=md-ellipsis> Tokenization: Vocabulary Choices and Byte-Pair Encoding </span> </a> </li> <li class=md-nav__item> <a href=#training-objectives-masked-and-causal-language-modeling class=md-nav__link> <span class=md-ellipsis> Training Objectives: Masked and Causal Language Modeling </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-llm-fine-tuning-approaches class=md-nav__link> <span class=md-ellipsis> 4. LLM Fine-tuning Approaches </span> </a> <nav class=md-nav aria-label="4. LLM Fine-tuning Approaches"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#approaches-to-fine-tuning-llms class=md-nav__link> <span class=md-ellipsis> Approaches to Fine-tuning LLMs </span> </a> </li> <li class=md-nav__item> <a href=#modern-fine-tuning-approaches class=md-nav__link> <span class=md-ellipsis> Modern fine-tuning approaches </span> </a> </li> <li class=md-nav__item> <a href=#selecting-layers-to-update class=md-nav__link> <span class=md-ellipsis> Selecting Layers to Update </span> </a> </li> <li class=md-nav__item> <a href=#weight-adjustments-during-fine-tuning class=md-nav__link> <span class=md-ellipsis> Weight Adjustments During Fine-tuning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#5-training-infrastructure-for-large-language-models class=md-nav__link> <span class=md-ellipsis> 5. Training Infrastructure for Large Language Models </span> </a> <nav class=md-nav aria-label="5. Training Infrastructure for Large Language Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#distributed-computing-for-large-scale-training class=md-nav__link> <span class=md-ellipsis> Distributed Computing for Large-scale Training </span> </a> </li> <li class=md-nav__item> <a href=#specialized-hardware-gpus-and-tpus class=md-nav__link> <span class=md-ellipsis> Specialized Hardware: GPUs and TPUs </span> </a> </li> <li class=md-nav__item> <a href=#efficient-training-methods class=md-nav__link> <span class=md-ellipsis> Efficient Training Methods </span> </a> </li> <li class=md-nav__item> <a href=#scalable-and-cost-effective-cloud-solutions class=md-nav__link> <span class=md-ellipsis> Scalable and Cost-effective Cloud Solutions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#6-evaluation-and-benchmarking class=md-nav__link> <span class=md-ellipsis> 6. Evaluation and Benchmarking </span> </a> <nav class=md-nav aria-label="6. Evaluation and Benchmarking"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-are-benchmarks-in-ai class=md-nav__link> <span class=md-ellipsis> What Are Benchmarks in AI? </span> </a> </li> <li class=md-nav__item> <a href=#key-components-of-ai-benchmarks class=md-nav__link> <span class=md-ellipsis> Key Components of AI Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#importance-of-benchmarks class=md-nav__link> <span class=md-ellipsis> Importance of Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#example-benchmarks-in-practice class=md-nav__link> <span class=md-ellipsis> Example Benchmarks in Practice </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#7-popular-llm-benchmarks-and-n-shot-learning class=md-nav__link> <span class=md-ellipsis> 7. Popular LLM Benchmarks and n-shot Learning </span> </a> <nav class=md-nav aria-label="7. Popular LLM Benchmarks and n-shot Learning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-n-shot-learning class=md-nav__link> <span class=md-ellipsis> What Is n-shot Learning? </span> </a> </li> <li class=md-nav__item> <a href=#applications-of-n-shot-learning-in-llm-benchmarks class=md-nav__link> <span class=md-ellipsis> Applications of n-shot Learning in LLM Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#8-llm-fine-tuning-approaches class=md-nav__link> <span class=md-ellipsis> 8. LLM Fine-tuning Approaches </span> </a> </li> <li class=md-nav__item> <a href=#9-evaluation-metrics-for-llms class=md-nav__link> <span class=md-ellipsis> 9. Evaluation Metrics for LLMs </span> </a> </li> <li class=md-nav__item> <a href=#10-challenges-and-limitations-in-llm-training class=md-nav__link> <span class=md-ellipsis> 10. Challenges and Limitations in LLM Training </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <article class="md-content__inner md-typeset"> <p><img alt=LLM-Architecture-and-Training src=../assets/images/dspost/dsp6129-LLM-Architecture-and-Training.jpg></p> <h1 id=understanding-llm-architectures-and-model-training><strong>Understanding LLM Architectures and Model Training</strong><a class=headerlink href=#understanding-llm-architectures-and-model-training title="Permanent link">&para;</a></h1> <p>Large Language Models (LLMs) are transforming the field of artificial intelligence by enabling machines to understand and generate human language with unprecedented accuracy. This article delves into the architecture, training methods, and practical applications of LLMs. We’ll explore the core components that make these models so powerful and explain how they are trained and fine-tuned for real-world use cases.</p> <h2 id=1-introduction-to-large-language-models-llms><strong>1. Introduction to Large Language Models (LLMs)</strong><a class=headerlink href=#1-introduction-to-large-language-models-llms title="Permanent link">&para;</a></h2> <h3 id=definition-and-importance-of-llms><strong>Definition and Importance of LLMs</strong><a class=headerlink href=#definition-and-importance-of-llms title="Permanent link">&para;</a></h3> <p>Large Language Models are advanced deep learning models trained on massive amounts of text data. LLMs have made it possible to perform a wide variety of natural language tasks, from answering complex questions to generating human-like responses in chat applications. These models use billions (sometimes trillions) of parameters to capture intricate relationships within language, enabling them to comprehend and generate coherent responses.</p> <p>LLMs play an instrumental role across diverse applications, such as content creation, automated customer support, and scientific research. The sheer size and training complexity of these models equip them with a nuanced understanding of language, transforming how we interact with machines.</p> <h3 id=evolution-and-milestones-in-llm-development><strong>Evolution and Milestones in LLM Development</strong><a class=headerlink href=#evolution-and-milestones-in-llm-development title="Permanent link">&para;</a></h3> <p>The development of LLMs has advanced rapidly, with some key milestones including:</p> <ul> <li><strong>Word Embeddings</strong>: Early models like <strong>Word2Vec</strong> and <strong>GloVe</strong> introduced word embeddings, which assign each word a continuous vector, allowing models to capture relationships between words.</li> <li><strong>Transformers and Attention</strong>: The release of the <strong>Transformer</strong> model by Vaswani et al. in 2017 revolutionized LLM architecture, leading to bidirectional and autoregressive models capable of nuanced text understanding and generation.</li> <li><strong>Pre-trained Transformers</strong>: Models such as <strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) and <strong>GPT</strong> (Generative Pre-trained Transformer) expanded LLM applications, marking major advances in natural language processing.</li> </ul> <p>The latest models like <strong>GPT-4</strong> and <strong>Claude</strong> continue this trend, with billions of parameters enabling them to tackle more complex tasks across domains like healthcare, finance, and scientific discovery.</p> <h3 id=significance-of-llms-in-modern-ai-applications><strong>Significance of LLMs in Modern AI Applications</strong><a class=headerlink href=#significance-of-llms-in-modern-ai-applications title="Permanent link">&para;</a></h3> <p>LLMs power many applications across industries. For example, in healthcare, LLMs assist in summarizing medical records and providing insights into patient data. In customer service, they enable chatbots to handle inquiries with a human-like response. Such applications demonstrate how LLMs reshape business processes by enhancing efficiency and improving user experiences.</p> <hr> <h2 id=2-core-architectures-of-llms><strong>2. Core Architectures of LLMs</strong><a class=headerlink href=#2-core-architectures-of-llms title="Permanent link">&para;</a></h2> <h3 id=transformers-the-foundation-of-llms><strong>Transformers: The Foundation of LLMs</strong><a class=headerlink href=#transformers-the-foundation-of-llms title="Permanent link">&para;</a></h3> <p>The <strong>Transformer</strong> architecture, introduced in 2017, underpins most modern LLMs. Its defining feature is the <strong>self-attention mechanism</strong>, which allows the model to focus on different parts of the input sequence when predicting the next token. This mechanism provides the model with a global context for each word, allowing it to make connections across long text sequences efficiently.</p> <p>Transformers are structured around <strong>multi-head attention</strong>, <strong>position-wise feed-forward networks</strong>, and <strong>residual connections</strong>, which enable them to capture complex dependencies between words. This structure allows LLMs to process large amounts of text data efficiently and effectively.</p> <h3 id=key-models-and-differences-bert-gpt-and-t5><strong>Key Models and Differences (BERT, GPT, and T5)</strong><a class=headerlink href=#key-models-and-differences-bert-gpt-and-t5 title="Permanent link">&para;</a></h3> <p>Each of these models brings unique strengths to NLP tasks:</p> <ul> <li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: BERT is trained to understand the full context of words by looking at both the left and right context simultaneously (bidirectionally). This is particularly useful for tasks like text classification and question-answering, where deep comprehension is required.</li> <li><strong>GPT (Generative Pre-trained Transformer)</strong>: GPT models are autoregressive, meaning they generate text from left to right by predicting the next word based on prior words. This makes them ideal for generative tasks, such as text completion and storytelling.</li> <li><strong>T5 (Text-To-Text Transfer Transformer)</strong>: T5 redefines tasks as a text-to-text problem, enabling the model to tackle various tasks by framing inputs and outputs as sequences of text. This flexibility makes T5 effective across a wide range of NLP applications.</li> </ul> <h3 id=attention-mechanisms><strong>Attention Mechanisms</strong><a class=headerlink href=#attention-mechanisms title="Permanent link">&para;</a></h3> <p>At the heart of Transformer-based models is the <strong>attention mechanism</strong>, which enables models to focus on specific words that are contextually relevant. The <strong>self-attention</strong> process allows each word to attend to every other word in a sentence, building richer representations of language.</p> <p><strong>Multi-head attention</strong> extends this process by allowing multiple attention mechanisms to operate in parallel, with each head focusing on different aspects of the context. This is especially useful for complex, nuanced tasks requiring long-range dependencies, like summarizing lengthy documents.</p> <h3 id=encoder-decoder-architectures-vs-autoregressive-models><strong>Encoder-Decoder Architectures vs. Autoregressive Models</strong><a class=headerlink href=#encoder-decoder-architectures-vs-autoregressive-models title="Permanent link">&para;</a></h3> <p>Two main architectures dominate LLM design:</p> <ul> <li><strong>Encoder-Decoder Models</strong>: Common in tasks like translation, encoder-decoder models (e.g., T5) take in a full sequence via the encoder and generate a new sequence with the decoder. This setup is useful for tasks that require transforming one sequence to another.</li> <li><strong>Autoregressive Models</strong>: Models like GPT generate text sequentially, predicting one token at a time based on previous tokens. This approach is efficient for text generation tasks, where maintaining coherence over multiple sentences is crucial.</li> </ul> <hr> <h2 id=3-components-of-llm-training><strong>3. Components of LLM Training</strong><a class=headerlink href=#3-components-of-llm-training title="Permanent link">&para;</a></h2> <h3 id=data-collection-and-preprocessing><strong>Data Collection and Preprocessing</strong><a class=headerlink href=#data-collection-and-preprocessing title="Permanent link">&para;</a></h3> <p>Training LLMs requires vast, high-quality datasets sourced from books, websites, academic papers, and other text-rich sources. Key steps include:</p> <ul> <li><strong>Data Cleaning</strong>: Removing noisy, duplicate, or low-quality data ensures the model learns relevant and accurate information.</li> <li><strong>Data Filtering</strong>: Additional steps might involve removing biased or inappropriate content, particularly in sensitive applications.</li> <li><strong>Deduplication</strong>: Redundant information can degrade model performance, so datasets are often deduplicated to prevent repeated exposure to identical content.</li> </ul> <h3 id=tokenization-vocabulary-choices-and-byte-pair-encoding><strong>Tokenization: Vocabulary Choices and Byte-Pair Encoding</strong><a class=headerlink href=#tokenization-vocabulary-choices-and-byte-pair-encoding title="Permanent link">&para;</a></h3> <p>Tokenization divides text into smaller units, or tokens, for model processing. In LLMs, <strong>subword tokenization</strong> is widely used, enabling models to handle rare or complex words by breaking them into meaningful parts. Methods like <strong>Byte-Pair Encoding (BPE)</strong> or <strong>SentencePiece</strong> balance vocabulary size with flexibility, allowing LLMs to work effectively across languages or specialized domains.</p> <h3 id=training-objectives-masked-and-causal-language-modeling><strong>Training Objectives: Masked and Causal Language Modeling</strong><a class=headerlink href=#training-objectives-masked-and-causal-language-modeling title="Permanent link">&para;</a></h3> <p>LLMs are typically trained with one of two objectives:</p> <ul> <li><strong>Masked Language Modeling (MLM)</strong>: Used in bidirectional models like BERT, MLM involves masking certain tokens and training the model to predict them based on the surrounding context. This enables a deeper understanding of the full sentence context.</li> <li><strong>Causal Language Modeling (CLM)</strong>: Autoregressive models like GPT are trained to predict the next token based only on prior tokens, making them suitable for generative tasks like text completion and storytelling.</li> </ul> <p>These objectives help the model learn to understand and generate language effectively for different types of tasks.</p> <hr> <h2 id=4-llm-fine-tuning-approaches><strong>4. LLM Fine-tuning Approaches</strong><a class=headerlink href=#4-llm-fine-tuning-approaches title="Permanent link">&para;</a></h2> <p>Fine-tuning is a critical step in adapting pre-trained models to specific tasks or domains. Rather than training a model from scratch, which is resource-intensive, fine-tuning leverages the extensive, pre-trained knowledge base of a model and adapts it for specialized needs. Here, we explore popular approaches to fine-tuning LLMs.</p> <h3 id=approaches-to-fine-tuning-llms><strong>Approaches to Fine-tuning LLMs</strong><a class=headerlink href=#approaches-to-fine-tuning-llms title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Full Model Fine-tuning</strong>: All layers and parameters are updated during the fine-tuning process. This approach is effective for highly specialized tasks but requires significant computational resources, as each layer is adjusted to the new task’s data.</p> </li> <li> <p><strong>Feature-Based Fine-tuning</strong>: Here, the model’s pre-trained layers act as feature extractors, and only the final layer (or a few layers) is fine-tuned. This approach is less resource-intensive and is useful when the downstream task is relatively similar to the original training data.</p> </li> <li> <p><strong>Parameter-Efficient Fine-tuning</strong>: Techniques like <strong>Adapter Layers</strong> and <strong>Low-Rank Adaptation (LoRA)</strong> add smaller, task-specific parameters to the model while freezing most of the original weights. This method is more efficient, as only the new parameters are updated during training, making it suitable for tasks with limited data or computational resources.</p> </li> <li> <p><strong>Prompt Tuning</strong>: Also known as <strong>prompt-based fine-tuning</strong>, this approach involves prepending specific prompts to inputs without modifying the model’s architecture or weights. The model’s responses are adapted to the task based on these engineered prompts, providing a lightweight alternative to traditional fine-tuning methods.</p> </li> </ul> <h3 id=modern-fine-tuning-approaches>Modern fine-tuning approaches<a class=headerlink href=#modern-fine-tuning-approaches title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Quantization</strong>: Reduces model size by using lower-precision formats for weights and activations, such as 8-bit or even 4-bit representations. This reduces memory footprint and can speed up inference significantly without major accuracy losses.</p> </li> <li> <p><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong>: Focuses on tuning only a small subset of parameters, leaving the majority of the model’s parameters untouched. This approach helps make fine-tuning more accessible and computationally efficient, especially for large models.</p> </li> <li> <p><strong>Low-Rank Adaptation (LoRA)</strong>: Inserts low-rank matrices into the model's architecture to adapt it without modifying the main parameters. LoRA effectively introduces additional trainable parameters that can learn task-specific features, making it highly efficient for fine-tuning.</p> </li> </ul> <h3 id=selecting-layers-to-update><strong>Selecting Layers to Update</strong><a class=headerlink href=#selecting-layers-to-update title="Permanent link">&para;</a></h3> <p>The decision of which layers to fine-tune depends on the specific task and available resources:</p> <ul> <li><strong>Early Layers</strong>: Capture lower-level patterns and linguistic features, making them effective for broad language understanding but less specific to complex tasks.</li> <li><strong>Intermediate Layers</strong>: Often capture domain-specific knowledge and nuanced patterns, providing a balance between general language understanding and specialized adaptation.</li> <li><strong>Last Layers</strong>: Contain highly task-specific information, and fine-tuning these layers can quickly adapt the model to a new task with minimal training.</li> </ul> <h3 id=weight-adjustments-during-fine-tuning><strong>Weight Adjustments During Fine-tuning</strong><a class=headerlink href=#weight-adjustments-during-fine-tuning title="Permanent link">&para;</a></h3> <p>Updating weights during fine-tuning requires a balance to prevent <strong>catastrophic forgetting</strong> (whereby the model “forgets” its pre-trained knowledge). Techniques like <strong>learning rate scheduling</strong> and <strong>gradient clipping</strong> can help maintain the model’s pre-existing strengths while learning new information.</p> <hr> <h2 id=5-training-infrastructure-for-large-language-models><strong>5. Training Infrastructure for Large Language Models</strong><a class=headerlink href=#5-training-infrastructure-for-large-language-models title="Permanent link">&para;</a></h2> <p>LLMs require extensive computational resources, often involving complex infrastructures that include GPUs, TPUs, and specialized cloud-based platforms.</p> <h3 id=distributed-computing-for-large-scale-training><strong>Distributed Computing for Large-scale Training</strong><a class=headerlink href=#distributed-computing-for-large-scale-training title="Permanent link">&para;</a></h3> <p>Large models cannot fit in the memory of a single machine. <strong>Distributed training</strong> splits the model across multiple GPUs or TPUs, allowing parallel processing of data and gradient updates across machines. Techniques like <strong>model parallelism</strong> and <strong>data parallelism</strong> help accelerate training by efficiently dividing work across systems.</p> <h3 id=specialized-hardware-gpus-and-tpus><strong>Specialized Hardware: GPUs and TPUs</strong><a class=headerlink href=#specialized-hardware-gpus-and-tpus title="Permanent link">&para;</a></h3> <p>GPUs (Graphics Processing Units) and TPUs (Tensor Processing Units) are specialized hardware for handling large matrix operations efficiently, which are common in neural network training. TPUs, designed by Google specifically for machine learning, can achieve high performance in training and inferencing tasks, especially with larger LLMs.</p> <h3 id=efficient-training-methods><strong>Efficient Training Methods</strong><a class=headerlink href=#efficient-training-methods title="Permanent link">&para;</a></h3> <p>To optimize training, methods such as <strong>mixed-precision training</strong> (which uses lower-precision floats for computations) and <strong>gradient checkpointing</strong> (saving memory by only storing essential gradient data) reduce resource consumption without sacrificing accuracy.</p> <h3 id=scalable-and-cost-effective-cloud-solutions><strong>Scalable and Cost-effective Cloud Solutions</strong><a class=headerlink href=#scalable-and-cost-effective-cloud-solutions title="Permanent link">&para;</a></h3> <p>Public cloud platforms (e.g., AWS, Google Cloud, Azure) provide scalable solutions to train LLMs cost-effectively. Cloud-based solutions offer flexibility to scale resources up or down, making them suitable for organizations of all sizes and facilitating collaboration among distributed teams.</p> <hr> <h2 id=6-evaluation-and-benchmarking><strong>6. Evaluation and Benchmarking</strong><a class=headerlink href=#6-evaluation-and-benchmarking title="Permanent link">&para;</a></h2> <p>Benchmarks provide standard measures for evaluating model performance on various tasks, guiding model development and helping compare different LLMs on a common scale.</p> <h3 id=what-are-benchmarks-in-ai><strong>What Are Benchmarks in AI?</strong><a class=headerlink href=#what-are-benchmarks-in-ai title="Permanent link">&para;</a></h3> <p>Benchmarks are curated datasets and tasks used to measure a model’s abilities across specific challenges (e.g., reasoning, knowledge retrieval, translation). They ensure models meet the quality and capability standards required for real-world deployment.</p> <h3 id=key-components-of-ai-benchmarks><strong>Key Components of AI Benchmarks</strong><a class=headerlink href=#key-components-of-ai-benchmarks title="Permanent link">&para;</a></h3> <p>A robust benchmark typically includes:</p> <ul> <li><strong>Task Diversity</strong>: Benchmarks assess multiple aspects of language understanding (e.g., comprehension, logical reasoning, multilingual abilities).</li> <li><strong>Evaluation Metrics</strong>: Metrics like accuracy, F1-score, BLEU, and ROUGE evaluate model responses and performance across tasks.</li> <li><strong>Data Quality and Coverage</strong>: The benchmark dataset should have high-quality, representative data across different domains to avoid bias and ensure generalizability.</li> </ul> <h3 id=importance-of-benchmarks><strong>Importance of Benchmarks</strong><a class=headerlink href=#importance-of-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks provide a structured approach to evaluating progress in LLM development, ensuring models improve in areas of practical importance, such as accuracy, speed, and robustness. They help users select models suited to their needs and guide model development in industry and academia.</p> <h3 id=example-benchmarks-in-practice><strong>Example Benchmarks in Practice</strong><a class=headerlink href=#example-benchmarks-in-practice title="Permanent link">&para;</a></h3> <p>Popular LLM benchmarks include:</p> <ul> <li><strong>GLUE and SuperGLUE</strong>: Evaluate models on linguistic and logical reasoning tasks.</li> <li><strong>MMLU (Massive Multitask Language Understanding)</strong>: Tests model performance across a diverse range of topics (e.g., humanities, STEM), often in a few-shot setting.</li> <li><strong>Big-Bench (BBH)</strong>: A collaborative benchmark project assessing model performance on creative and high-level reasoning tasks across multiple domains.</li> <li><strong>WinoGrande</strong>: Designed to evaluate commonsense reasoning in a more challenging setup, requiring nuanced understanding of context to make accurate predictions.</li> </ul> <hr> <h2 id=7-popular-llm-benchmarks-and-n-shot-learning><strong>7. Popular LLM Benchmarks and n-shot Learning</strong><a class=headerlink href=#7-popular-llm-benchmarks-and-n-shot-learning title="Permanent link">&para;</a></h2> <p>n-shot learning assesses how well a model adapts to new tasks with limited training data, typically in settings like zero-shot, one-shot, and few-shot learning.</p> <h3 id=what-is-n-shot-learning><strong>What Is n-shot Learning?</strong><a class=headerlink href=#what-is-n-shot-learning title="Permanent link">&para;</a></h3> <p>n-shot learning is an evaluation paradigm in which models are provided with <strong>n</strong> examples to help them understand the target task:</p> <ul> <li><strong>Zero-shot learning</strong>: The model receives no examples and must perform based on its pre-trained knowledge alone.</li> <li><strong>One-shot learning</strong>: The model is given one example to generalize from.</li> <li><strong>Few-shot learning</strong>: The model is given a small number of examples (usually between 2-10) to help it adapt to a task.</li> </ul> <h3 id=applications-of-n-shot-learning-in-llm-benchmarks><strong>Applications of n-shot Learning in LLM Benchmarks</strong><a class=headerlink href=#applications-of-n-shot-learning-in-llm-benchmarks title="Permanent link">&para;</a></h3> <p>n-shot settings, particularly <strong>few-shot</strong>, are commonly used in LLM benchmarks like MMLU and Big-Bench. These benchmarks assess a model’s flexibility and its ability to generalize with minimal information, which is crucial for applications where annotated data is scarce.</p> <hr> <h3 id=8-llm-fine-tuning-approaches>8. <strong>LLM Fine-tuning Approaches</strong><a class=headerlink href=#8-llm-fine-tuning-approaches title="Permanent link">&para;</a></h3> <p>Fine-tuning large language models (LLMs) involves adjusting specific model parameters to tailor the LLM to a particular task or domain. The goal is to refine the model without the heavy computational cost of training all parameters from scratch. Here are some of the key fine-tuning techniques:</p> <ul> <li> <p><strong>Standard Fine-Tuning</strong>: This approach involves updating all or most model parameters using labeled data relevant to the target task. While highly effective, it requires significant computational resources, especially for large models.</p> </li> <li> <p><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong>: This technique updates only a subset of parameters, reducing memory and processing needs. PEFT is particularly advantageous in low-resource or low-power environments, where tuning a full LLM isn’t feasible.</p> </li> <li> <p><strong>Quantization</strong>: Quantization compresses model weights to lower-precision formats, such as 8-bit or even 4-bit, to reduce memory usage and improve inference speed. While initially popular for inference, quantization is now also being used in fine-tuning, making it possible to train large models on less powerful hardware.</p> </li> <li> <p><strong>Low-Rank Adaptation (LoRA)</strong>: LoRA is a method that inserts additional low-rank matrices into transformer layers. This allows the model to learn task-specific changes while leaving the original model parameters mostly untouched. LoRA is particularly useful in transfer learning, as it maintains model integrity while achieving effective fine-tuning.</p> </li> <li> <p><strong>Adapters</strong>: Adapters are lightweight layers added to each layer of the transformer. By training only the adapter layers and keeping the core model frozen, we save time and computational resources. This method is highly modular, allowing a single model to be adapted to various tasks by simply switching adapters.</p> </li> </ul> <hr> <h3 id=9-evaluation-metrics-for-llms>9. <strong>Evaluation Metrics for LLMs</strong><a class=headerlink href=#9-evaluation-metrics-for-llms title="Permanent link">&para;</a></h3> <p>Evaluating large language models (LLMs) accurately is crucial to ensure they perform well across diverse use cases. The right metrics offer insight into the model’s accuracy, coherence, relevance, and ethical implications. Here are some key metrics used to evaluate LLMs:</p> <ul> <li> <p><strong>Perplexity</strong>: Measures how well the model predicts a sample of text. Lower perplexity indicates better model performance, showing that the model generates fluent, plausible sequences.</p> </li> <li> <p><strong>BLEU (Bilingual Evaluation Understudy)</strong> and <strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong>: Commonly used in text generation and summarization, these metrics compare generated text with human references to determine closeness in terms of wording and structure.</p> </li> <li> <p><strong>Accuracy &amp; F1 Score</strong>: For tasks like classification and QA, accuracy and F1 scores measure the model’s precision and recall. These are often employed when the model output has a right/wrong answer, such as fact-based questions.</p> </li> <li> <p><strong>Human Evaluation</strong>: Human assessments are vital for subjective attributes like coherence, appropriateness, and sentiment. Evaluators rate model outputs based on these qualitative aspects, often forming a critical component of LLM evaluation.</p> </li> <li> <p><strong>Ethical and Bias Metrics</strong>: These metrics evaluate the model’s tendency to reinforce harmful stereotypes, generate offensive content, or exhibit undesirable biases. Fairness metrics like demographic parity and bias amplification are used to assess the ethical implications of a model’s outputs.</p> </li> </ul> <hr> <h3 id=10-challenges-and-limitations-in-llm-training>10. <strong>Challenges and Limitations in LLM Training</strong><a class=headerlink href=#10-challenges-and-limitations-in-llm-training title="Permanent link">&para;</a></h3> <p>Despite their transformative potential, LLMs come with several challenges and limitations. Here are some of the most significant:</p> <ul> <li> <p><strong>Computational Costs</strong>: Training LLMs requires significant computational resources, particularly when dealing with very large models or massive datasets. The hardware, energy, and storage demands can be prohibitive, often limiting LLM training to organizations with substantial resources.</p> </li> <li> <p><strong>Data Privacy and Security</strong>: Training on massive, diverse datasets raises concerns about privacy and security. Models can unintentionally memorize sensitive information from training data, leading to potential privacy violations. Ensuring data integrity and anonymization in training data is critical.</p> </li> <li> <p><strong>Bias and Fairness</strong>: LLMs trained on unfiltered internet data often reflect societal biases, as they learn from a vast array of human-generated content. Addressing bias requires careful curation of training datasets, post-processing techniques, and ongoing monitoring of outputs.</p> </li> <li> <p><strong>Interpretability and Explainability</strong>: As models become larger and more complex, interpreting their predictions and explaining decision-making becomes increasingly difficult. Explainability is especially crucial for applications in high-stakes fields like healthcare, law, and finance.</p> </li> <li> <p><strong>Generalization vs. Overfitting</strong>: Striking the right balance between generalization (performing well on diverse inputs) and overfitting (memorizing training data) is difficult with LLMs. While more data can reduce overfitting, it requires careful validation to ensure the model doesn’t simply “remember” data.</p> </li> </ul> <div class=author-bio style="margin-top: 2rem; display: flex; gap: 1rem;"> <img src=../assets/images/myphotos/Profilephoto1.jpg alt="Hari Thapliyaal" style="border-radius: 50%; width: 80px; height: 80px;"> <div> <strong>Dr. Hari Thapliyaal</strong><br> <small>Dr. Hari Thapliyal is a prolific blogger and seasoned professional with an extensive background in Data Science, Project Management, and Advait-Vedanta Philosophy. He holds a Doctorate in AI/NLP from SSBM, Geneva, along with Master’s degrees in Computers, Business Management, Data Science, and Economics. With over three decades of experience in management and leadership, Hari has extensive expertise in training, consulting, and coaching within the technology sector. His specializations include Data Science, AI, Computer Vision, NLP, and machine learning. Hari is also passionate about meditation and nature, often retreating to secluded places for reflection and peace.</small> </div> </div> <div class=share-buttons style="margin-top: 2rem;"> <strong>Share this article:</strong><br> <a href="https://twitter.com/intent/tweet?text=LLM%20Architecture%20and%20Training&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/LLM-Architecture-and-Training.html" target=_blank>Twitter</a> | <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/LLM-Architecture-and-Training.html" target=_blank>Facebook</a> | <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/LLM-Architecture-and-Training.html" target=_blank>LinkedIn</a> </div> <div id=comments style="margin-top: 3rem;"> <script src=https://giscus.app/client.js data-repo=dasarpai/dasarpai-comments data-repo-id=R_kgDOOGVFpA data-category=General data-category-id=DIC_kwDOOGVFpM4CnzHR data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async>
        </script> </div> </article> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.js></script> </body> </html>