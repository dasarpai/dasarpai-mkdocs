<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Exploring-Dense-Embedding-Models-in-AI.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Exploring Dense Embedding Models in AI - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#what-is-dense-embedding-in-ai class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Exploring Dense Embedding Models in AI </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-dense-embedding-in-ai class=md-nav__link> <span class=md-ellipsis> What is dense embedding in AI? </span> </a> <nav class=md-nav aria-label="What is dense embedding in AI?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#examples class=md-nav__link> <span class=md-ellipsis> Examples: </span> </a> </li> <li class=md-nav__item> <a href=#advantages class=md-nav__link> <span class=md-ellipsis> Advantages: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-different-dense-embedding-models class=md-nav__link> <span class=md-ellipsis> What are different dense embedding models ? </span> </a> </li> <li class=md-nav__item> <a href=#wha-are-word-embedding-models-nlp class=md-nav__link> <span class=md-ellipsis> Wha are Word Embedding Models (NLP) </span> </a> <nav class=md-nav aria-label="Wha are Word Embedding Models (NLP)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#word2vec class=md-nav__link> <span class=md-ellipsis> Word2Vec </span> </a> </li> <li class=md-nav__item> <a href=#glove-global-vectors-for-word-representation class=md-nav__link> <span class=md-ellipsis> GloVe (Global Vectors for Word Representation) </span> </a> </li> <li class=md-nav__item> <a href=#fasttext class=md-nav__link> <span class=md-ellipsis> FastText </span> </a> </li> <li class=md-nav__item> <a href=#elmo-embeddings-from-language-models class=md-nav__link> <span class=md-ellipsis> ELMo (Embeddings from Language Models) </span> </a> </li> <li class=md-nav__item> <a href=#bert-bidirectional-encoder-representations-from-transformers class=md-nav__link> <span class=md-ellipsis> BERT (Bidirectional Encoder Representations from Transformers) </span> </a> </li> <li class=md-nav__item> <a href=#gpt-generative-pre-trained-transformer class=md-nav__link> <span class=md-ellipsis> GPT (Generative Pre-trained Transformer) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-sentence-embedding-models-nlp class=md-nav__link> <span class=md-ellipsis> What are Sentence Embedding Models (NLP) </span> </a> <nav class=md-nav aria-label="What are Sentence Embedding Models (NLP)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#infersent class=md-nav__link> <span class=md-ellipsis> InferSent </span> </a> </li> <li class=md-nav__item> <a href=#universal-sentence-encoder-use class=md-nav__link> <span class=md-ellipsis> Universal Sentence Encoder (USE) </span> </a> </li> <li class=md-nav__item> <a href=#sbert-sentence-bert class=md-nav__link> <span class=md-ellipsis> SBERT (Sentence-BERT) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-image-embedding-models-computer-vision class=md-nav__link> <span class=md-ellipsis> What are Image Embedding Models (Computer Vision) </span> </a> <nav class=md-nav aria-label="What are Image Embedding Models (Computer Vision)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#convolutional-neural-networks-cnns class=md-nav__link> <span class=md-ellipsis> Convolutional Neural Networks (CNNs) </span> </a> </li> <li class=md-nav__item> <a href=#clip-contrastive-language-image-pre-training class=md-nav__link> <span class=md-ellipsis> CLIP (Contrastive Language-Image Pre-training) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-graph-embedding-models-graph-data class=md-nav__link> <span class=md-ellipsis> What are Graph Embedding Models (Graph Data) </span> </a> <nav class=md-nav aria-label="What are Graph Embedding Models (Graph Data)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#node2vec class=md-nav__link> <span class=md-ellipsis> Node2Vec </span> </a> </li> <li class=md-nav__item> <a href=#graphsage class=md-nav__link> <span class=md-ellipsis> GraphSAGE </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-multimodal-embedding-models class=md-nav__link> <span class=md-ellipsis> What are Multimodal Embedding Models </span> </a> <nav class=md-nav aria-label="What are Multimodal Embedding Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#visualbert class=md-nav__link> <span class=md-ellipsis> VisualBERT </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#advanced-embedding-models class=md-nav__link> <span class=md-ellipsis> Advanced Embedding Models </span> </a> <nav class=md-nav aria-label="Advanced Embedding Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#voyage class=md-nav__link> <span class=md-ellipsis> Voyage </span> </a> </li> <li class=md-nav__item> <a href=#laser-language-agnostic-sentence-representations class=md-nav__link> <span class=md-ellipsis> LASER (Language-Agnostic SEntence Representations) </span> </a> </li> <li class=md-nav__item> <a href=#dalle-embeddings class=md-nav__link> <span class=md-ellipsis> DALL·E Embeddings </span> </a> </li> <li class=md-nav__item> <a href=#align-vision-language-pre-training class=md-nav__link> <span class=md-ellipsis> ALIGN (Vision-Language Pre-training) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#how-to-generate-embeddings-of-my-documents class=md-nav__link> <span class=md-ellipsis> How to generate embeddings of my documents? </span> </a> <nav class=md-nav aria-label="How to generate embeddings of my documents?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-pre-trained-embedding-models-and-libraries class=md-nav__link> <span class=md-ellipsis> 1. Pre-trained Embedding Models and Libraries </span> </a> <nav class=md-nav aria-label="1. Pre-trained Embedding Models and Libraries"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-sentence-transformers-sbert class=md-nav__link> <span class=md-ellipsis> 1.1. Sentence-Transformers (SBERT) </span> </a> </li> <li class=md-nav__item> <a href=#12-hugging-face-transformers class=md-nav__link> <span class=md-ellipsis> 1.2. Hugging Face Transformers </span> </a> </li> <li class=md-nav__item> <a href=#13-universal-sentence-encoder-use class=md-nav__link> <span class=md-ellipsis> 1.3. Universal Sentence Encoder (USE) </span> </a> </li> <li class=md-nav__item> <a href=#14-fasttext class=md-nav__link> <span class=md-ellipsis> 1.4. FastText </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-cloud-apis-for-generating-embeddings class=md-nav__link> <span class=md-ellipsis> 2. Cloud APIs for Generating Embeddings </span> </a> <nav class=md-nav aria-label="2. Cloud APIs for Generating Embeddings"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-openais-embedding-api class=md-nav__link> <span class=md-ellipsis> 2.1. OpenAI’s Embedding API </span> </a> </li> <li class=md-nav__item> <a href=#22-azure-cognitive-search class=md-nav__link> <span class=md-ellipsis> 2.2. Azure Cognitive Search </span> </a> </li> <li class=md-nav__item> <a href=#23-google-clouds-vertex-ai-and-ai-hub class=md-nav__link> <span class=md-ellipsis> 2.3. Google Cloud’s Vertex AI and AI Hub </span> </a> </li> <li class=md-nav__item> <a href=#24-openais-embedding-api class=md-nav__link> <span class=md-ellipsis> 2.4. OpenAI’s Embedding API </span> </a> </li> <li class=md-nav__item> <a href=#25-voyage-embeddings class=md-nav__link> <span class=md-ellipsis> 2.5. Voyage Embeddings </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3-custom-training-on-your-own-data class=md-nav__link> <span class=md-ellipsis> 3. Custom Training on Your Own Data </span> </a> <nav class=md-nav aria-label="3. Custom Training on Your Own Data"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31-doc2vec-gensim class=md-nav__link> <span class=md-ellipsis> 3.1. Doc2Vec (Gensim) </span> </a> </li> <li class=md-nav__item> <a href=#32-fine-tuning-bert-or-other-transformers class=md-nav__link> <span class=md-ellipsis> 3.2. Fine-Tuning BERT or Other Transformers </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-online-platforms-for-document-embedding class=md-nav__link> <span class=md-ellipsis> 4. Online Platforms for Document Embedding </span> </a> <nav class=md-nav aria-label="4. Online Platforms for Document Embedding"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#41-pinecone class=md-nav__link> <span class=md-ellipsis> 4.1. Pinecone </span> </a> </li> <li class=md-nav__item> <a href=#42-weaviate class=md-nav__link> <span class=md-ellipsis> 4.2. Weaviate </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#hashtags class=md-nav__link> <span class=md-ellipsis> Hashtags </span> </a> </li> <li class=md-nav__item> <a href=#denseembeddings class=md-nav__link> <span class=md-ellipsis> DenseEmbeddings </span> </a> </li> <li class=md-nav__item> <a href=#machinelearning class=md-nav__link> <span class=md-ellipsis> MachineLearning </span> </a> </li> <li class=md-nav__item> <a href=#nlp class=md-nav__link> <span class=md-ellipsis> NLP </span> </a> </li> <li class=md-nav__item> <a href=#wordembeddings class=md-nav__link> <span class=md-ellipsis> WordEmbeddings </span> </a> </li> <li class=md-nav__item> <a href=#voyagemodel class=md-nav__link> <span class=md-ellipsis> VoyageModel </span> </a> </li> <li class=md-nav__item> <a href=#multimodalai class=md-nav__link> <span class=md-ellipsis> MultimodalAI </span> </a> </li> <li class=md-nav__item> <a href=#textembeddings class=md-nav__link> <span class=md-ellipsis> TextEmbeddings </span> </a> </li> <li class=md-nav__item> <a href=#deeplearning class=md-nav__link> <span class=md-ellipsis> DeepLearning </span> </a> </li> <li class=md-nav__item> <a href=#transformers class=md-nav__link> <span class=md-ellipsis> Transformers </span> </a> </li> <li class=md-nav__item> <a href=#contextualembeddings class=md-nav__link> <span class=md-ellipsis> ContextualEmbeddings </span> </a> </li> <li class=md-nav__item> <a href=#crossmodalai class=md-nav__link> <span class=md-ellipsis> CrossModalAI </span> </a> </li> <li class=md-nav__item> <a href=#semanticsearch class=md-nav__link> <span class=md-ellipsis> SemanticSearch </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/Exploring-Dense-Embedding-Models-in-AI.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/Exploring-Dense-Embedding-Models-in-AI.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="Exploring Dense Embedding Models in AI" src=../assets/images/dspost/dsp6157-Exploring-Dense-Embedding-Models-in-AI.jpg></p> <h2 id=what-is-dense-embedding-in-ai>What is dense embedding in AI?<a class=headerlink href=#what-is-dense-embedding-in-ai title="Permanent link">&para;</a></h2> <p>Dense embeddings are critical in many AI applications, particularly in deep learning, where they help reduce data complexity and enhance the model’s ability to generalize from patterns in data.</p> <p>In artificial intelligence (AI), <strong>dense embedding</strong> refers to a method of representing data (like words, sentences, images, or other inputs) as dense vectors in a continuous, lower-dimensional (lessor number of dimensions) space. These vectors, known as <strong>embeddings</strong>, encode semantic information, enabling AI models to work with data in a more meaningful way.</p> <ol> <li> <p><strong>Dense</strong>: Unlike sparse vectors (which have a lot of zeros), dense vectors contain mostly non-zero values. A sparse vector might have many dimensions with mostly zero values, while a dense vector has fewer dimensions and mostly non-zero values.</p> </li> <li> <p><strong>Embedding</strong>: An embedding maps data from its original, often high-dimensional representation (e.g., a one-hot encoded word vector) into a lower-dimensional space. In this space, similar inputs (like semantically similar words or images with similar features) will have similar vector representations.</p> </li> </ol> <h3 id=examples>Examples:<a class=headerlink href=#examples title="Permanent link">&para;</a></h3> <ul> <li> <p><strong>Word Embeddings</strong>: In Natural Language Processing (NLP), dense embeddings like <strong>Word2Vec</strong>, <strong>GloVe</strong>, or <strong>BERT</strong> represent words as vectors where words with similar meanings are located closer to each other in the vector space. For instance, the words "king" and "queen" would have embeddings that are close together.</p> </li> <li> <p><strong>Image Embeddings</strong>: In computer vision, embeddings generated by neural networks (e.g., from convolutional layers) encode visual features. For instance, images of cats will have similar embeddings, distinct from embeddings of dogs.</p> </li> </ul> <h3 id=advantages>Advantages:<a class=headerlink href=#advantages title="Permanent link">&para;</a></h3> <ul> <li><strong>Efficient Representation</strong>: Dense embeddings capture the most relevant information while reducing the dimensionality, making computations more efficient.</li> <li><strong>Semantic Meaning</strong>: In NLP, dense embeddings can capture relationships between words, such as analogy relationships (e.g., the vector difference between "king" and "queen" is similar to "man" and "woman").</li> </ul> <h2 id=what-are-different-dense-embedding-models>What are different dense embedding models ?<a class=headerlink href=#what-are-different-dense-embedding-models title="Permanent link">&para;</a></h2> <p>There are several popular dense embedding models used across different domains in AI, especially in Natural Language Processing (NLP) and computer vision. Here are the key types of dense embedding models. Embedding models are used to represent words, sentences, images, or other data in a continuous, lower-dimensional space.</p> <h2 id=wha-are-word-embedding-models-nlp>Wha are <strong>Word Embedding Models (NLP)</strong><a class=headerlink href=#wha-are-word-embedding-models-nlp title="Permanent link">&para;</a></h2> <h3 id=word2vec><strong>Word2Vec</strong><a class=headerlink href=#word2vec title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Word2Vec is one of the earliest dense embedding models. It transforms words into vectors using either the <strong>Continuous Bag of Words (CBOW)</strong> or <strong>Skip-gram</strong> approach.</li> <li><strong>How It Works</strong>: It learns word associations by predicting surrounding words given a target word (CBOW) or predicting a target word given its surrounding context (Skip-gram).</li> <li><strong>Key Feature</strong>: Captures semantic similarity between words.</li> <li><strong>Applications</strong>: NLP tasks such as text classification, sentiment analysis, and document retrieval.</li> </ul> <h3 id=glove-global-vectors-for-word-representation><strong>GloVe (Global Vectors for Word Representation)</strong><a class=headerlink href=#glove-global-vectors-for-word-representation title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: GloVe is a global log-bilinear regression model that creates dense word embeddings by factoring in global co-occurrence statistics across the entire corpus.</li> <li><strong>How It Works</strong>: It uses matrix factorization on word co-occurrence matrices to create dense embeddings.</li> <li><strong>Key Feature</strong>: Combines the benefits of both Word2Vec’s local context approach and global matrix factorization techniques.</li> <li><strong>Applications</strong>: NLP tasks that require a fixed set of pre-trained word embeddings.</li> </ul> <h3 id=fasttext><strong>FastText</strong><a class=headerlink href=#fasttext title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: An extension of Word2Vec developed by Facebook AI Research. FastText generates embeddings for subwords, rather than just words, which helps in representing rare or out-of-vocabulary words.</li> <li><strong>How It Works</strong>: Breaks words into n-grams (subwords) and learns embeddings for these subword units.</li> <li><strong>Key Feature</strong>: Handles morphological variations and rare words better than Word2Vec.</li> <li><strong>Applications</strong>: NLP tasks in languages with rich morphology or where rare words are a concern.</li> </ul> <h3 id=elmo-embeddings-from-language-models><strong>ELMo (Embeddings from Language Models)</strong><a class=headerlink href=#elmo-embeddings-from-language-models title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: ELMo generates context-sensitive word embeddings, meaning that the embedding for a word changes based on the surrounding context.</li> <li><strong>How It Works</strong>: It uses deep bidirectional language models (bi-LSTM) to generate dynamic embeddings.</li> <li><strong>Key Feature</strong>: Contextual embeddings for words, as opposed to static embeddings in Word2Vec and GloVe.</li> <li><strong>Applications</strong>: Question answering, named entity recognition, and machine translation.</li> </ul> <h3 id=bert-bidirectional-encoder-representations-from-transformers><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong><a class=headerlink href=#bert-bidirectional-encoder-representations-from-transformers title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: BERT generates embeddings using a transformer-based architecture that understands both the left and right context of a word.</li> <li><strong>How It Works</strong>: Pre-trains on masked language modeling (MLM) and next sentence prediction (NSP) tasks. Outputs contextual embeddings for words based on the entire sentence.</li> <li><strong>Key Feature</strong>: Fully bidirectional, capturing deep context in sentences.</li> <li><strong>Applications</strong>: Text classification, sentiment analysis, question answering, and more complex NLP tasks.</li> </ul> <h3 id=gpt-generative-pre-trained-transformer><strong>GPT (Generative Pre-trained Transformer)</strong><a class=headerlink href=#gpt-generative-pre-trained-transformer title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: GPT models generate dense embeddings using transformer decoders, trained to predict the next word in a sequence. Though primarily used for generation tasks, embeddings can be extracted from intermediate layers.</li> <li><strong>How It Works</strong>: GPT is trained in a unidirectional (left-to-right) fashion, generating embeddings based on past context.</li> <li><strong>Key Feature</strong>: Contextual embeddings, but with more emphasis on generative tasks.</li> <li><strong>Applications</strong>: Text generation, chatbots, and language modeling.</li> </ul> <h2 id=what-are-sentence-embedding-models-nlp>What are <strong>Sentence Embedding Models (NLP)</strong><a class=headerlink href=#what-are-sentence-embedding-models-nlp title="Permanent link">&para;</a></h2> <h3 id=infersent><strong>InferSent</strong><a class=headerlink href=#infersent title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: A supervised sentence embedding model trained for natural language inference tasks.</li> <li><strong>How It Works</strong>: Uses a bi-directional LSTM network with max-pooling to generate sentence-level embeddings.</li> <li><strong>Key Feature</strong>: Produces meaningful embeddings for entire sentences, not just words.</li> <li><strong>Applications</strong>: Sentiment analysis, text similarity, and entailment detection.</li> </ul> <h3 id=universal-sentence-encoder-use><strong>Universal Sentence Encoder (USE)</strong><a class=headerlink href=#universal-sentence-encoder-use title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Developed by Google, USE generates sentence embeddings using a transformer-based architecture and can be used for various downstream NLP tasks.</li> <li><strong>How It Works</strong>: Pre-trained on a wide variety of tasks like conversational responses and document classification.</li> <li><strong>Key Feature</strong>: Provides fixed-length sentence embeddings and supports a variety of languages.</li> <li><strong>Applications</strong>: Semantic search, question answering, and information retrieval.</li> </ul> <h3 id=sbert-sentence-bert><strong>SBERT (Sentence-BERT)</strong><a class=headerlink href=#sbert-sentence-bert title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: A variation of BERT, designed to generate embeddings for sentences by fine-tuning BERT for sentence-pair tasks.</li> <li><strong>How It Works</strong>: Fine-tunes BERT for tasks like semantic textual similarity, using Siamese networks to generate sentence embeddings efficiently.</li> <li><strong>Key Feature</strong>: Faster and more effective for sentence similarity tasks compared to BERT.</li> <li><strong>Applications</strong>: Sentence similarity, paraphrase detection, and semantic search.</li> </ul> <h2 id=what-are-image-embedding-models-computer-vision>What are <strong>Image Embedding Models (Computer Vision)</strong><a class=headerlink href=#what-are-image-embedding-models-computer-vision title="Permanent link">&para;</a></h2> <h3 id=convolutional-neural-networks-cnns><strong>Convolutional Neural Networks (CNNs)</strong><a class=headerlink href=#convolutional-neural-networks-cnns title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: CNNs are the most common models for generating dense image embeddings. Pre-trained models like <strong>ResNet</strong>, <strong>VGG</strong>, and <strong>Inception</strong> can be used to extract embeddings from intermediate layers.</li> <li><strong>How It Works</strong>: CNNs extract features from images and compress them into dense embeddings.</li> <li><strong>Key Feature</strong>: Captures spatial hierarchies and features in images (e.g., edges, textures).</li> <li><strong>Applications</strong>: Image classification, object detection, and image retrieval.</li> </ul> <h3 id=clip-contrastive-language-image-pre-training><strong>CLIP (Contrastive Language-Image Pre-training)</strong><a class=headerlink href=#clip-contrastive-language-image-pre-training title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Developed by OpenAI, CLIP aligns text and image embeddings in a shared latent space, enabling zero-shot transfer to new vision tasks.</li> <li><strong>How It Works</strong>: Trains on a large dataset of text-image pairs, using contrastive learning to bring corresponding text and image embeddings closer.</li> <li><strong>Key Feature</strong>: Unifies text and image embeddings, allowing for flexible multimodal tasks.</li> <li><strong>Applications</strong>: Image classification, image captioning, and visual search.</li> </ul> <h2 id=what-are-graph-embedding-models-graph-data>What are <strong>Graph Embedding Models (Graph Data)</strong><a class=headerlink href=#what-are-graph-embedding-models-graph-data title="Permanent link">&para;</a></h2> <h3 id=node2vec><strong>Node2Vec</strong><a class=headerlink href=#node2vec title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Node2Vec generates dense embeddings for nodes in a graph by using random walks to sample node sequences.</li> <li><strong>How It Works</strong>: Performs biased random walks on the graph to create a sequence of nodes, and then applies the Word2Vec algorithm to learn node embeddings.</li> <li><strong>Key Feature</strong>: Captures both local and global graph structures.</li> <li><strong>Applications</strong>: Social network analysis, recommendation systems, and fraud detection.</li> </ul> <h3 id=graphsage><strong>GraphSAGE</strong><a class=headerlink href=#graphsage title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: GraphSAGE is an inductive framework that generates node embeddings by sampling and aggregating features from a node’s local neighborhood.</li> <li><strong>How It Works</strong>: Uses a graph-based convolutional network to generate node embeddings in an inductive way, meaning it can generalize to unseen nodes.</li> <li><strong>Key Feature</strong>: Handles dynamic and evolving graphs.</li> <li><strong>Applications</strong>: Graph classification, node classification, and link prediction.</li> </ul> <h2 id=what-are-multimodal-embedding-models>What are <strong>Multimodal Embedding Models</strong><a class=headerlink href=#what-are-multimodal-embedding-models title="Permanent link">&para;</a></h2> <h3 id=visualbert><strong>VisualBERT</strong><a class=headerlink href=#visualbert title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: VisualBERT generates joint text and image embeddings by fusing information from both modalities using a BERT-like architecture.</li> <li><strong>How It Works</strong>: Combines pre-trained BERT with image region embeddings (from object detectors) to create joint representations of text and image.</li> <li><strong>Key Feature</strong>: Multimodal understanding of language and vision.</li> <li><strong>Applications</strong>: Visual question answering, image captioning, and visual grounding.</li> </ul> <h2 id=advanced-embedding-models><strong>Advanced Embedding Models</strong><a class=headerlink href=#advanced-embedding-models title="Permanent link">&para;</a></h2> <h3 id=voyage><strong>Voyage</strong><a class=headerlink href=#voyage title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Voyage is a more recent model designed for <strong>universal cross-modal</strong> dense embeddings, focusing on aligning data from various modalities like text, images, audio, and even video into a shared embedding space.</li> <li><strong>How It Works</strong>: Uses a combination of deep contrastive learning and transformer-based architectures to encode inputs from different data types. Voyage models capture contextual relationships across modalities.</li> <li><strong>Key Feature</strong>: Cross-modal understanding (e.g., relating text to images or audio), making it highly effective in multimodal tasks.</li> <li><strong>Applications</strong>: Used in multimodal search engines, recommendation systems, and applications that require the fusion of information from multiple modalities (like augmented reality, voice assistants, or multimedia content search).</li> </ul> <h3 id=laser-language-agnostic-sentence-representations><strong>LASER (Language-Agnostic SEntence Representations)</strong><a class=headerlink href=#laser-language-agnostic-sentence-representations title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Developed by Facebook AI, LASER generates dense sentence embeddings that are <strong>language-agnostic</strong>, meaning the model works across multiple languages.</li> <li><strong>How It Works</strong>: Trains on large multilingual datasets using bi-directional LSTM encoders to produce embeddings that are useful across different languages.</li> <li><strong>Key Feature</strong>: Enables cross-lingual transfer for tasks such as machine translation and multilingual text classification.</li> <li><strong>Applications</strong>: Multilingual sentence similarity, document alignment, and translation.</li> </ul> <h3 id=dalle-embeddings><strong>DALL·E Embeddings</strong><a class=headerlink href=#dalle-embeddings title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: While DALL·E is primarily a generative model for creating images from textual descriptions, the <strong>embeddings</strong> learned in its model can be used for dense, cross-modal text-image relationships.</li> <li><strong>How It Works</strong>: Uses a transformer-based architecture to learn joint embeddings between images and text.</li> <li><strong>Key Feature</strong>: Dense embeddings that capture the semantics of text and images in a shared space.</li> <li><strong>Applications</strong>: Image generation, image captioning, and multimodal search.</li> </ul> <h3 id=align-vision-language-pre-training><strong>ALIGN (Vision-Language Pre-training)</strong><a class=headerlink href=#align-vision-language-pre-training title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: ALIGN, like CLIP, is designed to align visual and textual information into a single embedding space using large-scale training.</li> <li><strong>How It Works</strong>: Pre-trains a vision model and a text model jointly using contrastive learning to associate image-text pairs.</li> <li><strong>Key Feature</strong>: Generalizes well to unseen tasks and datasets, making it a powerful model for multimodal applications.</li> <li><strong>Applications</strong>: Image classification, text-based image retrieval, and multimodal content understanding.</li> </ul> <p>Advance models like <strong>Voyage</strong> and <strong>CLIP</strong> are vital for bridging the gap between different modalities, which is crucial in real-world applications where data comes from multiple sources—text, images, audio, and beyond. These models allow for a <strong>unified understanding</strong> of diverse types of information, enabling AI systems to perform tasks like semantic search, recommendation, and even interaction between different sensory inputs (e.g., voice-controlled devices understanding visual context).</p> <h2 id=how-to-generate-embeddings-of-my-documents>How to generate embeddings of my documents?<a class=headerlink href=#how-to-generate-embeddings-of-my-documents title="Permanent link">&para;</a></h2> <p>To generate embeddings of your documents, you can use a variety of tools and libraries that support document embedding. These resources typically provide pre-trained models and APIs to convert text into embeddings for downstream tasks like document similarity, search, or classification.</p> <h3 id=1-pre-trained-embedding-models-and-libraries><strong>1. Pre-trained Embedding Models and Libraries</strong><a class=headerlink href=#1-pre-trained-embedding-models-and-libraries title="Permanent link">&para;</a></h3> <h4 id=11-sentence-transformers-sbert><strong>1.1. Sentence-Transformers (SBERT)</strong><a class=headerlink href=#11-sentence-transformers-sbert title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: A Python library that provides easy-to-use interfaces for generating embeddings for sentences, paragraphs, and documents. It is built on top of BERT and other transformer models.</li> <li><strong>How to Use</strong>: <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>from</span><span class=w> </span><span class=nn>sentence_transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>SentenceTransformer</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=n>model</span> <span class=o>=</span> <span class=n>SentenceTransformer</span><span class=p>(</span><span class=s1>&#39;all-MiniLM-L6-v2&#39;</span><span class=p>)</span>  <span class=c1># Load a pre-trained model</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=n>documents</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;Document 1 text here&quot;</span><span class=p>,</span> <span class=s2>&quot;Document 2 text here&quot;</span><span class=p>]</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=n>embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span>
</span></code></pre></div></li> <li><strong>Resources</strong>:</li> <li><a href=https://www.sbert.net/ >Sentence-Transformers Documentation</a></li> <li><a href=https://www.sbert.net/docs/pretrained_models.html>Pre-trained Models</a></li> </ul> <h4 id=12-hugging-face-transformers><strong>1.2. Hugging Face Transformers</strong><a class=headerlink href=#12-hugging-face-transformers title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: Hugging Face’s <code>transformers</code> library provides a wide range of pre-trained models for generating dense embeddings from text documents.</li> <li><strong>How to Use</strong>: <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>from</span><span class=w> </span><span class=nn>transformers</span><span class=w> </span><span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModel</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=kn>import</span><span class=w> </span><span class=nn>torch</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>)</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;bert-base-uncased&#39;</span><span class=p>)</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=n>documents</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;Your document text here.&quot;</span><span class=p>]</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>documents</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>truncation</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a>    <span class=n>embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>)</span><span class=o>.</span><span class=n>last_hidden_state</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># Mean pooling for sentence embedding</span>
</span></code></pre></div></li> <li><strong>Resources</strong>:</li> <li><a href=https://huggingface.co/docs/transformers>Hugging Face Documentation</a></li> <li><a href=https://huggingface.co/models>Model Hub</a></li> </ul> <h4 id=13-universal-sentence-encoder-use><strong>1.3. Universal Sentence Encoder (USE)</strong><a class=headerlink href=#13-universal-sentence-encoder-use title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: Developed by Google, USE provides pre-trained models for generating embeddings that work across languages and for multiple tasks.</li> <li><strong>How to Use</strong> (via TensorFlow Hub): <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=kn>import</span><span class=w> </span><span class=nn>tensorflow_hub</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>hub</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=n>model</span> <span class=o>=</span> <span class=n>hub</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;</span><span class=p>)</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=n>documents</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;Document text here.&quot;</span><span class=p>]</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=n>embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span>
</span></code></pre></div></li> <li><strong>Resources</strong>:</li> <li><a href=https://tfhub.dev/google/universal-sentence-encoder/4>USE TensorFlow Hub</a></li> <li><a href=https://arxiv.org/abs/1803.11175>USE Paper</a></li> </ul> <h4 id=14-fasttext><strong>1.4. FastText</strong><a class=headerlink href=#14-fasttext title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: FastText (by Facebook AI) can generate word and document embeddings and supports training on your own data for custom embeddings.</li> <li><strong>How to Use</strong>: <div class="language-bash highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a>pip<span class=w> </span>install<span class=w> </span>fasttext
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=kn>import</span><span class=w> </span><span class=nn>fasttext</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=c1># Train on your own documents</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a><span class=n>model</span> <span class=o>=</span> <span class=n>fasttext</span><span class=o>.</span><span class=n>train_unsupervised</span><span class=p>(</span><span class=s1>&#39;your_document_file.txt&#39;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=s1>&#39;skipgram&#39;</span><span class=p>)</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=n>embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>get_sentence_vector</span><span class=p>(</span><span class=s2>&quot;Your document text here&quot;</span><span class=p>)</span>
</span></code></pre></div></li> <li><strong>Resources</strong>:</li> <li><a href=https://fasttext.cc/ >FastText Documentation</a></li> <li><a href=https://fasttext.cc/docs/en/crawl-vectors.html>Pre-trained Models</a></li> </ul> <h3 id=2-cloud-apis-for-generating-embeddings><strong>2. Cloud APIs for Generating Embeddings</strong><a class=headerlink href=#2-cloud-apis-for-generating-embeddings title="Permanent link">&para;</a></h3> <h4 id=21-openais-embedding-api><strong>2.1. OpenAI’s Embedding API</strong><a class=headerlink href=#21-openais-embedding-api title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: OpenAI provides an API that can generate high-quality embeddings using models like GPT and others. These embeddings can be used for document search, clustering, and other tasks.</li> <li><strong>How to Use</strong>: <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=kn>import</span><span class=w> </span><span class=nn>openai</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a><span class=n>openai</span><span class=o>.</span><span class=n>api_key</span> <span class=o>=</span> <span class=s2>&quot;your-api-key&quot;</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=n>response</span> <span class=o>=</span> <span class=n>openai</span><span class=o>.</span><span class=n>Embedding</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>    <span class=nb>input</span><span class=o>=</span><span class=s2>&quot;Your document text here&quot;</span><span class=p>,</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>    <span class=n>model</span><span class=o>=</span><span class=s2>&quot;text-embedding-ada-002&quot;</span>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a><span class=p>)</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a><span class=n>embeddings</span> <span class=o>=</span> <span class=n>response</span><span class=p>[</span><span class=s1>&#39;data&#39;</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=s1>&#39;embedding&#39;</span><span class=p>]</span>
</span></code></pre></div></li> <li><strong>Resources</strong>:</li> <li><a href=https://platform.openai.com/docs/guides/embeddings>OpenAI Embedding API</a></li> </ul> <h4 id=22-azure-cognitive-search><strong>2.2. Azure Cognitive Search</strong><a class=headerlink href=#22-azure-cognitive-search title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: Azure Cognitive Search provides document indexing and embedding generation using pre-built AI models.</li> <li><strong>How to Use</strong>:</li> <li>Index your documents using the Azure portal and apply built-in cognitive skills for embedding.</li> <li><strong>Resources</strong>:</li> <li><a href=https://docs.microsoft.com/en-us/azure/search/cognitive-search-what-is-concept>Azure Cognitive Search Documentation</a></li> </ul> <h4 id=23-google-clouds-vertex-ai-and-ai-hub><strong>2.3. Google Cloud’s Vertex AI and AI Hub</strong><a class=headerlink href=#23-google-clouds-vertex-ai-and-ai-hub title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: Google Cloud’s Vertex AI platform provides pre-trained models, including the Universal Sentence Encoder, for generating document embeddings.</li> <li><strong>How to Use</strong>:</li> <li>Access the USE model or other models via the AI Platform Notebooks or AI Hub.</li> <li><strong>Resources</strong>:</li> <li><a href=https://cloud.google.com/vertex-ai>Google Cloud Vertex AI</a></li> </ul> <h4 id=24-openais-embedding-api><strong>2.4. OpenAI’s Embedding API</strong><a class=headerlink href=#24-openais-embedding-api title="Permanent link">&para;</a></h4> <p><div class="language-text highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>import openai
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>openai.api_key = &quot;your-api-key&quot;
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>documents = [&quot;Document 1 text here.&quot;, &quot;Document 2 text here.&quot;]
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>response = openai.Embedding.create(
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>    input=documents,
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>    model=&quot;text-embedding-ada-002&quot;
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>)
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a>embeddings = [data[&#39;embedding&#39;] for data in response[&#39;data&#39;]]
</span></code></pre></div> <a href=https://platform.openai.com/docs/models/embeddings>List of OpenAI Embedding Models</a></p> <h4 id=25-voyage-embeddings><strong>2.5. Voyage Embeddings</strong><a class=headerlink href=#25-voyage-embeddings title="Permanent link">&para;</a></h4> <p><strong>What It Is</strong>: Voyage is a more recent model designed for <strong>universal cross-modal</strong> dense embeddings, focusing on aligning data from various modalities like text, images, audio, and even video into a shared embedding space. <a href=https://docs.voyageai.com/docs/embeddings>All Voyage Models</a></p> <p>List of Voyage Models: - voyage-3 - voyage-3-lite - voyage-finance-2 - voyage-multilingual-2 - voyage-law-2 - voyage-code-2</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>import voyageai
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>vo = voyageai.Client()
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a># This will automatically use the environment variable VOYAGE_API_KEY.
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a># Alternatively, you can use vo = voyageai.Client(api_key=&quot;&lt;your secret key&gt;&quot;)
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a>texts = [
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a>    &quot;The Mediterranean diet emphasizes fish, olive oil, and vegetables, believed to reduce chronic diseases.&quot;,
</span><span id=__span-7-9><a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a>    &quot;Photosynthesis in plants converts light energy into glucose and produces essential oxygen.&quot;,
</span><span id=__span-7-10><a id=__codelineno-7-10 name=__codelineno-7-10 href=#__codelineno-7-10></a>    &quot;20th-century innovations, from radios to smartphones, centered on electronic advancements.&quot;,
</span><span id=__span-7-11><a id=__codelineno-7-11 name=__codelineno-7-11 href=#__codelineno-7-11></a>    &quot;Rivers provide water, irrigation, and habitat for aquatic species, vital for ecosystems.&quot;,
</span><span id=__span-7-12><a id=__codelineno-7-12 name=__codelineno-7-12 href=#__codelineno-7-12></a>    &quot;Apple’s conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET.&quot;,
</span><span id=__span-7-13><a id=__codelineno-7-13 name=__codelineno-7-13 href=#__codelineno-7-13></a>    &quot;Shakespeare&#39;s works, like &#39;Hamlet&#39; and &#39;A Midsummer Night&#39;s Dream,&#39; endure in literature.&quot;
</span><span id=__span-7-14><a id=__codelineno-7-14 name=__codelineno-7-14 href=#__codelineno-7-14></a>]
</span><span id=__span-7-15><a id=__codelineno-7-15 name=__codelineno-7-15 href=#__codelineno-7-15></a>
</span><span id=__span-7-16><a id=__codelineno-7-16 name=__codelineno-7-16 href=#__codelineno-7-16></a># Embed the documents
</span><span id=__span-7-17><a id=__codelineno-7-17 name=__codelineno-7-17 href=#__codelineno-7-17></a>result = vo.embed(texts, model=&quot;voyage-3&quot;, input_type=&quot;document&quot;)
</span><span id=__span-7-18><a id=__codelineno-7-18 name=__codelineno-7-18 href=#__codelineno-7-18></a>print(result.embeddings)
</span></code></pre></div> <h3 id=3-custom-training-on-your-own-data><strong>3. Custom Training on Your Own Data</strong><a class=headerlink href=#3-custom-training-on-your-own-data title="Permanent link">&para;</a></h3> <p>If your documents are highly domain-specific, you may want to train your own embedding model. Here are a few approaches:</p> <h4 id=31-doc2vec-gensim><strong>3.1. Doc2Vec (Gensim)</strong><a class=headerlink href=#31-doc2vec-gensim title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: Doc2Vec is an extension of Word2Vec that generates document-level embeddings.</li> <li><strong>How to Use</strong>: <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=kn>import</span><span class=w> </span><span class=nn>gensim.models</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>g</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=kn>from</span><span class=w> </span><span class=nn>gensim.models.doc2vec</span><span class=w> </span><span class=kn>import</span> <span class=n>TaggedDocument</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a><span class=n>documents</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;Document 1 text here.&quot;</span><span class=p>,</span> <span class=s2>&quot;Document 2 text here.&quot;</span><span class=p>]</span>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a><span class=n>tagged_data</span> <span class=o>=</span> <span class=p>[</span><span class=n>TaggedDocument</span><span class=p>(</span><span class=n>words</span><span class=o>=</span><span class=n>doc</span><span class=o>.</span><span class=n>split</span><span class=p>(),</span> <span class=n>tags</span><span class=o>=</span><span class=p>[</span><span class=nb>str</span><span class=p>(</span><span class=n>i</span><span class=p>)])</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>doc</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>documents</span><span class=p>)]</span>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a><span class=n>model</span> <span class=o>=</span> <span class=n>g</span><span class=o>.</span><span class=n>Doc2Vec</span><span class=p>(</span><span class=n>tagged_data</span><span class=p>,</span> <span class=n>vector_size</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>window</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>min_count</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>workers</span><span class=o>=</span><span class=mi>4</span><span class=p>)</span>
</span><span id=__span-8-8><a id=__codelineno-8-8 name=__codelineno-8-8 href=#__codelineno-8-8></a><span class=n>embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>dv</span><span class=p>[</span><span class=s1>&#39;0&#39;</span><span class=p>]</span>  <span class=c1># Embedding for the first document</span>
</span></code></pre></div></li> <li><strong>Resources</strong>:</li> <li><a href=https://radimrehurek.com/gensim/models/doc2vec.html>Gensim Doc2Vec Documentation</a></li> </ul> <h4 id=32-fine-tuning-bert-or-other-transformers><strong>3.2. Fine-Tuning BERT or Other Transformers</strong><a class=headerlink href=#32-fine-tuning-bert-or-other-transformers title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: You can fine-tune a pre-trained BERT model (or any transformer) on your own documents to generate domain-specific embeddings.</li> <li><strong>How to Use</strong>: Hugging Face's <code>transformers</code> library can be used for fine-tuning.</li> <li>Follow a tutorial like this one: <a href=https://huggingface.co/transformers/training.html>Fine-Tuning Transformers</a></li> </ul> <h3 id=4-online-platforms-for-document-embedding><strong>4. Online Platforms for Document Embedding</strong><a class=headerlink href=#4-online-platforms-for-document-embedding title="Permanent link">&para;</a></h3> <h4 id=41-pinecone><strong>4.1. Pinecone</strong><a class=headerlink href=#41-pinecone title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: Pinecone is a vector database that provides APIs to generate, store, and search document embeddings efficiently.</li> <li><strong>How to Use</strong>:</li> <li>Integrate your embeddings generated by models like SBERT or OpenAI’s API into Pinecone for document search and retrieval.</li> <li><strong>Resources</strong>:</li> <li><a href=https://www.pinecone.io/ >Pinecone API</a></li> </ul> <h4 id=42-weaviate><strong>4.2. Weaviate</strong><a class=headerlink href=#42-weaviate title="Permanent link">&para;</a></h4> <ul> <li><strong>What It Is</strong>: Weaviate is an open-source vector search engine that supports storing and querying dense embeddings.</li> <li><strong>How to Use</strong>:</li> <li>Weaviate provides connectors for generating embeddings using pre-trained models and frameworks like Hugging Face.</li> <li><strong>Resources</strong>:</li> <li><a href=https://weaviate.io/ >Weaviate Documentation</a></li> </ul> <h2 id=hashtags>Hashtags<a class=headerlink href=#hashtags title="Permanent link">&para;</a></h2> <h1 id=denseembeddings>DenseEmbeddings<a class=headerlink href=#denseembeddings title="Permanent link">&para;</a></h1> <h1 id=machinelearning>MachineLearning<a class=headerlink href=#machinelearning title="Permanent link">&para;</a></h1> <h1 id=nlp>NLP<a class=headerlink href=#nlp title="Permanent link">&para;</a></h1> <h1 id=wordembeddings>WordEmbeddings<a class=headerlink href=#wordembeddings title="Permanent link">&para;</a></h1> <h1 id=voyagemodel>VoyageModel<a class=headerlink href=#voyagemodel title="Permanent link">&para;</a></h1> <h1 id=multimodalai>MultimodalAI<a class=headerlink href=#multimodalai title="Permanent link">&para;</a></h1> <h1 id=textembeddings>TextEmbeddings<a class=headerlink href=#textembeddings title="Permanent link">&para;</a></h1> <h1 id=deeplearning>DeepLearning<a class=headerlink href=#deeplearning title="Permanent link">&para;</a></h1> <h1 id=transformers>Transformers<a class=headerlink href=#transformers title="Permanent link">&para;</a></h1> <h1 id=contextualembeddings>ContextualEmbeddings<a class=headerlink href=#contextualembeddings title="Permanent link">&para;</a></h1> <h1 id=crossmodalai>CrossModalAI<a class=headerlink href=#crossmodalai title="Permanent link">&para;</a></h1> <h1 id=semanticsearch>SemanticSearch<a class=headerlink href=#semanticsearch title="Permanent link">&para;</a></h1> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>