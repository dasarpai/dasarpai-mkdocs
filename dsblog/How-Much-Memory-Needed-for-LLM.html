<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/How-Much-Memory-Needed-for-LLM.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>How Much Memory Needed for LLM - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#how-much-memory-needed-for-llm class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> How Much Memory Needed for LLM </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-llm class=md-nav__link> <span class=md-ellipsis> What is LLM? </span> </a> <nav class=md-nav aria-label="What is LLM?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-defines-a-large-language-model class=md-nav__link> <span class=md-ellipsis> What Defines a Large Language Model? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#how-much-memory-is-required-to-host-a-llama-31-with-405-billion-parameters class=md-nav__link> <span class=md-ellipsis> How much memory is required to host a Llama 3.1 with 405 billion parameters? </span> </a> <nav class=md-nav aria-label="How much memory is required to host a Llama 3.1 with 405 billion parameters?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gpu-requirements class=md-nav__link> <span class=md-ellipsis> GPU Requirements </span> </a> </li> <li class=md-nav__item> <a href=#cpu-and-ram-requirements class=md-nav__link> <span class=md-ellipsis> CPU and RAM Requirements </span> </a> </li> <li class=md-nav__item> <a href=#storage class=md-nav__link> <span class=md-ellipsis> Storage </span> </a> </li> <li class=md-nav__item> <a href=#networking class=md-nav__link> <span class=md-ellipsis> Networking </span> </a> </li> <li class=md-nav__item> <a href=#power-and-cooling class=md-nav__link> <span class=md-ellipsis> Power and Cooling </span> </a> </li> <li class=md-nav__item> <a href=#example-hardware-setup class=md-nav__link> <span class=md-ellipsis> Example Hardware Setup </span> </a> </li> <li class=md-nav__item> <a href=#cloud-solutions class=md-nav__link> <span class=md-ellipsis> Cloud Solutions </span> </a> </li> <li class=md-nav__item> <a href=#optimizations class=md-nav__link> <span class=md-ellipsis> Optimizations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#how-much-memory-is-required-to-host-a-llama-31-with-70-billion-parameters class=md-nav__link> <span class=md-ellipsis> How much memory is required to host a Llama 3.1 with 70 billion parameters? </span> </a> <nav class=md-nav aria-label="How much memory is required to host a Llama 3.1 with 70 billion parameters?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gpu-requirements_1 class=md-nav__link> <span class=md-ellipsis> GPU Requirements </span> </a> </li> <li class=md-nav__item> <a href=#cpu-and-ram-requirements_1 class=md-nav__link> <span class=md-ellipsis> CPU and RAM Requirements </span> </a> </li> <li class=md-nav__item> <a href=#storage_1 class=md-nav__link> <span class=md-ellipsis> Storage </span> </a> </li> <li class=md-nav__item> <a href=#networking_1 class=md-nav__link> <span class=md-ellipsis> Networking </span> </a> </li> <li class=md-nav__item> <a href=#power-and-cooling_1 class=md-nav__link> <span class=md-ellipsis> Power and Cooling </span> </a> </li> <li class=md-nav__item> <a href=#example-hardware-setup_1 class=md-nav__link> <span class=md-ellipsis> Example Hardware Setup </span> </a> </li> <li class=md-nav__item> <a href=#cloud-solutions_1 class=md-nav__link> <span class=md-ellipsis> Cloud Solutions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#how-to-calculate-memory-requirement-of-llm-based-on-parameter-information class=md-nav__link> <span class=md-ellipsis> How to calculate Memory requirement of LLM based on parameter information? </span> </a> <nav class=md-nav aria-label="How to calculate Memory requirement of LLM based on parameter information?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#estimating-memory-for-model-parameters-during-training class=md-nav__link> <span class=md-ellipsis> Estimating Memory for Model Parameters (during training) </span> </a> <nav class=md-nav aria-label="Estimating Memory for Model Parameters (during training)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-calculation class=md-nav__link> <span class=md-ellipsis> Example Calculation </span> </a> </li> <li class=md-nav__item> <a href=#additional-memory-requirements class=md-nav__link> <span class=md-ellipsis> Additional Memory Requirements </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#memory-required-for-inference class=md-nav__link> <span class=md-ellipsis> Memory Required for Inference </span> </a> <nav class=md-nav aria-label="Memory Required for Inference"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#example-memory-calculation-for-inference class=md-nav__link> <span class=md-ellipsis> Example Memory Calculation for Inference </span> </a> </li> <li class=md-nav__item> <a href=#total-memory-estimate-for-inference class=md-nav__link> <span class=md-ellipsis> Total Memory Estimate for Inference </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#how-to-calculate-memory-requirement-for-the-activation-function class=md-nav__link> <span class=md-ellipsis> How to calculate memory requirement for the Activation Function? </span> </a> <nav class=md-nav aria-label="How to calculate memory requirement for the Activation Function?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#factors-affecting-activation-memory class=md-nav__link> <span class=md-ellipsis> Factors Affecting Activation Memory </span> </a> </li> <li class=md-nav__item> <a href=#general-calculation class=md-nav__link> <span class=md-ellipsis> General Calculation </span> </a> </li> <li class=md-nav__item> <a href=#example-calculation_1 class=md-nav__link> <span class=md-ellipsis> Example Calculation </span> </a> <nav class=md-nav aria-label="Example Calculation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#memory-per-activation-tensor class=md-nav__link> <span class=md-ellipsis> Memory per Activation Tensor </span> </a> </li> <li class=md-nav__item> <a href=#total-activation-memory class=md-nav__link> <span class=md-ellipsis> Total Activation Memory </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#additional-considerations class=md-nav__link> <span class=md-ellipsis> Additional Considerations </span> </a> </li> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#popular-high-performance-large-language-models-llms-available-as-open-source-projects class=md-nav__link> <span class=md-ellipsis> Popular high-performance large language models (LLMs) available as open-source projects. </span> </a> <nav class=md-nav aria-label="Popular high-performance large language models (LLMs) available as open-source projects."> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-llama-large-language-model-meta-ai class=md-nav__link> <span class=md-ellipsis> 1. LLaMA (Large Language Model Meta AI) </span> </a> </li> <li class=md-nav__item> <a href=#2-gpt-j class=md-nav__link> <span class=md-ellipsis> 2. GPT-J </span> </a> </li> <li class=md-nav__item> <a href=#3-bloom class=md-nav__link> <span class=md-ellipsis> 3. BLOOM </span> </a> </li> <li class=md-nav__item> <a href=#4-opt-open-pretrained-transformer class=md-nav__link> <span class=md-ellipsis> 4. OPT (Open Pretrained Transformer) </span> </a> </li> <li class=md-nav__item> <a href=#5-t5-text-to-text-transfer-transformer class=md-nav__link> <span class=md-ellipsis> 5. T5 (Text-to-Text Transfer Transformer) </span> </a> </li> <li class=md-nav__item> <a href=#6-roberta class=md-nav__link> <span class=md-ellipsis> 6. RoBERTa </span> </a> </li> <li class=md-nav__item> <a href=#7-albert-a-lite-bert class=md-nav__link> <span class=md-ellipsis> 7. ALBERT (A Lite BERT) </span> </a> </li> <li class=md-nav__item> <a href=#8-bert-bidirectional-encoder-representations-from-transformers class=md-nav__link> <span class=md-ellipsis> 8. BERT (Bidirectional Encoder Representations from Transformers) </span> </a> </li> <li class=md-nav__item> <a href=#9-megatron-lm class=md-nav__link> <span class=md-ellipsis> 9. Megatron-LM </span> </a> </li> <li class=md-nav__item> <a href=#10-gpt-neox class=md-nav__link> <span class=md-ellipsis> 10. GPT-NeoX </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/How-Much-Memory-Needed-for-LLM.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/How-Much-Memory-Needed-for-LLM.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt=How-Much-Memory-Needed-for-LLM src=../assets/images/dspost/dsp6133-How-Much-Memory-Needed-for-LLM.jpg></p> <h1 id=how-much-memory-needed-for-llm>How Much Memory Needed for LLM?<a class=headerlink href=#how-much-memory-needed-for-llm title="Permanent link">&para;</a></h1> <h2 id=what-is-llm>What is LLM?<a class=headerlink href=#what-is-llm title="Permanent link">&para;</a></h2> <p>LLM stands for <strong>Large Language Model</strong>. These are machine learning models that are trained on massive amounts of text data to understand, generate, and work with human language in a way that mimics natural language understanding. They are called "large" because of the significant number of parameters they contain, often numbering in the billions or even trillions.</p> <h3 id=what-defines-a-large-language-model>What Defines a Large Language Model?<a class=headerlink href=#what-defines-a-large-language-model title="Permanent link">&para;</a></h3> <p>There is no strict or universally accepted benchmark to define what constitutes an LLM purely based on the number of parameters. The term "large" is relative and depends on the current state of technology and the size of models being developed. As technology progresses, what is considered "large" may continue to grow. However, some general guidelines have emerged:</p> <ol> <li><strong>Scale of Parameters</strong>: </li> <li>Models with hundreds of millions to billions of parameters are typically referred to as large language models. </li> <li> <p>For instance, OpenAI's GPT-2 has 1.5 billion parameters, and GPT-3 has 175 billion parameters. Models like these are typically categorized as LLMs.</p> </li> <li> <p><strong>Training Data</strong>: </p> </li> <li> <p>LLMs are trained on vast and diverse datasets, often encompassing hundreds of gigabytes or even terabytes of text data. This data can include books, websites, articles, and more, which helps the model learn a broad range of language patterns and knowledge.</p> </li> <li> <p><strong>Capabilities</strong>: </p> </li> <li>LLMs are distinguished by their ability to perform a wide variety of tasks without task-specific training. They can generate coherent and contextually relevant text, translate languages, summarize documents, answer questions, and more, simply based on the data they were trained on.</li> </ol> <h2 id=how-much-memory-is-required-to-host-a-llama-31-with-405-billion-parameters>How much memory is required to host a Llama 3.1 with 405 billion parameters?<a class=headerlink href=#how-much-memory-is-required-to-host-a-llama-31-with-405-billion-parameters title="Permanent link">&para;</a></h2> <p>Training or deploying large language models (LLMs) is a big challenge. The first question that comes to mind is how much memory is required to develop or use these models. How much memory is enough? Training and deploying these models is so expensive that small or mid-sized companies cannot afford to commit resources for the long term unless the value gained exceeds the costs incurred in developing, running, and maintaining these models. This article is not just about memory but overall what kind of hardware we may need to work with LLMs.</p> <p>Deploying a large-scale model like Llama 3.1 with 405 billion parameters for inference purposes requires substantial hardware resources. Here are the main considerations:</p> <h3 id=gpu-requirements>GPU Requirements<a class=headerlink href=#gpu-requirements title="Permanent link">&para;</a></h3> <ol> <li><strong>Memory (VRAM)</strong>:</li> <li>Large models require significant GPU memory. A model with 405 billion parameters will need several high-memory GPUs. The exact amount of VRAM required will depend on the specific architecture and the optimizations available.</li> <li> <p>As a rough estimate, you might need multiple GPUs with at least 80 GB of VRAM each (like NVIDIA A100 80GB).</p> </li> <li> <p><strong>Compute Power</strong>:</p> </li> <li> <p>High-end GPUs like NVIDIA A100, NVIDIA V100, or the newer H100 are recommended due to their high compute power and memory bandwidth.</p> </li> <li> <p><strong>Multi-GPU Setup</strong>:</p> </li> <li>Given the size of the model, you will likely need a multi-GPU setup. This could be achieved using NVIDIA's NVLink or NVSwitch to enable efficient communication between GPUs.</li> <li>Distributed inference frameworks like NVIDIA Triton Inference Server, Hugging Face Accelerate, or DeepSpeed can help manage multi-GPU setups effectively.</li> </ol> <h3 id=cpu-and-ram-requirements>CPU and RAM Requirements<a class=headerlink href=#cpu-and-ram-requirements title="Permanent link">&para;</a></h3> <ol> <li><strong>CPU</strong>:</li> <li>A powerful multi-core CPU is recommended to handle data preprocessing, postprocessing, and feeding data to GPUs. CPUs with many cores and high clock speeds will be beneficial.</li> <li> <p>Examples: AMD EPYC or Intel Xeon processors with at least 32 cores.</p> </li> <li> <p><strong>RAM</strong>:</p> </li> <li>Adequate system memory (RAM) is needed to support data handling and GPU operations. A system with at least 512 GB of RAM is recommended, but more might be necessary depending on the exact workload.</li> </ol> <h3 id=storage>Storage<a class=headerlink href=#storage title="Permanent link">&para;</a></h3> <ol> <li><strong>High-Speed Storage</strong>:</li> <li>NVMe SSDs are recommended for fast read/write operations, especially when dealing with large datasets.</li> <li>Having several terabytes of storage is advisable to store model checkpoints, datasets, and intermediate outputs.</li> </ol> <h3 id=networking>Networking<a class=headerlink href=#networking title="Permanent link">&para;</a></h3> <ol> <li><strong>High-Speed Networking</strong>:</li> <li>If you are using a multi-node setup, high-speed networking (e.g., InfiniBand) is crucial for efficient data transfer between nodes.</li> <li>Low-latency networking ensures that the GPUs can communicate quickly, reducing bottlenecks during inference.</li> </ol> <h3 id=power-and-cooling>Power and Cooling<a class=headerlink href=#power-and-cooling title="Permanent link">&para;</a></h3> <ol> <li><strong>Power Supply</strong>:</li> <li>Ensure that the power supply can handle the load from multiple high-end GPUs and CPUs.</li> <li> <p>Redundant power supplies are recommended for stability and reliability.</p> </li> <li> <p><strong>Cooling</strong>:</p> </li> <li>Effective cooling solutions are necessary to maintain the optimal temperature of GPUs and CPUs.</li> <li>Data center-grade cooling systems or liquid cooling solutions may be required.</li> </ol> <h3 id=example-hardware-setup>Example Hardware Setup<a class=headerlink href=#example-hardware-setup title="Permanent link">&para;</a></h3> <p>Here is an example of a possible hardware setup:</p> <ol> <li><strong>GPUs</strong>: 8x NVIDIA A100 80GB</li> <li><strong>CPU</strong>: Dual AMD EPYC 7742 (64 cores per CPU, 128 threads total)</li> <li><strong>RAM</strong>: 1 TB DDR4</li> <li><strong>Storage</strong>: 4x 2 TB NVMe SSDs</li> <li><strong>Networking</strong>: InfiniBand networking with 100 Gbps bandwidth</li> <li><strong>Cooling</strong>: Advanced liquid cooling system</li> <li><strong>Power</strong>: Redundant power supplies with at least 3 kW capacity</li> </ol> <h3 id=cloud-solutions>Cloud Solutions<a class=headerlink href=#cloud-solutions title="Permanent link">&para;</a></h3> <p>If investing in physical hardware is not feasible, cloud service providers like AWS, Google Cloud, and Azure offer instances specifically designed for machine learning workloads. For instance:</p> <ul> <li><strong>AWS</strong>: p4d.24xlarge (8x NVIDIA A100 40GB) or p4de.24xlarge (8x NVIDIA A100 80GB)</li> <li><strong>Google Cloud</strong>: A2 MegaMachine (up to 16x NVIDIA A100 40GB)</li> <li><strong>Azure</strong>: NDm A100 v4 series (up to 8x NVIDIA A100 40GB)</li> </ul> <p>Using cloud instances can provide the necessary hardware resources on a pay-as-you-go basis, allowing for scalability and flexibility.</p> <h3 id=optimizations>Optimizations<a class=headerlink href=#optimizations title="Permanent link">&para;</a></h3> <p>To make the most out of your hardware, consider the following optimizations:</p> <ul> <li><strong>Mixed Precision Inference</strong>: Use mixed precision (FP16) to reduce memory usage and improve performance.</li> <li><strong>Model Parallelism</strong>: Split the model across multiple GPUs using libraries like Megatron-LM or Hugging Face Accelerate.</li> <li><strong>Efficient Data Loading</strong>: Use efficient data loading and preprocessing techniques to keep the GPUs fed with data without bottlenecks.</li> </ul> <p>By carefully selecting and configuring your hardware and optimizing your inference pipeline, you can effectively deploy a large-scale model like Llama 3.1 405B for inference purposes.</p> <h2 id=how-much-memory-is-required-to-host-a-llama-31-with-70-billion-parameters>How much memory is required to host a Llama 3.1 with 70 billion parameters?<a class=headerlink href=#how-much-memory-is-required-to-host-a-llama-31-with-70-billion-parameters title="Permanent link">&para;</a></h2> <p>Deploying the Llama 3.1 70B model for inference is more feasible compared to the 405B model but still requires substantial hardware resources. Here's a detailed breakdown of the hardware needed:</p> <h3 id=gpu-requirements_1>GPU Requirements<a class=headerlink href=#gpu-requirements_1 title="Permanent link">&para;</a></h3> <ol> <li><strong>Memory (VRAM)</strong>:</li> <li>The Llama 3.1 70B model requires significant GPU memory. You will need GPUs with large VRAM capacities. Typically, GPUs with 40-80 GB of VRAM are suitable.</li> <li> <p>For efficient inference, using GPUs like the NVIDIA A100 40GB or 80GB is recommended.</p> </li> <li> <p><strong>Compute Power</strong>:</p> </li> <li> <p>High-end GPUs are recommended due to their compute capabilities. NVIDIA A100 or newer H100 GPUs are well-suited for this task.</p> </li> <li> <p><strong>Multi-GPU Setup</strong>:</p> </li> <li>Depending on the specific implementation and optimizations, you might need multiple GPUs. Using frameworks that support model parallelism (e.g., NVIDIA Triton Inference Server, Hugging Face Accelerate, or DeepSpeed) can help manage this setup efficiently.</li> </ol> <h3 id=cpu-and-ram-requirements_1>CPU and RAM Requirements<a class=headerlink href=#cpu-and-ram-requirements_1 title="Permanent link">&para;</a></h3> <ol> <li><strong>CPU</strong>:</li> <li> <p>A powerful multi-core CPU is recommended to handle data preprocessing, postprocessing, and managing data transfer to GPUs. AMD EPYC or Intel Xeon processors with at least 16-32 cores are ideal.</p> </li> <li> <p><strong>RAM</strong>:</p> </li> <li>Adequate system memory (RAM) is needed to support data handling and GPU operations. A system with at least 256 GB of RAM is recommended, though 512 GB or more would be better for more demanding workloads.</li> </ol> <h3 id=storage_1>Storage<a class=headerlink href=#storage_1 title="Permanent link">&para;</a></h3> <ol> <li><strong>High-Speed Storage</strong>:</li> <li>NVMe SSDs are recommended for fast read/write operations, especially when dealing with large datasets. Several terabytes of storage may be needed to store model checkpoints, datasets, and intermediate outputs.</li> </ol> <h3 id=networking_1>Networking<a class=headerlink href=#networking_1 title="Permanent link">&para;</a></h3> <ol> <li><strong>High-Speed Networking</strong>:</li> <li>For multi-node setups, high-speed networking (e.g., InfiniBand) is crucial for efficient data transfer between nodes. This ensures low latency and high bandwidth.</li> </ol> <h3 id=power-and-cooling_1>Power and Cooling<a class=headerlink href=#power-and-cooling_1 title="Permanent link">&para;</a></h3> <ol> <li><strong>Power Supply</strong>:</li> <li> <p>Ensure that the power supply can handle the load from multiple high-end GPUs and CPUs. Redundant power supplies are recommended for stability and reliability.</p> </li> <li> <p><strong>Cooling</strong>:</p> </li> <li>Effective cooling solutions are necessary to maintain optimal temperatures for GPUs and CPUs. Data center-grade cooling systems or advanced liquid cooling solutions may be required.</li> </ol> <h3 id=example-hardware-setup_1>Example Hardware Setup<a class=headerlink href=#example-hardware-setup_1 title="Permanent link">&para;</a></h3> <p>Here is an example of a possible hardware setup for deploying the Llama 3.1 70B model:</p> <ol> <li><strong>GPUs</strong>: 4x NVIDIA A100 80GB</li> <li><strong>CPU</strong>: Dual AMD EPYC 7742 (64 cores per CPU, 128 threads total)</li> <li><strong>RAM</strong>: 512 GB DDR4</li> <li><strong>Storage</strong>: 4x 2 TB NVMe SSDs</li> <li><strong>Networking</strong>: InfiniBand networking with 100 Gbps bandwidth</li> <li><strong>Cooling</strong>: Advanced liquid cooling system</li> <li><strong>Power</strong>: Redundant power supplies with at least 3 kW capacity</li> </ol> <h3 id=cloud-solutions_1>Cloud Solutions<a class=headerlink href=#cloud-solutions_1 title="Permanent link">&para;</a></h3> <p>If investing in physical hardware is not feasible, consider using cloud service providers like AWS, Google Cloud, or Azure, which offer instances specifically designed for machine learning workloads. For instance:</p> <ul> <li><strong>AWS</strong>: p4d.24xlarge (8x NVIDIA A100 40GB) or p4de.24xlarge (8x NVIDIA A100 80GB)</li> <li><strong>Google Cloud</strong>: A2 MegaMachine (up to 16x NVIDIA A100 40GB)</li> <li><strong>Azure</strong>: NDm A100 v4 series (up to 8x NVIDIA A100 40GB)</li> </ul> <p>Using cloud instances provides the necessary hardware resources on a pay-as-you-go basis, allowing for scalability and flexibility.</p> <h2 id=how-to-calculate-memory-requirement-of-llm-based-on-parameter-information>How to calculate Memory requirement of LLM based on parameter information?<a class=headerlink href=#how-to-calculate-memory-requirement-of-llm-based-on-parameter-information title="Permanent link">&para;</a></h2> <p>There is a rough calculation for estimating the memory needed based on the model size. The memory required for a language model primarily depends on the number of parameters, the precision used for storing those parameters, and additional memory needed for activations and intermediate computations during training or inference.</p> <h3 id=estimating-memory-for-model-parameters-during-training>Estimating Memory for Model Parameters (during training)<a class=headerlink href=#estimating-memory-for-model-parameters-during-training title="Permanent link">&para;</a></h3> <ol> <li><strong>Parameters</strong>:</li> <li> <p>Each parameter in the model typically requires storage. For floating-point precision:</p> <ul> <li><strong>32-bit float (FP32)</strong>: 4 bytes per parameter</li> <li><strong>16-bit float (FP16)</strong>: 2 bytes per parameter</li> <li><strong>8-bit integer (INT8)</strong>: 1 byte per parameter (usually used for quantized models)</li> </ul> </li> <li> <p><strong>Calculation</strong>:</p> </li> <li><strong>Memory for parameters</strong> = Number of parameters × Size per parameter</li> </ol> <h4 id=example-calculation>Example Calculation<a class=headerlink href=#example-calculation title="Permanent link">&para;</a></h4> <p>For a model with 70 billion parameters using FP32 precision:</p> <ol> <li><strong>Parameters</strong>: 70 billion</li> <li><strong>Size per parameter (FP32)</strong>: 4 bytes</li> </ol> <div class=arithmatex>\[ \text{Memory for parameters} = 70 \times 10^9 \text{ parameters} \times 4 \text{ bytes/parameter}$$ $$ \text{Memory for parameters} = 280 \text{ GB}\]</div> <p>For FP16 precision:</p> <ol> <li><strong>Parameters</strong>: 70 billion</li> <li><strong>Size per parameter (FP16)</strong>: 2 bytes</li> </ol> <div class=arithmatex>\[ \text{Memory for parameters} = 70 \times 10^9 \text{ parameters} \times 2 \text{ bytes/parameter}$$ $$ \text{Memory for parameters} = 140 \text{ GB}\]</div> <h4 id=additional-memory-requirements>Additional Memory Requirements<a class=headerlink href=#additional-memory-requirements title="Permanent link">&para;</a></h4> <ol> <li> <p><strong>Activations</strong>: During inference and especially during training, additional memory is needed for activations, which can be substantial. The memory required for activations depends on the batch size, sequence length, and model architecture.</p> </li> <li> <p><strong>Gradient Storage (Training)</strong>: During training, you also need memory for gradients, which is roughly the same as the memory for the parameters, but this is not needed for inference.</p> </li> <li> <p><strong>Optimizer States (Training)</strong>: Optimizers (like Adam) maintain additional states (momentum, variance) that require additional memory, often about the same as the model parameters.</p> </li> </ol> <h3 id=memory-required-for-inference>Memory Required for Inference<a class=headerlink href=#memory-required-for-inference title="Permanent link">&para;</a></h3> <p>For inference, you can often reduce the memory footprint by: - <strong>Using FP16 precision</strong>: Reduces the memory requirement by half compared to FP32. - <strong>Activations</strong>: Memory for activations depends on the model's architecture and sequence length.</p> <h4 id=example-memory-calculation-for-inference>Example Memory Calculation for Inference<a class=headerlink href=#example-memory-calculation-for-inference title="Permanent link">&para;</a></h4> <p>If using a model with 70 billion parameters and FP16 precision:</p> <ol> <li><strong>Memory for parameters</strong>: 140 GB</li> <li><strong>Estimated activation memory</strong>: This can vary but might be around 10-30 GB depending on batch size and sequence length.</li> </ol> <h4 id=total-memory-estimate-for-inference>Total Memory Estimate for Inference<a class=headerlink href=#total-memory-estimate-for-inference title="Permanent link">&para;</a></h4> <ul> <li><strong>Total Memory Estimate</strong>: Memory for parameters + Memory for activations</li> <li>For a rough estimate: 140 GB (parameters) + 10-30 GB (activations) = <strong>150-170 GB</strong></li> </ul> <p>This is a rough estimate and can vary based on implementation specifics, additional overheads, and optimizations used. For accurate hardware requirements, you should consider using profiling tools specific to your deployment environment.</p> <h2 id=how-to-calculate-memory-requirement-for-the-activation-function>How to calculate memory requirement for the Activation Function?<a class=headerlink href=#how-to-calculate-memory-requirement-for-the-activation-function title="Permanent link">&para;</a></h2> <p>Estimating the memory required for activation functions in a large language model like a 70 billion parameter model involves several factors, including the model architecture, batch size, sequence length, and the specific operations performed by the activation functions. Here’s a general approach to estimate the memory required:</p> <h3 id=factors-affecting-activation-memory>Factors Affecting Activation Memory<a class=headerlink href=#factors-affecting-activation-memory title="Permanent link">&para;</a></h3> <ol> <li><strong>Model Architecture</strong>:</li> <li> <p>The architecture (e.g., Transformer) affects the number of intermediate activations. For instance, the attention mechanisms and feed-forward layers generate activations that need to be stored.</p> </li> <li> <p><strong>Batch Size</strong>:</p> </li> <li> <p>The batch size impacts the number of activations stored per forward pass. Larger batch sizes will require more memory.</p> </li> <li> <p><strong>Sequence Length</strong>:</p> </li> <li> <p>For sequence-based models (like Transformers), the sequence length affects the size of activations, especially in attention layers.</p> </li> <li> <p><strong>Number of Layers</strong>:</p> </li> <li>More layers mean more intermediate activations need to be stored.</li> </ol> <h3 id=general-calculation>General Calculation<a class=headerlink href=#general-calculation title="Permanent link">&para;</a></h3> <p>Let's outline a rough calculation for a Transformer-based model:</p> <ol> <li><strong>Layer Outputs</strong>:</li> <li> <p>Each layer in a Transformer typically has an activation tensor with the shape <code>(batch_size, sequence_length, hidden_size)</code>.</p> </li> <li> <p><strong>Memory per Activation Tensor</strong>:</p> </li> <li><strong>Memory per tensor</strong> = <code>batch_size × sequence_length × hidden_size × size_per_element</code></li> <li> <p>For FP32 (4 bytes per element) or FP16 (2 bytes per element).</p> </li> <li> <p><strong>Total Activations</strong>:</p> </li> <li>The total memory for activations is the sum of activations across all layers.</li> </ol> <h3 id=example-calculation_1>Example Calculation<a class=headerlink href=#example-calculation_1 title="Permanent link">&para;</a></h3> <p>Let’s assume a Transformer model with the following characteristics:</p> <ul> <li><strong>Hidden Size</strong>: 16,384 (for large models)</li> <li><strong>Batch Size</strong>: 1</li> <li><strong>Sequence Length</strong>: 2,048</li> <li><strong>Number of Layers</strong>: 96 (typical for very large models)</li> </ul> <h4 id=memory-per-activation-tensor>Memory per Activation Tensor<a class=headerlink href=#memory-per-activation-tensor title="Permanent link">&para;</a></h4> <p>For FP32:</p> <ul> <li><strong>Memory per tensor</strong> = <code>1 × 2048 × 16384 × 4 bytes</code></li> <li><strong>Memory per tensor</strong> = <code>134,217,728 bytes</code> ≈ 134.2 MB</li> </ul> <p>For FP16:</p> <ul> <li><strong>Memory per tensor</strong> = <code>1 × 2048 × 16384 × 2 bytes</code></li> <li><strong>Memory per tensor</strong> = <code>67,108,864 bytes</code> ≈ 67.1 MB</li> </ul> <h4 id=total-activation-memory>Total Activation Memory<a class=headerlink href=#total-activation-memory title="Permanent link">&para;</a></h4> <p>Each Transformer layer typically generates activations for: - <strong>Self-attention</strong>: Depends on attention heads and sequence length. - <strong>Feed-forward network</strong>: Additional activations.</p> <p>For simplicity, assume 2 main activation tensors per layer (one for attention and one for the feed-forward network):</p> <ul> <li><strong>Total memory for activations per layer</strong> (FP32) = <code>2 × 134.2 MB</code> = <code>268.4 MB</code></li> <li><strong>Total memory for activations per layer</strong> (FP16) = <code>2 × 67.1 MB</code> = <code>134.2 MB</code></li> </ul> <p>For 96 layers:</p> <ul> <li><strong>Total activation memory (FP32)</strong> = <code>96 × 268.4 MB</code> ≈ <code>25.8 GB</code></li> <li><strong>Total activation memory (FP16)</strong> = <code>96 × 134.2 MB</code> ≈ <code>12.9 GB</code></li> </ul> <h3 id=additional-considerations>Additional Considerations<a class=headerlink href=#additional-considerations title="Permanent link">&para;</a></h3> <ol> <li> <p><strong>Intermediate Activations</strong>: Other intermediate activations and temporary tensors during operations will add to the total memory requirement.</p> </li> <li> <p><strong>Activation Checkpoints</strong>: Techniques like activation checkpointing can help reduce memory usage by storing only a subset of activations and recomputing others as needed.</p> </li> <li> <p><strong>Optimizations</strong>: Advanced optimizations and frameworks can significantly impact actual memory usage.</p> </li> </ol> <h3 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">&para;</a></h3> <p>For a 70 billion parameter model, the activation memory required can range approximately between <strong>12.9 GB</strong> (using FP16 precision) and <strong>25.8 GB</strong> (using FP32 precision), depending on the specific model architecture and sequence length. This is a rough estimate and actual memory usage can vary based on implementation details and optimizations.</p> <h2 id=popular-high-performance-large-language-models-llms-available-as-open-source-projects>Popular high-performance large language models (LLMs) available as open-source projects.<a class=headerlink href=#popular-high-performance-large-language-models-llms-available-as-open-source-projects title="Permanent link">&para;</a></h2> <h3 id=1-llama-large-language-model-meta-ai>1. <strong>LLaMA (Large Language Model Meta AI)</strong><a class=headerlink href=#1-llama-large-language-model-meta-ai title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: Meta AI (Facebook)</li> <li><strong>Description</strong>: LLaMA is designed to provide efficient and scalable solutions for large language models.</li> <li><strong>Key Features</strong>: High performance on various benchmarks, open-source availability, and multiple parameter sizes.</li> <li><strong>Repository</strong>: <a href=https://github.com/facebookresearch/llama>GitHub</a></li> </ul> <h3 id=2-gpt-j>2. <strong>GPT-J</strong><a class=headerlink href=#2-gpt-j title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: EleutherAI</li> <li><strong>Description</strong>: GPT-J is a 6 billion parameter model trained by EleutherAI. It aims to provide a large-scale, open-source alternative to proprietary models like GPT-3.</li> <li><strong>Key Features</strong>: High performance on various NLP tasks, easy to deploy, and community-supported.</li> <li><strong>Repository</strong>: <a href=https://github.com/kingoflolz/mesh-transformer-jax>GitHub</a></li> </ul> <h3 id=3-bloom>3. <strong>BLOOM</strong><a class=headerlink href=#3-bloom title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: BigScience</li> <li><strong>Description</strong>: BLOOM is an open-source language model created through a large-scale collaboration involving over a thousand researchers. It aims to democratize access to large-scale language models.</li> <li><strong>Key Features</strong>: Multilingual capabilities, various model sizes, and designed with ethical considerations in mind.</li> <li><strong>Repository</strong>: <a href=https://huggingface.co/bigscience/bloom>Hugging Face</a></li> </ul> <h3 id=4-opt-open-pretrained-transformer>4. <strong>OPT (Open Pretrained Transformer)</strong><a class=headerlink href=#4-opt-open-pretrained-transformer title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: Meta AI (Facebook)</li> <li><strong>Description</strong>: OPT is an open-source series of language models designed to provide transparency and reproducibility in training large-scale models.</li> <li><strong>Key Features</strong>: Models up to 175 billion parameters, transparent documentation of training processes, and available checkpoints.</li> <li><strong>Repository</strong>: <a href=https://github.com/facebookresearch/metaseq>GitHub</a></li> </ul> <h3 id=5-t5-text-to-text-transfer-transformer>5. <strong>T5 (Text-to-Text Transfer Transformer)</strong><a class=headerlink href=#5-t5-text-to-text-transfer-transformer title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: Google Research</li> <li><strong>Description</strong>: T5 is a flexible and powerful model that frames all NLP tasks as text-to-text problems, allowing for a unified approach to a wide range of tasks.</li> <li><strong>Key Features</strong>: Versatile across multiple NLP tasks, pre-trained on a diverse dataset, and various model sizes.</li> <li><strong>Repository</strong>: <a href=https://github.com/google-research/text-to-text-transfer-transformer>GitHub</a></li> </ul> <h3 id=6-roberta>6. <strong>RoBERTa</strong><a class=headerlink href=#6-roberta title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: Facebook AI</li> <li><strong>Description</strong>: RoBERTa (A Robustly Optimized BERT Pretraining Approach) is an optimized version of BERT, designed to maximize performance on NLP benchmarks.</li> <li><strong>Key Features</strong>: Enhanced training techniques, improved performance over BERT, and widely used in the NLP community.</li> <li><strong>Repository</strong>: <a href=https://github.com/pytorch/fairseq/tree/main/examples/roberta>GitHub</a></li> </ul> <h3 id=7-albert-a-lite-bert>7. <strong>ALBERT (A Lite BERT)</strong><a class=headerlink href=#7-albert-a-lite-bert title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: Google Research</li> <li><strong>Description</strong>: ALBERT reduces the memory and computation requirements of BERT while maintaining similar performance levels.</li> <li><strong>Key Features</strong>: Parameter-sharing across layers, factorized embedding parameterization, and efficient training.</li> <li><strong>Repository</strong>: <a href=https://github.com/google-research/albert>GitHub</a></li> </ul> <h3 id=8-bert-bidirectional-encoder-representations-from-transformers>8. <strong>BERT (Bidirectional Encoder Representations from Transformers)</strong><a class=headerlink href=#8-bert-bidirectional-encoder-representations-from-transformers title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: Google AI</li> <li><strong>Description</strong>: BERT is a groundbreaking model that introduced bidirectional training of transformers for NLP tasks.</li> <li><strong>Key Features</strong>: Strong performance on a wide range of NLP tasks, pre-trained on large datasets, and the basis for many subsequent models.</li> <li><strong>Repository</strong>: <a href=https://github.com/google-research/bert>GitHub</a></li> </ul> <h3 id=9-megatron-lm>9. <strong>Megatron-LM</strong><a class=headerlink href=#9-megatron-lm title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: NVIDIA</li> <li><strong>Description</strong>: Megatron-LM is a framework for training large-scale transformer models with efficient model parallelism.</li> <li><strong>Key Features</strong>: Scalable to very large models, efficient training on multi-GPU systems, and used for training models up to hundreds of billions of parameters.</li> <li><strong>Repository</strong>: <a href=https://github.com/NVIDIA/Megatron-LM>GitHub</a></li> </ul> <h3 id=10-gpt-neox>10. <strong>GPT-NeoX</strong><a class=headerlink href=#10-gpt-neox title="Permanent link">&para;</a></h3> <ul> <li><strong>Developed by</strong>: EleutherAI</li> <li><strong>Description</strong>: GPT-NeoX is a large-scale implementation of GPT-3 architecture. It's designed to be a flexible and efficient framework for training and deploying large language models.</li> <li><strong>Key Features</strong>: Supports models up to 20 billion parameters, distributed training, and inference capabilities.</li> <li><strong>Repository</strong>: <a href=https://github.com/EleutherAI/gpt-neox>GitHub</a></li> </ul> <p>These models provide a range of capabilities and are backed by active research and community support. Depending on your specific use case, computational resources, and expertise, you can choose a model that best fits your needs.</p> <p><strong>Author</strong> <br> Dr Hari Thapliyaal <br> dasarpai.com <br> linkedin.com/in/harithapliyal </p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>