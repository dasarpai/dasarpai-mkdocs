<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Exploring-AI-Benchmarks-and-Leaderboards.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Exploring AI Benchmarks & Leaderboards - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta name=author content="Hari Thapliyaal"><meta name=description content="Investigate the role of benchmarks and leaderboards in evaluating AI models, focusing on performance metrics and industry standards."><meta name=keywords content="evaluating AI models, performance metrics for AI, AI model evaluation benchmarks, AI leaderboards, standardized AI evaluation, comparing AI models, AI model performance evaluation"><meta property=og:type content=article><meta property=og:locale content=en_US><meta property=og:site_name content=DasarpAI><meta property=og:title content="Exploring AI Benchmarks & Leaderboards"><meta property=og:description content="Investigate the role of benchmarks and leaderboards in evaluating AI models, focusing on performance metrics and industry standards."><meta property=og:url content=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Exploring-AI-Benchmarks-and-Leaderboards.html><meta property=og:image content=../../assets/images/dspost/dsp6210-Exploring-AI-Benchmarks-and-Leaderboards.jpg><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta name=twitter:card content=summary_large_image><meta name=twitter:site content=@dasarpai><meta name=twitter:title content="Exploring AI Benchmarks & Leaderboards"><meta name=twitter:description content="Investigate the role of benchmarks and leaderboards in evaluating AI models, focusing on performance metrics and industry standards."><meta name=twitter:image content=../../assets/images/dspost/dsp6210-Exploring-AI-Benchmarks-and-Leaderboards.jpg><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Exploring-AI-Benchmarks-and-Leaderboards.html><link rel=stylesheet href=../assets/stylesheets/custom.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#exploring-ai-benchmarks-leaderboards class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@harithapliyal</strong> on <a rel=me href=https://linkedin.com/in/harithapliyal> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/dasarpai> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Exploring AI Benchmarks & Leaderboards </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> <nav class=md-nav aria-label=Introduction> <ul class=md-nav__list> <li class=md-nav__item> <a href=#why-do-we-need-benchmarks class=md-nav__link> <span class=md-ellipsis> Why do we need Benchmarks? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-the-components-of-an-ai-benchmark class=md-nav__link> <span class=md-ellipsis> What are the Components of an AI Benchmark? </span> </a> </li> <li class=md-nav__item> <a href=#types-of-benchmarks class=md-nav__link> <span class=md-ellipsis> Types of Benchmarks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-widely-known-benchmarks class=md-nav__link> <span class=md-ellipsis> What are widely known Benchmarks? </span> </a> <nav class=md-nav aria-label="What are widely known Benchmarks?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-natural-language-processing-nlp-benchmarks class=md-nav__link> <span class=md-ellipsis> 1. Natural Language Processing (NLP) Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#2-computer-vision-benchmarks class=md-nav__link> <span class=md-ellipsis> 2. Computer Vision Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#3-speech-and-audio-benchmarks class=md-nav__link> <span class=md-ellipsis> 3. Speech and Audio Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#4-reinforcement-learning-benchmarks class=md-nav__link> <span class=md-ellipsis> 4. Reinforcement Learning Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#5-generative-ai-benchmarks class=md-nav__link> <span class=md-ellipsis> 5. Generative AI Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#6-multimodal-benchmarks class=md-nav__link> <span class=md-ellipsis> 6. Multimodal Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#7-ethics-and-fairness-benchmarks class=md-nav__link> <span class=md-ellipsis> 7. Ethics and Fairness Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#8-general-ai-agi-benchmarks class=md-nav__link> <span class=md-ellipsis> 8. General AI (AGI) Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#9-temporal-and-sequential-benchmarks class=md-nav__link> <span class=md-ellipsis> 9. Temporal and Sequential Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#10-robotics-benchmarks class=md-nav__link> <span class=md-ellipsis> 10. Robotics Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#11-scientific-ai-benchmarks class=md-nav__link> <span class=md-ellipsis> 11. Scientific AI Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#12-generalization-benchmarks class=md-nav__link> <span class=md-ellipsis> 12. Generalization Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#13-few-shot-and-zero-shot-benchmarks class=md-nav__link> <span class=md-ellipsis> 13. Few-Shot and Zero-Shot Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#14-explainability-benchmarks class=md-nav__link> <span class=md-ellipsis> 14. Explainability Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#15-continuous-learning-lifelong-learning-benchmarks class=md-nav__link> <span class=md-ellipsis> 15. Continuous Learning (Lifelong Learning) Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#16-multi-agent-and-collaboration-benchmarks class=md-nav__link> <span class=md-ellipsis> 16. Multi-Agent and Collaboration Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#17-energy-and-carbon-efficiency-benchmarks class=md-nav__link> <span class=md-ellipsis> 17. Energy and Carbon Efficiency Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#18-safety-benchmarks class=md-nav__link> <span class=md-ellipsis> 18. Safety Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#19-creativity-and-generative-ai-benchmarks class=md-nav__link> <span class=md-ellipsis> 19. Creativity and Generative AI Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#20-alignment-and-intent-understanding-benchmarks class=md-nav__link> <span class=md-ellipsis> 20. Alignment and Intent Understanding Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#21-knowledge-representation-and-reasoning-benchmarks class=md-nav__link> <span class=md-ellipsis> 21. Knowledge Representation and Reasoning Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#22-specialized-benchmarks-for-emerging-domains class=md-nav__link> <span class=md-ellipsis> 22. Specialized Benchmarks for Emerging Domains </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#computer-use-and-browser-use-benchmarks class=md-nav__link> <span class=md-ellipsis> Computer Use and Browser Use Benchmarks </span> </a> <nav class=md-nav aria-label="Computer Use and Browser Use Benchmarks"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#23-computer-use-benchmarks class=md-nav__link> <span class=md-ellipsis> 23 Computer Use Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=#24-browser-use-benchmarks class=md-nav__link> <span class=md-ellipsis> 24. Browser Use Benchmarks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#the-most-popular-benechmarks-used-by-researchers-and-industry class=md-nav__link> <span class=md-ellipsis> The Most Popular Benechmarks Used by Researchers and Industry </span> </a> </li> <li class=md-nav__item> <a href=#what-is-leaderboard class=md-nav__link> <span class=md-ellipsis> What is Leaderboard? </span> </a> <nav class=md-nav aria-label="What is Leaderboard?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-are-popular-leaderboard class=md-nav__link> <span class=md-ellipsis> What are popular Leaderboard? </span> </a> </li> <li class=md-nav__item> <a href=#huggingface-hosted-leaderboards class=md-nav__link> <span class=md-ellipsis> Huggingface Hosted Leaderboards </span> </a> </li> <li class=md-nav__item> <a href=#githubio-hosted-leaderboards class=md-nav__link> <span class=md-ellipsis> Github.io Hosted Leaderboards </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#paperswithcode-hosted-leaderboards class=md-nav__link> <span class=md-ellipsis> Paperswithcode Hosted Leaderboards </span> </a> </li> <li class=md-nav__item> <a href=#chatbot-arena-formerly-lmsys-leadboards class=md-nav__link> <span class=md-ellipsis> Chatbot Arena (formerly LMSYS) Leadboards: </span> </a> </li> <li class=md-nav__item> <a href=#miscellaneous-leaderboards class=md-nav__link> <span class=md-ellipsis> Miscellaneous Leaderboards </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <article class="md-content__inner md-typeset"> <p><img alt="Exploring AI Benchmarks & Leaderboards" src=../assets/images/dspost/dsp6210-Exploring-AI-Benchmarks-and-Leaderboards.jpg></p> <h1 id=exploring-ai-benchmarks-leaderboards>Exploring AI Benchmarks &amp; Leaderboards<a class=headerlink href=#exploring-ai-benchmarks-leaderboards title="Permanent link">&para;</a></h1> <h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h2> <p>A <strong>benchmark</strong> is a standardized test or set of metrics used to measure and compare the performance, capabilities, or quality of systems, models, or algorithms. In the context of <strong>AI and machine learning</strong>, benchmarks provide a way to evaluate how well models perform on specific tasks or datasets, often with respect to predefined metrics like accuracy, speed, robustness, or resource efficiency. </p> <h3 id=why-do-we-need-benchmarks><strong>Why do we need Benchmarks?</strong><a class=headerlink href=#why-do-we-need-benchmarks title="Permanent link">&para;</a></h3> <ul> <li><strong>Standardization</strong>: Benchmarks define a consistent set of tasks, datasets, or metrics, ensuring comparability across different systems or models. </li> <li><strong>Reproducibility</strong>: Results from benchmarks are replicable by others using the same conditions and configurations. </li> <li><strong>Metrics</strong>: Benchmarks provide clear metrics (e.g., accuracy, F1-score, latency) for evaluation. </li> <li><strong>Domain-Specific</strong>: Benchmarks can be tailored to specific tasks or domains (e.g., NLP, computer vision, robotics). </li> <li><strong>Progress Measurement</strong>: To track advancements in AI over time. </li> <li><strong>Innovation Incentive</strong>: To encourage researchers and developers to design better models that surpass existing benchmarks. </li> </ul> <h3 id=what-are-the-components-of-an-ai-benchmark><strong>What are the Components of an AI Benchmark?</strong><a class=headerlink href=#what-are-the-components-of-an-ai-benchmark title="Permanent link">&para;</a></h3> <ol> <li><strong>Dataset</strong>: A collection of data used for training, validation, or testing. Examples: <a href=https://www.image-net.org/ >ImageNet</a>, <a href=https://rajpurkar.github.io/SQuAD-explorer/ >SQuAD</a>, <a href=https://gluebenchmark.com/ >GLUE</a>. </li> <li><strong>Tasks</strong>: Specific problems the model needs to solve, such as classification, translation, or question answering. </li> <li><strong>Metrics</strong>: Quantitative measures for evaluation (e.g., precision, recall, BLEU score). </li> <li><strong>Baselines</strong>: Pre-existing results or models to compare against (e.g., human performance or older algorithms). </li> </ol> <h3 id=types-of-benchmarks><strong>Types of Benchmarks</strong><a class=headerlink href=#types-of-benchmarks title="Permanent link">&para;</a></h3> <p>At high levels, benchmarks can be classified into <strong>performance</strong>, <strong>robustness</strong>, <strong>efficiency</strong>, and <strong>ethics and fairness</strong>.<br> 1. <strong>Performance Benchmarks</strong>: Evaluate how well a model performs a specific task (e.g., accuracy in classification tasks).<br> 2. <strong>Robustness Benchmarks</strong>: Test how models perform under challenging conditions, such as noise, adversarial inputs, or distribution shifts.<br> 3. <strong>Efficiency Benchmarks</strong>: Measure resource usage, such as computation time, memory, or energy consumption.<br> 4. <strong>Ethics and Fairness Benchmarks</strong>: Assess whether a model is fair and unbiased across demographic groups. </p> <h2 id=what-are-widely-known-benchmarks><strong>What are widely known Benchmarks?</strong><a class=headerlink href=#what-are-widely-known-benchmarks title="Permanent link">&para;</a></h2> <p>Each benchmark mentioned after this can be either <strong>performance</strong>, <strong>robustness</strong>, <strong>efficiency</strong>, or <strong>ethics and fairness</strong> benchmark. Benchmarks cover a wide variety of tasks and domains, addressing different aspects of model performance, usability, and impact. As AI evolves, new benchmarks continue to emerge, reflecting advances in technology and shifting societal priorities. </p> <h3 id=1-natural-language-processing-nlp-benchmarks>1. <strong>Natural Language Processing (NLP) Benchmarks</strong><a class=headerlink href=#1-natural-language-processing-nlp-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks for tasks like text classification, machine translation, question answering, summarization, and more. </p> <p><strong>Examples</strong>:<br> - <strong>GLUE (General Language Understanding Evaluation):</strong> Evaluates natural language understanding on tasks like sentiment analysis, textual entailment, and more. <a href=https://gluebenchmark.com/ >GLUE</a><br> - <strong>SuperGLUE:</strong> A more challenging version of GLUE. <a href=https://super.gluebenchmark.com/ >SuperGLUE</a><br> - <strong>XTREME:</strong> Evaluates multilingual models on tasks like question answering, named entity recognition, and sentence retrieval. <a href=https://sites.research.google/xtreme/ >XTREME</a><br> - <strong>SQuAD (Stanford Question Answering Dataset):</strong> Measures performance in machine reading comprehension. <a href=https://rajpurkar.github.io/SQuAD-explorer/ >SQuAD</a> </p> <h3 id=2-computer-vision-benchmarks>2. <strong>Computer Vision Benchmarks</strong><a class=headerlink href=#2-computer-vision-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks for tasks such as image classification, object detection, segmentation, and more. </p> <p><strong>Examples</strong>:<br> - <strong>ImageNet:</strong> A dataset for image classification and object detection. <a href=https://www.image-net.org/ >ImageNet</a><br> - <strong>COCO (Common Objects in Context):</strong> Used for object detection, segmentation, and captioning. <a href=https://cocodataset.org/ >COCO</a><br> - <strong>OpenImages:</strong> A dataset for large-scale object detection and segmentation. <a href=https://storage.googleapis.com/openimages/web/index.html>OpenImages</a><br> - <strong>CIFAR-10/100:</strong> Used for small-scale image classification. <a href=https://www.cs.toronto.edu/~kriz/cifar.html>CIFAR</a><br> - <strong>Cityscapes:</strong> Focused on urban scene segmentation. <a href=https://www.cityscapes-dataset.com/ >Cityscapes</a> </p> <h3 id=3-speech-and-audio-benchmarks>3. <strong>Speech and Audio Benchmarks</strong><a class=headerlink href=#3-speech-and-audio-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks for speech recognition, speaker verification, and sound classification. </p> <p><strong>Examples</strong>:<br> - <strong>LibriSpeech:</strong> Used for speech recognition tasks. <a href=https://www.openslr.org/12/ >LibriSpeech</a><br> - <strong>VoxCeleb:</strong> A dataset for speaker recognition and verification. <a href=https://www.robots.ox.ac.uk/~vgg/data/voxceleb/ >VoxCeleb</a><br> - <strong>TIMIT:</strong> Used for phoneme recognition. <a href=https://catalog.ldc.upenn.edu/LDC93S1>TIMIT</a><br> - <strong>ESC-50:</strong> For environmental sound classification. <a href=https://github.com/karolpiczak/ESC-50>ESC-50</a> </p> <h3 id=4-reinforcement-learning-benchmarks>4. <strong>Reinforcement Learning Benchmarks</strong><a class=headerlink href=#4-reinforcement-learning-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks for evaluating performance on tasks involving sequential decision-making and control. </p> <p><strong>Examples</strong>:<br> - <strong>OpenAI Gym:</strong> A collection of environments for RL algorithms, such as CartPole and Atari games. <a href=https://www.gymlibrary.dev/ >OpenAI Gym</a><br> - <strong>MuJoCo:</strong> A physics engine for robotics and continuous control tasks. <a href=https://mujoco.org/ >MuJoCo</a><br> - <strong>DeepMind Control Suite:</strong> Focused on simulated control tasks. <a href=https://github.com/deepmind/dm_control>DeepMind Control Suite</a><br> - <strong>StarCraft II Learning Environment (SC2LE):</strong> For real-time strategy game learning. <a href=https://github.com/deepmind/pysc2>SC2LE</a> </p> <h3 id=5-generative-ai-benchmarks>5. <strong>Generative AI Benchmarks</strong><a class=headerlink href=#5-generative-ai-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks for tasks like text-to-image generation, style transfer, and music generation. </p> <p><strong>Examples</strong>:<br> - <strong>MS COCO Captioning Challenge:</strong> Evaluates text-to-image generation models. <a href=https://cocodataset.org/#captions-2015>MS COCO</a><br> - <strong>FID (Fréchet Inception Distance):</strong> Measures the quality of generated images. <a href=https://github.com/mseitzer/pytorch-fid>FID</a><br> - <strong>ChatGPT Eval:</strong> Measures the performance of generative conversational agents. <a href=https://github.com/openai/chatgpt-eval>ChatGPT Eval</a><br> - <strong>BLEU and ROUGE:</strong> Evaluate text generation tasks such as summarization and translation. <a href=https://en.wikipedia.org/wiki/BLEU>BLEU</a>, <a href=https://en.wikipedia.org/wiki/ROUGE_(metric)>ROUGE</a> </p> <h3 id=6-multimodal-benchmarks>6. <strong>Multimodal Benchmarks</strong><a class=headerlink href=#6-multimodal-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks that evaluate models capable of handling multiple data types, like text, images, and video. </p> <p><strong>Examples</strong>:<br> - <strong>Visual Question Answering (VQA):</strong> Combines image and text understanding. <a href=https://visualqa.org/ >VQA</a><br> - <strong>Image-Text Retrieval (Flickr30k, MS COCO):</strong> Aligns images with text captions. <a href=https://shannon.cs.illinois.edu/DenotationGraph/ >Flickr30k</a>, <a href=https://cocodataset.org/#captions-2015>MS COCO</a><br> - <strong>CLIP Benchmark:</strong> Evaluates zero-shot image classification using multimodal models. <a href=https://github.com/openai/CLIP>CLIP</a><br> - <strong>MMBench</strong>: Tests models on tasks requiring integration of multiple data modalities. <a href=https://github.com/open-mmlab/mmbench>MMBench</a><br> - <strong>FLAVA Tasks</strong>: Benchmarks for vision and language alignment in multi-modal models. <a href=https://github.com/facebookresearch/flava>FLAVA</a> </p> <h3 id=7-ethics-and-fairness-benchmarks>7. <strong>Ethics and Fairness Benchmarks</strong><a class=headerlink href=#7-ethics-and-fairness-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks for measuring bias, fairness, and robustness of models. </p> <p><strong>Examples</strong>:<br> - <strong>FairFace:</strong> A dataset for evaluating bias in facial recognition. <a href=https://github.com/dchen236/FairFace>FairFace</a><br> - <strong>Datasheets for Datasets:</strong> Provides guidelines for dataset documentation to improve transparency. <a href=https://arxiv.org/abs/1803.09010>Datasheets</a><br> - <strong>Gender Shades:</strong> Measures bias in gender classification systems. <a href=http://gendershades.org/ >Gender Shades</a> </p> <h3 id=8-general-ai-agi-benchmarks>8. <strong>General AI (AGI) Benchmarks</strong><a class=headerlink href=#8-general-ai-agi-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks for evaluating models that aim to generalize across diverse tasks. </p> <p><strong>Examples</strong>:<br> - <strong>BIG-Bench (Beyond the Imitation Game Benchmark):</strong> Evaluates language models on tasks requiring reasoning, comprehension, and knowledge. <a href=https://github.com/google/BIG-bench>BIG-Bench</a><br> - <strong>ARC (AI2 Reasoning Challenge):</strong> Tests commonsense and scientific reasoning. <a href=https://allenai.org/data/arc>ARC</a><br> - <strong>HumanEval:</strong> Evaluates models on code generation tasks. <a href=https://github.com/openai/human-eval>HumanEval</a> </p> <h3 id=9-temporal-and-sequential-benchmarks>9. <strong>Temporal and Sequential Benchmarks</strong><a class=headerlink href=#9-temporal-and-sequential-benchmarks title="Permanent link">&para;</a></h3> <p>Evaluate models on tasks involving time-series or sequential data. </p> <p><strong>Examples</strong>:<br> - <strong>MuJoCo Physics Simulation</strong>: Temporal reasoning and decision-making in physical simulations. <a href=https://mujoco.org/ >MuJoCo</a><br> - <strong>M4 Competition Dataset:</strong> For forecasting time series. <a href=https://www.m4.unic.ac.cy/ >M4</a><br> - <strong>UCR Time Series Classification Archive:</strong> A comprehensive benchmark for time series classification tasks. <a href=https://www.cs.ucr.edu/~eamonn/time_series_data_2018/ >UCR</a><br> - <strong>Electricity and Traffic:</strong> Common datasets used in forecasting and anomaly detection. <a href=https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014>Electricity</a>, <a href=https://archive.ics.uci.edu/ml/datasets/Dodgers+Loop+Sensor>Traffic</a> </p> <h3 id=10-robotics-benchmarks>10. <strong>Robotics Benchmarks</strong><a class=headerlink href=#10-robotics-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks for evaluating performance in robotic manipulation, navigation, and control. </p> <p><strong>Examples</strong>:<br> - <strong>RoboSuite:</strong> Focused on robotic manipulation. <a href=https://robosuite.ai/ >RoboSuite</a><br> - <strong>Habitat:</strong> A simulator for embodied AI tasks like navigation and object interaction. <a href=https://aihabitat.org/ >Habitat</a><br> - <strong>Fetch Benchmark:</strong> Used for robotic grasping tasks. <a href=https://fetchrobotics.com/ >Fetch</a> </p> <h3 id=11-scientific-ai-benchmarks>11. <strong>Scientific AI Benchmarks</strong><a class=headerlink href=#11-scientific-ai-benchmarks title="Permanent link">&para;</a></h3> <p>Benchmarks for AI models in scientific applications such as biology, chemistry, and physics. </p> <p><strong>Examples</strong>:<br> - <strong>AlphaFold Dataset:</strong> For protein structure prediction. <a href=https://alphafold.ebi.ac.uk/ >AlphaFold</a><br> - <strong>QM9:</strong> A dataset for molecular property prediction. <a href=https://www.nature.com/articles/sdata201422>QM9</a><br> - <strong>Physics Simulations (DeepMind Simulations):</strong> For evaluating models on physical interactions and properties. <a href=https://www.deepmind.com/ >DeepMind</a> </p> <h3 id=12-generalization-benchmarks>12. <strong>Generalization Benchmarks</strong><a class=headerlink href=#12-generalization-benchmarks title="Permanent link">&para;</a></h3> <p>Test how well models can generalize to unseen data, tasks, or domains. </p> <p><strong>Examples</strong>:<br> - <strong>WILDS</strong>: Evaluates models on real-world distribution shifts across domains like healthcare and satellite imagery. <a href=https://wilds.stanford.edu/ >WILDS</a><br> - <strong>DomainNet</strong>: Assesses domain adaptation and generalization for image classification across different styles (e.g., photos, sketches). <a href=http://ai.bu.edu/DomainNet/ >DomainNet</a><br> - <strong>Meta-Dataset</strong>: Evaluates few-shot learning and generalization across diverse datasets. <a href=https://github.com/google-research/meta-dataset>Meta-Dataset</a> </p> <h3 id=13-few-shot-and-zero-shot-benchmarks>13. <strong>Few-Shot and Zero-Shot Benchmarks</strong><a class=headerlink href=#13-few-shot-and-zero-shot-benchmarks title="Permanent link">&para;</a></h3> <p>Assess models’ ability to perform tasks with limited or no prior examples. </p> <p><strong>Examples</strong>:<br> - <strong>LEGOEval</strong>: Few-shot NLP tasks like classification and translation. <a href=https://github.com/allenai/LEGOEval>LEGOEval</a><br> - <strong>CrossFit</strong>: Benchmarks for cross-task few-shot generalization. <a href=https://github.com/IBM/crossfit>CrossFit</a><br> - <strong>Natural Instructions</strong>: Evaluates zero-shot task adaptation across natural language instructions. <a href=https://github.com/allenai/natural-instructions>Natural Instructions</a> </p> <h3 id=14-explainability-benchmarks>14. <strong>Explainability Benchmarks</strong><a class=headerlink href=#14-explainability-benchmarks title="Permanent link">&para;</a></h3> <p>Measure how interpretable and explainable an AI model’s outputs or decisions are to humans. </p> <p><strong>Examples</strong>:<br> - <strong>ARRIVE</strong>: Focuses on explainability in reinforcement learning. <a href=https://arxiv.org/abs/2107.06299>ARRIVE</a><br> - <strong>ExplainBoard</strong>: Evaluates explainability in NLP models. <a href=https://github.com/neulab/ExplainBoard>ExplainBoard</a><br> - <strong>FACT Benchmark</strong>: Measures the fidelity and consistency of explainability methods for machine learning models. <a href=https://github.com/interpretml/interpret>FACT</a> </p> <h3 id=15-continuous-learning-lifelong-learning-benchmarks>15. <strong>Continuous Learning (Lifelong Learning) Benchmarks</strong><a class=headerlink href=#15-continuous-learning-lifelong-learning-benchmarks title="Permanent link">&para;</a></h3> <p>Measure models' ability to learn new tasks without forgetting previously learned ones. </p> <p><strong>Examples</strong>:<br> - <strong>CLBenchmark</strong>: Evaluates continual learning in classification and regression tasks. <a href=https://github.com/ContinualAI/continual-learning-baselines>CLBenchmark</a><br> - <strong>CLEAR</strong>: Tests continual reinforcement learning in dynamic environments. <a href=https://github.com/clear-benchmark/clear>CLEAR</a><br> - <strong>Split CIFAR-100</strong>: Assesses lifelong learning in image classification. <a href=https://github.com/ContinualAI/avalanche>Split CIFAR-100</a> </p> <h3 id=16-multi-agent-and-collaboration-benchmarks>16. <strong>Multi-Agent and Collaboration Benchmarks</strong><a class=headerlink href=#16-multi-agent-and-collaboration-benchmarks title="Permanent link">&para;</a></h3> <p>Test models on tasks requiring collaboration, communication, or competition between agents. </p> <p><strong>Examples</strong>:<br> - <strong>StarCraft II Multi-Agent Challenge (SMAC):</strong> Evaluates multi-agent coordination strategies. <a href=https://github.com/oxwhirl/smac>SMAC</a><br> - <strong>Overcooked-AI:</strong> Benchmarks for human-AI collaboration in cooperative tasks. <a href=https://github.com/HumanCompatibleAI/overcooked_ai>Overcooked-AI</a><br> - <strong>Magent:</strong> A multi-agent environment for reinforcement learning. <a href=https://github.com/geek-ai/Magent>Magent</a> </p> <h3 id=17-energy-and-carbon-efficiency-benchmarks>17. <strong>Energy and Carbon Efficiency Benchmarks</strong><a class=headerlink href=#17-energy-and-carbon-efficiency-benchmarks title="Permanent link">&para;</a></h3> <p>Focus on the environmental impact of training and deploying AI models. </p> <p><strong>Examples</strong>:<br> - <strong>Carbontracker:</strong> Tracks energy usage and carbon emissions of AI systems. <a href=https://github.com/lfwa/carbontracker>Carbontracker</a><br> - <strong>GreenAI Benchmarks:</strong> Encourages the development of energy-efficient AI systems. <a href=https://github.com/GreenAI-NYUAD/GreenAI>GreenAI</a><br> - <strong>MLPerf Power Benchmark:</strong> Measures energy consumption during model training and inference. <a href=https://mlcommons.org/en/ >MLPerf</a> </p> <h3 id=18-safety-benchmarks>18. <strong>Safety Benchmarks</strong><a class=headerlink href=#18-safety-benchmarks title="Permanent link">&para;</a></h3> <p>Test the reliability and safety of AI systems under real-world constraints. </p> <p><strong>Examples</strong>:<br> - <strong>SafeLife:</strong> A benchmark for safe exploration in reinforcement learning. <a href=https://github.com/PartnershipOnAI/safelife>SafeLife</a><br> - <strong>Safety Gym:</strong> Evaluates safe navigation and control in simulated environments. <a href=https://github.com/openai/safety-gym>Safety Gym</a><br> - <strong>Adversarial Robustness Toolbox:</strong> Tests how models handle adversarial attacks while ensuring safety. <a href=https://github.com/Trusted-AI/adversarial-robustness-toolbox>ART</a> </p> <h3 id=19-creativity-and-generative-ai-benchmarks>19. <strong>Creativity and Generative AI Benchmarks</strong><a class=headerlink href=#19-creativity-and-generative-ai-benchmarks title="Permanent link">&para;</a></h3> <p>Evaluate models' ability to generate creative or novel outputs in text, images, or other formats. </p> <p><strong>Examples</strong>:<br> - <strong>MS-COCO Captions:</strong> A benchmark for image caption generation. <a href=https://cocodataset.org/#captions-2015>MS-COCO</a><br> - <strong>Story Cloze Test:</strong> Tests the ability to generate plausible endings for short stories. <a href=https://github.com/uclanlp/StoryCloze>Story Cloze</a><br> - <strong>GauGAN:</strong> Benchmarks for creative AI in image synthesis. <a href=https://github.com/NVlabs/SPADE>GauGAN</a> </p> <h3 id=20-alignment-and-intent-understanding-benchmarks>20. <strong>Alignment and Intent Understanding Benchmarks</strong><a class=headerlink href=#20-alignment-and-intent-understanding-benchmarks title="Permanent link">&para;</a></h3> <p>Measure how well models align with human values, goals, or intentions. </p> <p><strong>Examples</strong>:<br> - <strong>Anthropic’s HH-RLHF:</strong> Evaluates alignment with human feedback in reinforcement learning tasks. <a href=https://github.com/anthropics/hh-rlhf>HH-RLHF</a><br> - <strong>BIG-Bench (Beyond the Imitation Game):</strong> Includes alignment-focused tasks, such as ethical reasoning and understanding intent. <a href=https://github.com/google/BIG-bench>BIG-Bench</a><br> - <strong>REALM:</strong> Measures retrieval-augmented language model alignment with queries. <a href=https://github.com/google-research/realm>REALM</a> </p> <h3 id=21-knowledge-representation-and-reasoning-benchmarks>21. <strong>Knowledge Representation and Reasoning Benchmarks</strong><a class=headerlink href=#21-knowledge-representation-and-reasoning-benchmarks title="Permanent link">&para;</a></h3> <p>Test a model’s ability to understand, manipulate, and reason with structured knowledge. </p> <p><strong>Examples</strong>:<br> - <strong>OpenBookQA:</strong> Evaluates reasoning using common-sense and scientific facts. <a href=https://allenai.org/data/openbook-qa>OpenBookQA</a><br> - <strong>ConceptNet Benchmark:</strong> Tests common-sense reasoning and knowledge graphs. <a href=https://github.com/commonsense/conceptnet5>ConceptNet</a><br> - <strong>ATOMIC:</strong> Assesses models for inferential knowledge about everyday events. <a href=https://homes.cs.washington.edu/~msap/atomic/ >ATOMIC</a> </p> <h3 id=22-specialized-benchmarks-for-emerging-domains>22. <strong>Specialized Benchmarks for Emerging Domains</strong><a class=headerlink href=#22-specialized-benchmarks-for-emerging-domains title="Permanent link">&para;</a></h3> <p>Benchmarks are also emerging in highly specialized areas like quantum computing, space exploration, and neuroscience. </p> <p><strong>Examples</strong>:<br> - <strong>Quantum ML Benchmarks:</strong> For evaluating quantum-enhanced machine learning algorithms. <a href=https://github.com/quantumlib/Cirq>Quantum ML</a><br> - <strong>SpaceNet:</strong> A benchmark for satellite imagery analysis. <a href=https://spacenet.ai/ >SpaceNet</a><br> - <strong>NeuroBench:</strong> Tests AI systems for neuroscience applications. <a href=https://github.com/neurobench/neurobench>NeuroBench</a> </p> <h2 id=computer-use-and-browser-use-benchmarks>Computer Use and Browser Use Benchmarks<a class=headerlink href=#computer-use-and-browser-use-benchmarks title="Permanent link">&para;</a></h2> <p><strong>"Computer Use"</strong> and <strong>"Browser Use"</strong> are designed for human-computer interaction (HCI), automation, and web-based tasks. These are Interaction Benchmarks. These benchmarks are primarily aimed at testing models or agents for their ability to perform <strong>interactive tasks</strong> involving user interfaces, browsers, or other digital tools. Here's an overview: </p> <p>These benchmarks are key for advancing AI systems capable of seamlessly interacting with digital tools, paving the way for highly capable personal assistants, RPA systems, and adaptive agents. </p> <p><strong>Evaluation Metrics for Computer/Browser Use Benchmarks</strong> <br> - <strong>Task Completion Rate</strong>: Percentage of tasks completed successfully.<br> - <strong>Error Rate</strong>: Frequency of errors (e.g., incorrect clicks or invalid entries).<br> - <strong>Time to Completion</strong>: The time taken to complete the task.<br> - <strong>Efficiency and Resource Usage</strong>: Particularly for browser performance.<br> - <strong>Human-Like Interaction</strong>: Measures how closely the AI’s actions align with typical human behaviors. </p> <h3 id=23-computer-use-benchmarks>23 <strong>Computer Use Benchmarks</strong><a class=headerlink href=#23-computer-use-benchmarks title="Permanent link">&para;</a></h3> <p>These benchmarks evaluate AI systems for their ability to interact with traditional desktop or mobile applications, including file management, text editing, and other GUI-based tasks. </p> <p><strong>Examples:</strong><br> - <strong>HUMAN-AI Interaction Benchmarks</strong>:<br> Evaluates AI assistants in assisting humans with tasks like email management, file organization, or using desktop applications. <a href=https://github.com/HumanCompatibleAI/human-ai-interaction>HUMAN-AI</a><br> - <strong>Virtual Desktop Environments</strong>:<br> - <strong>MetaWorld:</strong> A virtual environment for reinforcement learning where AI agents perform computer-use tasks, such as dragging and dropping files or using apps. <a href=https://github.com/rlworkgroup/metaworld>MetaWorld</a><br> - <strong>RoboDesk:</strong> Benchmarks for robotic systems performing desktop-level tasks. <a href=https://github.com/robodesk/robodesk>RoboDesk</a><br> - <strong>MiniWoB++ (Mini World of Bits):</strong><br> A suite of web-based UI tasks designed for testing AI systems on basic computer interactions like clicking buttons, filling forms, or selecting options from menus. <a href=https://github.com/stanfordnlp/miniwob-plusplus>MiniWoB++</a><br> - <strong>User Interface Interaction Datasets:</strong><br> Benchmarks like the <strong>ClickMe Dataset</strong> track user interactions with buttons, icons, and forms in GUI-based settings. <a href=https://github.com/ClickMe-Dataset/ClickMe>ClickMe</a> </p> <h3 id=24-browser-use-benchmarks>24. <strong>Browser Use Benchmarks</strong><a class=headerlink href=#24-browser-use-benchmarks title="Permanent link">&para;</a></h3> <p>These benchmarks are designed for tasks involving web browsers, such as form filling, navigation, web scraping, or multi-step workflows (e.g., booking a flight or ordering a product online). </p> <p><strong>Examples:</strong><br> - <strong>WebGPT Benchmarks:</strong><br> Evaluates AI models that search the web and extract or summarize relevant information to answer user queries. <a href=https://github.com/openai/webgpt>WebGPT</a><br> - <strong>BrowserBench:</strong><br> Benchmarks designed to evaluate browser engines' performance in handling tasks like rendering, navigation, and resource loading. (E.g., <strong>Speedometer</strong>, <strong>JetStream</strong>, <strong>MotionMark</strong>). These are more about browser performance than AI but are indirectly relevant. <a href=https://browserbench.org/ >BrowserBench</a><br> - <strong>MiniWoB++ for Web Tasks:</strong><br> Includes tasks like navigating through webpages, clicking specific elements, or extracting data from websites. <a href=https://github.com/stanfordnlp/miniwob-plusplus>MiniWoB++</a><br> - <strong>OpenAI's WebGPT:</strong><br> Benchmarks assessing an agent's ability to use a browser for tasks like multi-step searches, citing sources, or reasoning across multiple pages. <a href=https://github.com/openai/webgpt>WebGPT</a><br> - <strong>Browser Automation Benchmarks (RPA):</strong><br> - Datasets and tools from robotic process automation (RPA) platforms like <strong>SikuliX</strong> or <strong>UiPath</strong>, which evaluate the performance of agents automating browser-based workflows. <a href=https://sikulix.github.io/ >SikuliX</a>, <a href=https://www.uipath.com/ >UiPath</a><br> - <strong>DeepMind's Alphacode Web Automation Tasks:</strong><br> A set of benchmarks testing AI for automating workflows within web-based environments. <a href=https://github.com/deepmind/alphacode>Alphacode</a> </p> <h2 id=the-most-popular-benechmarks-used-by-researchers-and-industry>The Most Popular Benechmarks Used by Researchers and Industry<a class=headerlink href=#the-most-popular-benechmarks-used-by-researchers-and-industry title="Permanent link">&para;</a></h2> <ol> <li><strong>AGIEval</strong>: A suite of human-centric exams (e.g., SAT, LSAT, GRE) to evaluate reasoning and knowledge in academic contexts.</li> <li><strong>APPS</strong>: Automated Programming Problem Set, with 10,000 coding problems ranging from introductory to competition-level, testing code generation and correctness.</li> <li><strong>ARC (AI2 Reasoning Challenge)</strong>: A set of grade-school science questions requiring reasoning over facts, split into Easy and Challenge sets. The Challenge set is especially tough, with human-level performance still elusive for most models.</li> <li><strong>BIG-Bench</strong>: A massive, collaborative benchmark with over 200 diverse tasks, from linguistics to math to commonsense reasoning. It’s designed to push LLMs beyond their training data and includes a "Hard" subset (BIG-Bench Hard, or BBH) for extra challenge.</li> <li><strong>BoolQ</strong>: A yes/no question-answering benchmark derived from Google search queries, assessing reading comprehension and reasoning over passages.</li> <li><strong>C-Eval</strong>: A Chinese-language benchmark testing reasoning and knowledge across subjects like STEM, humanities, and professional fields, similar to MMLU but tailored for Chinese LLMs.</li> <li><strong>Chinese SimpleQA</strong>: A factual accuracy test with concise, fact-based questions in Chinese, designed to assess straightforward knowledge recall.</li> <li><strong>CLUEWSC</strong>: Part of the Chinese Language Understanding Evaluation (CLUE), this is a Winograd Schema Challenge variant testing coreference resolution and commonsense reasoning in Chinese.</li> <li><strong>CMath</strong>: A Chinese math benchmark, possibly focusing on advanced or competition-level problems, testing structured reasoning in a Chinese context.</li> <li><strong>CMMLU</strong>: Chinese Massive Multitask Language Understanding, a broad evaluation of knowledge and reasoning in Chinese across 57 subjects, akin to MMLU but culturally and linguistically specific.</li> <li><strong>CNMO2024</strong>: Likely the Chinese National Mathematical Olympiad 2024, a high-level math competition dataset used to evaluate advanced problem-solving skills.</li> <li><strong>COCO (Common Objects in Context)</strong>: Another vision benchmark, but focused on tasks like object detection, segmentation, and captioning. It’s more about understanding scenes holistically, not just classifying single objects.</li> <li><strong>Codeforces/CodeJam</strong>: Competitive programming datasets from platforms like Codeforces or Google Code Jam, used informally to test advanced coding skills.</li> <li><strong>CoQA</strong>: Conversational Question Answering, a dataset of 127,000+ questions across 8,000 conversations, evaluating contextual understanding and dialogue coherence.</li> <li><strong>DROP</strong>: A reading comprehension benchmark requiring discrete reasoning over paragraphs, often involving numerical or logical deductions from text.</li> <li><strong>EvalPlus (HumanEval+ &amp; MBPP+)</strong>: Enhanced versions of HumanEval and MBPP with stricter test cases to catch edge cases and ensure robustness in code generation. -- <strong>GLUE</strong>: General Language Understanding Evaluation, a collection of 9 tasks (e.g., sentiment analysis, textual entailment) to test natural language understanding. SuperGLUE is its harder successor.</li> <li><strong>GPQA</strong>: Graduate-Level Google-Proof Q&amp;A Benchmark, featuring 448 challenging multiple-choice questions in biology, physics, and chemistry. Designed to be difficult even for PhD experts (65% accuracy) and resistant to simple web searches.</li> <li><strong>GSM8K</strong>: Grade School Math 8K (if not what you meant by GSMBK), 8,000 math word problems requiring multi-step reasoning, popular for testing logical skills.</li> <li><strong>HellaSwag</strong>: A commonsense reasoning benchmark where models pick the most plausible ending to a story or scenario. It’s tricky because it requires understanding context and human-like intuition, not just pattern matching.</li> <li><strong>HumanEval</strong>: For code generation, this benchmark checks if a model can write functioning Python code to solve programming problems. It’s practical and directly tied to real-world utility in software development.</li> <li><strong>ImageNet</strong>: For vision models, this is the classic. It’s a huge dataset of labeled images used to test object recognition accuracy. Though it’s been around a while, it’s still a foundational metric for computer vision.</li> <li><strong>LAMBADA</strong>: Tests long-range dependency understanding by predicting the last word in a passage, requiring coherence over extended text.</li> <li><strong>LiveCodeBench</strong>: A coding benchmark with fresh, real-world problems (e.g., from LeetCode, Codeforces) to evaluate code generation, repair, and execution, updated regularly to avoid contamination.</li> <li><strong>MATH</strong>: A dataset of 12,500 free-response math problems from high school competitions, spanning algebra, calculus, and more—extremely challenging for LLMs.</li> <li><strong>Math-500</strong>: A dataset of 500 math problems, likely an updated or in-distribution version of the MATH benchmark, testing logical and quantitative reasoning from basic to advanced levels.</li> <li><strong>MBPP+</strong>: An enhanced version of the Mostly Basic Python Problems dataset, with around 1,000 entry-level programming tasks, including automated test cases for evaluation.</li> <li><strong>MMLU (Massive Multitask Language Understanding)</strong>: This benchmark tests a model’s ability to handle college-level questions across 57 subjects, from STEM to humanities. It’s become a staple for assessing general knowledge and reasoning in large language models.</li> <li><strong>MMLU-Pro</strong>: An advanced version of the Massive Multitask Language Understanding (MMLU) benchmark, with 12,000 complex, reasoning-focused questions across various disciplines (e.g., STEM, law, humanities). It uses 10 multiple-choice options instead of 4, making it tougher and less prone to random guessing.</li> <li><strong>PIQA</strong>: Physical Interaction QA, with 16,000 questions testing physical commonsense reasoning (e.g., "How do you open a jar?").</li> <li><strong>SQuAD (Stanford Question Answering Dataset)</strong>: A reading comprehension test where models answer questions based on a given passage. It’s a standard for evaluating how well AI can extract and interpret information from text.</li> <li><strong>SuperGLUE</strong>: An upgraded GLUE with more complex tasks like coreference resolution and question answering, designed to differentiate top-performing models.</li> <li><strong>TriviaQA</strong>: A large-scale trivia dataset with 650,000+ question-answer pairs, testing factual recall and reasoning over noisy web data.</li> <li><strong>TruthfulQA</strong>: Designed to measure how truthful a model is, this benchmark throws curveballs with questions that might trip up models prone to hallucination or overconfidence. It’s increasingly relevant as trustworthiness becomes a bigger focus.</li> <li><strong>WinoGrande</strong>: A larger-scale Winograd Schema Challenge (1,267 examples) testing commonsense reasoning through pronoun resolution in ambiguous sentences.</li> </ol> <h2 id=what-is-leaderboard>What is Leaderboard?<a class=headerlink href=#what-is-leaderboard title="Permanent link">&para;</a></h2> <p>An AI Leaderboard is a publicly available platform or tool that ranks AI models, systems, or algorithms based on their performance on predefined benchmarks or datasets. It acts as a scoreboard for comparing different approaches and identifying the current state-of-the-art (SOTA) methods in specific AI tasks or domains. When above discussed different benchmarks are used by model provider or model evaluator against a given model then the performance of the model is reported on the leader board. So, a leaderboard is </p> <ul> <li>Ranking Mechanism: Models are ranked according to their performance on a specific task, measured using metrics like accuracy, F1 score, BLEU, or others depending on the benchmark.</li> <li>Transparency: Leaderboards display detailed information about submitted models, including the methodology, configuration, and even code, fostering reproducibility and openness.</li> <li>Task-Specific: Each leaderboard is typically associated with a particular dataset or task, such as machine translation, image recognition, or reinforcement learning.</li> <li>Dynamic Updates: As new models are submitted, the rankings are updated, reflecting ongoing progress in the field.</li> <li>Community Engagement: Researchers and practitioners actively submit their models to compete for the top position, driving innovation and improvement.</li> </ul> <h3 id=what-are-popular-leaderboard>What are popular Leaderboard?<a class=headerlink href=#what-are-popular-leaderboard title="Permanent link">&para;</a></h3> <p>Here is a list of notable AI leaderboards and their purposes. Keep in mind, a leaderboard is as good as it is updated by the community and model builder. If a leaderboard is not seeing any activity in last 3-6 month time, it means there is better leaderboard in place of that and people are not using it to report the model performance. The leaderboard contains current progress of the model therefore they are hosted at some places. There are many hosting spaces where leaderboards are hosted and one of the famous is huggingface.</p> <h3 id=huggingface-hosted-leaderboards>Huggingface Hosted Leaderboards<a class=headerlink href=#huggingface-hosted-leaderboards title="Permanent link">&para;</a></h3> <ul> <li><a href=https://huggingface.co/spaces/ArtificialAnalysis/Text-to-Image-Leaderboard>Text To Image Leaderboard - ArtificialAnalysis HF space</a></li> <li><a href=https://huggingface.co/spaces/optimum/llm-perf-leaderboard>LLM-Perf Leaderboard - optimum HF space</a></li> <li><a href=https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard>Open LLM Leaderboard - open-llm-leaderboard HF space</a></li> <li><a href=https://huggingface.co/spaces/Krisseck/IFEval-Leaderboard>IFEval Leaderboard - Krisseck HF space</a></li> <li><a href=https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard>Chatbot Arena Leaderboard - lmarena-ai HF space</a></li> <li><a href=https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard>Chatbot Arena Leaderboard - lmsys HF space</a></li> <li><a href=https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard>Big Code Models Leaderboard - bigcode HF space</a></li> <li><a href=https://huggingface.co/spaces/EffiBench/effibench-leaderboard>EffiBench Leaderboard - EffiBench HF space</a></li> <li><a href=https://huggingface.co/collections/clefourrier/leaderboards-and-benchmarks-64f99d2e11e92ca5568a7cce>Leaderboards and benchmarks - a clefourrier HF Collection</a></li> <li><a href=https://huggingface.co/spaces/mteb/leaderboard>MTEB (Massive Text Embedding Benchmark) Leaderboard</a> : Evaluates text embedding models on tasks like classification, clustering, and retrieval. <strong>Key Metrics</strong>: Performance across multiple embedding tasks. </li> <li><a href=https://huggingface.co/spaces/AIEnergyScore/Leaderboard>AI Energy Score Leaderboard</a> : Evaluates AI models based on their energy efficiency and environmental impact. <strong>Key Metrics</strong>: Energy consumption (kWh), carbon emissions, and computational efficiency (FLOPs).</li> </ul> <h3 id=githubio-hosted-leaderboards>Github.io Hosted Leaderboards<a class=headerlink href=#githubio-hosted-leaderboards title="Permanent link">&para;</a></h3> <ul> <li><a href=https://evalplus.github.io/leaderboard.html>EvalPlus Leaderboard</a></li> <li><a href=https://bigcode-bench.github.io/ >BigCodeBench</a></li> <li><a href=https://github.com/amazon-science/cceval>CrossCodeEval</a></li> <li><a href=https://fudanselab-classeval.github.io/ >ClassEval</a></li> <li><a href=https://crux-eval.github.io/leaderboard.html>CRUXEval</a></li> <li><a href=https://codetlingua.github.io/leaderboard.html>Code Lingua</a></li> <li><a href=https://evo-eval.github.io/ >Evo-Eval</a></li> <li><a href=https://github.com/01-ai/HumanEval.jl>HumanEval.jl - Julia version HumanEval with EvalPlus test cases</a></li> <li><a href=https://livecodebench.github.io/leaderboard.html>LiveCodeBench</a></li> <li><a href=https://sparksofagi.github.io/MHPP/ >MHPP</a></li> <li><a href=https://github.com/THUDM/NaturalCodeBench>NaturalCodeBench</a></li> <li><a href=https://github.com/Leolty/repobench>RepoBench</a></li> <li><a href=https://llm4softwaretesting.github.io/ >LLM4 Software Testing - TestEval</a></li> <li><a href=https://github.com/sylinrl/TruthfulQA>TruthfulQA</a> : Measures the accuracy of LLMs in answering questions without generating misleading or false information. Truthfulness scores across 38 categories of questions.</li> <li><a href=https://github.com/evalplus/evalplus>HumanEval+</a> : Evaluates LLMs on programming tasks, focusing on code generation and debugging. <strong>Key Metrics</strong>: Accuracy and efficiency in coding tasks. </li> <li><a href=https://flageval.github.io/ >FlagEval</a> : A comprehensive platform for evaluating foundation models across multiple dimensions, including performance, safety, and efficiency. <strong>Key Metrics</strong>: Multi-dimensional evaluation scores. </li> </ul> <h2 id=paperswithcode-hosted-leaderboards>Paperswithcode Hosted Leaderboards<a class=headerlink href=#paperswithcode-hosted-leaderboards title="Permanent link">&para;</a></h2> <ul> <li><a href=https://paperswithcode.com/sota/common-sense-reasoning-on-arc-challenge>Common Sense Reasoning</a> : Assesses LLMs' ability to answer complex, science-based questions requiring deep reasoning and knowledge. <strong>Key Metrics</strong>: Accuracy on grade-school science questions. </li> <li><a href=https://paperswithcode.com/ >Papers With Code</a> : Links AI research papers with code and benchmarks, fostering transparency and reproducibility in machine learning. <strong>Key Metrics</strong>: State-of-the-art (SOTA) results across various tasks. </li> </ul> <h2 id=chatbot-arena-formerly-lmsys-leadboards>Chatbot Arena (formerly LMSYS) Leadboards:<a class=headerlink href=#chatbot-arena-formerly-lmsys-leadboards title="Permanent link">&para;</a></h2> <p>The LMSYS Chatbot Arena Leaderboard is a comprehensive ranking platform that assesses the performance of large language models (LLMs) in conversational tasks. It uses a combination of human feedback and automated scoring to evaluate models - <a href=https://lmarena.ai/?leaderboard>Chatbot Arena (formerly LMSYS): Free AI Chat to Compare &amp; Test Best AI Chatbots</a> - <a href=https://klu.ai/glossary/lmsys-leaderboard>LMSYS Chatbot Arena Leaderboard — Klu</a> - <a href=https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard>Chatbot Arena</a> : A crowdsourced platform where users compare LLMs in head-to-head battles, ranking models based on user satisfaction and conversational performance. <strong>Key Metrics</strong>: User ratings, win rates, and response quality. </p> <h2 id=miscellaneous-leaderboards>Miscellaneous Leaderboards<a class=headerlink href=#miscellaneous-leaderboards title="Permanent link">&para;</a></h2> <ul> <li><a href=https://klu.ai/llm-leaderboard>Klu.ai</a></li> <li><a href=https://livebench.ai/ >LiveBench</a></li> <li><a href=https://llm.extractum.io/list>extractum.io - OpenLLM Leaderboard</a></li> <li><a href=https://openlm.ai/chatbot-arena/ >OpenLM.ai - Chatbot Arena</a></li> <li><a href=https://aider.chat/docs/leaderboards>Aider.chat - LLM Leaderboards | aider</a></li> <li><a href=https://www.swebench.com/ >SWE-bench</a></li> <li><a href=https://leaderboard.tabbyml.com/ >TabbyML Leaderboard</a></li> <li><a href=https://artificialanalysis.ai/leaderboards/models>Open LLM Leaderboard</a> : Tracks and ranks open-source language models (LLMs) across various benchmarks, such as accuracy, reasoning, and commonsense understanding. <strong>Key Metrics</strong>: Quality, price, performance, and speed (tokens per second, latency). </li> <li><a href=https://arxiv.org/abs/2412.18551>Libra-Leaderboard</a> : Evaluates the safety and trustworthiness of LLMs, focusing on risks like misinformation, bias, and adversarial attacks. <strong>Key Metrics</strong>: Safety and capability balance, distance-to-optimal-score method. </li> <li><a href=https://leaderboard.allenai.org/arc/submissions/public>ARC Leaderboard</a> </li> <li><a href=https://rowanzellers.com/hellaswag/ >HellaSwag</a> : Evaluates commonsense reasoning in LLMs by testing their ability to complete sentences and scenarios. <strong>Key Metrics</strong>: Accuracy on commonsense reasoning tasks. </li> <li><a href=https://dynabench.org/ >Dynabench</a> : - Dynabench is a platform for dynamic dataset creation and benchmarking, focusing on evaluating AI models in real-world, adversarial, and evolving scenarios. They have dozens of leadboards for Text, Audio, Language, Code, Vision, Medical, <strong>Key Metrics</strong>: Human-and-model-in-the-loop evaluation, adversarial robustness, and generalization across tasks like NLP and vision.</li> <li><a href=https://leaderboards.allenai.org/ >Generative AI Leaderboards</a> : Tracks the performance of generative AI models, particularly in natural language generation, image synthesis, and other creative tasks. They have dozens of leaderboards for reasoning, robotics, agents, text, image, and video generation. <strong>Key Metrics</strong>: Perplexity, BLEU, ROUGE, FID (Fréchet Inception Distance), and human evaluation scores.</li> <li><a href=https://www.superclueai.com/ >SuperCLUE</a> : A Chinese AI evaluation benchmark focusing on large language models (LLMs) and their performance in Chinese language tasks. <strong>Key Metrics</strong>: Accuracy, fluency, and task-specific performance in Chinese NLP tasks.</li> </ul> <div class=author-bio style="margin-top: 2rem; display: flex; gap: 1rem;"> <img src=../assets/images/myphotos/Profilephoto1.jpg alt="Hari Thapliyaal" style="border-radius: 50%; width: 80px; height: 80px;"> <div> <strong>Dr. Hari Thapliyaal</strong><br> <small>Dr. Hari Thapliyal is a prolific blogger and seasoned professional with an extensive background in Data Science, Project Management, and Advait-Vedanta Philosophy. He holds a Doctorate in AI/NLP from SSBM, Geneva, along with Master’s degrees in Computers, Business Management, Data Science, and Economics. With over three decades of experience in management and leadership, Hari has extensive expertise in training, consulting, and coaching within the technology sector. His specializations include Data Science, AI, Computer Vision, NLP, and machine learning. Hari is also passionate about meditation and nature, often retreating to secluded places for reflection and peace.</small> </div> </div> <div class=share-buttons style="margin-top: 2rem;"> <strong>Share this article:</strong><br> <a href="https://twitter.com/intent/tweet?text=Exploring%20AI%20Benchmarks%20%26%20Leaderboards&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/Exploring-AI-Benchmarks-and-Leaderboards.html" target=_blank>Twitter</a> | <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/Exploring-AI-Benchmarks-and-Leaderboards.html" target=_blank>Facebook</a> | <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/Exploring-AI-Benchmarks-and-Leaderboards.html" target=_blank>LinkedIn</a> </div> <div id=comments style="margin-top: 3rem;"> <script src=https://giscus.app/client.js data-repo=dasarpai/dasarpai-comments data-repo-id=R_kgDOOGVFpA data-category=General data-category-id=DIC_kwDOOGVFpM4CnzHR data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=en crossorigin=anonymous async>
        </script> </div> </article> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.js></script> </body> </html>