<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Jane Smith"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Exploring-Tokenization-in-AI.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Exploring Tokenization and Embedding in NLP - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#exploring-tokenization-and-embedding-in-nlp class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Exploring Tokenization and Embedding in NLP </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-tokenization class=md-nav__link> <span class=md-ellipsis> What is tokenization? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-different-tokenzation-schemes class=md-nav__link> <span class=md-ellipsis> What are different Tokenzation schemes? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-oov-out-of-vocabulary-in-tokenization class=md-nav__link> <span class=md-ellipsis> What is OOV (Out-of-Vocabulary) in Tokenization? </span> </a> </li> <li class=md-nav__item> <a href=#if-a-word-does-not-exist-in-embedding-models-vocabulary-then-how-tokenization-and-embedding-is-done class=md-nav__link> <span class=md-ellipsis> If a word does not exist in embedding model's vocabulary, then how tokenization and embedding is done? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-criteria-of-splitting-a-word class=md-nav__link> <span class=md-ellipsis> What is criteria of splitting a word? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-subword-tokenization class=md-nav__link> <span class=md-ellipsis> What is Subword Tokenization? </span> </a> </li> <li class=md-nav__item> <a href=#how-fasttext-tokenization-works class=md-nav__link> <span class=md-ellipsis> How FastText Tokenization works? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-role-of-cls-token class=md-nav__link> <span class=md-ellipsis> What is role of [CLS] token? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-wordpiece-and-how-it-works class=md-nav__link> <span class=md-ellipsis> What is WordPiece and how it works? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-bpe-byte-pair-encoding-and-how-it-works class=md-nav__link> <span class=md-ellipsis> What is BPE (Byte Pair Encoding), and how it works? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-sentencepiece-and-how-it-works class=md-nav__link> <span class=md-ellipsis> What is SentencePiece and how it works? </span> </a> </li> <li class=md-nav__item> <a href=#for-indian-languages-what-tokenization-schemes-is-the-best class=md-nav__link> <span class=md-ellipsis> For Indian languages what tokenization schemes is the best ? </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/Exploring-Tokenization-in-AI.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/Exploring-Tokenization-in-AI.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="Exploring Tokenization and Embedding in NLP" src=../assets/images/dspost/dsp6215-Exploring-Tokenization-in-AI.jpg></p> <h1 id=exploring-tokenization-and-embedding-in-nlp>Exploring Tokenization and Embedding in NLP<a class=headerlink href=#exploring-tokenization-and-embedding-in-nlp title="Permanent link">&para;</a></h1> <p>Tokenization and embedding are key components of natural language processing (NLP) models. Sometimes people misunderstand tokenization and embedding and this article is to address those issues. This is in the question answer format and addressing following questions.</p> <ol> <li>What is tokenization?</li> <li>What are different Tokenzation schemes?</li> <li>What is OOV (Out-of-Vocabulary) in Tokenization?</li> <li>If a word does not exist in embedding model's vocabulary, then how tokenization and embedding is done?</li> <li>What is criteria of splitting a word?</li> <li>What is Subword Tokenization?</li> <li>How FastText Tokenization works?</li> <li>What is role of [CLS] token?</li> <li>What is WordPiece and how it works?</li> <li>What is BPE (Byte Pair Encoding), and how it works?</li> <li>What is SentencePiece and how it works?</li> <li>For Indian languages what tokenization schemes is the best?</li> </ol> <h2 id=what-is-tokenization>What is tokenization?<a class=headerlink href=#what-is-tokenization title="Permanent link">&para;</a></h2> <p>Tokenization is the process of breaking text into smaller units (tokens), such as words, subwords, or characters, for NLP tasks.</p> <p>Purpose: ‚úî Converts raw text into a structured format for machine learning models. ‚úî Handles out-of-vocabulary (OOV) words and improves text processing. ‚úî Enables models to understand and generate human language efficiently.</p> <h2 id=what-are-different-tokenzation-schemes>What are different Tokenzation schemes?<a class=headerlink href=#what-are-different-tokenzation-schemes title="Permanent link">&para;</a></h2> <p>The most popular and widely used in NLP today. Each scheme is optimized for different tasks, languages, and domains. </p> <ol> <li><strong>Word-Based</strong> (Simple, but suffers from OOV issues) </li> <li><strong>Whitespace &amp; Rule-Based</strong> (NLTK, spaCy, useful for preprocessing) </li> <li><strong>Subword-Based</strong> (WordPiece, BPE, Unigram‚Äîused in BERT, GPT, T5) </li> <li><strong>Character-Level</strong> (Good for misspellings, used in FastText, OCR) </li> <li><strong>Byte-Level</strong> (GPT-4, T5, handles emojis and special characters) </li> <li><strong>Morpheme-Based</strong> (Japanese, Korean, Chinese NLP) </li> <li><strong>Sentence-Level</strong> (Text summarization, document-level models) </li> <li><strong>Hybrid Tokenization</strong> (Combining multiple methods‚ÄîXLNet, ALBERT) </li> <li><strong>Augmented BPE</strong> (Multilingual NLP‚ÄîmBERT, XLM-R) </li> <li><strong>Meta-Corpus Tokenization</strong> (Self-learning tokenization‚ÄîDeepMind RETRO) </li> <li><strong>Hierarchical Tokenization</strong> (Sentence ‚Üí Word ‚Üí Subword ‚Üí Character) </li> <li><strong>Phoneme-Based</strong> (Speech recognition‚ÄîWhisper, wav2vec) </li> <li><strong>Radical-Based</strong> (For Chinese/Japanese‚ÄîMeCab, KoNLPy) </li> <li><strong>Graph-Based</strong> (For knowledge graphs‚ÄîKG-BERT) </li> <li><strong>Punctuation-Aware</strong> (Legal, TTS, sentiment analysis) </li> <li><strong>Code-Specific</strong> (For programming languages‚ÄîCodeBERT, Codex) </li> </ol> <h2 id=what-is-oov-out-of-vocabulary-in-tokenization>What is OOV (Out-of-Vocabulary) in Tokenization?<a class=headerlink href=#what-is-oov-out-of-vocabulary-in-tokenization title="Permanent link">&para;</a></h2> <p>OOV (Out-of-Vocabulary) refers to words that are not present in a model‚Äôs vocabulary and cannot be directly recognized or processed.</p> <p>Why Does OOV Occur? - Limited vocabulary size in NLP models. - Rare or new words, such as slang or technical terms. - Misspellings or variations of words. - Morphologically complex words, especially in rich languages like Hindi or Finnish.</p> <h2 id=if-a-word-does-not-exist-in-embedding-models-vocabulary-then-how-tokenization-and-embedding-is-done>If a word does not exist in embedding model's vocabulary, then how tokenization and embedding is done?<a class=headerlink href=#if-a-word-does-not-exist-in-embedding-models-vocabulary-then-how-tokenization-and-embedding-is-done title="Permanent link">&para;</a></h2> <p>For example we have word <em>simultaneously</em> which is not part of vocbulary. In that case it is tokenized into multiple tokens in a transformer-based model (e.g., BERT, GPT), each token gets its own embedding. To generate an embedding for the complete word, there are several strategies:</p> <ol> <li><strong>Taking the First Token's Embedding</strong> </li> <li> <p>Many transformer models (e.g., BERT) use <strong>wordpiece</strong> tokenization, where the first subword token often carries more meaning. Using the embedding of the first token can be a simple and efficient choice.</p> </li> <li> <p><strong>Averaging the Token Embeddings</strong> </p> </li> <li>Compute the <strong>mean</strong> of all subword embeddings: $$ E_{word} = \frac{1}{N} \sum_{i=1}^{N} E_{token_i}$$</li> <li> <p>This smooths out variations and gives a balanced representation.</p> </li> <li> <p><strong>Weighted Average (Attention-Based)</strong> </p> </li> <li> <p>Some methods use attention weights to give more importance to certain subwords, especially in contextual embeddings.</p> </li> <li> <p><strong>Using the Last Token's Embedding</strong> </p> </li> <li> <p>If the last token carries a suffix that changes the meaning (e.g., "-ing", "-ly"), it might be useful to consider its embedding.</p> </li> <li> <p><strong>Concatenation of Embeddings</strong> </p> </li> <li> <p>Instead of averaging, concatenate the embeddings of all tokens, creating a longer but richer word representation.</p> </li> <li> <p><strong>Fine-tuning with a Context-Aware Model</strong> </p> </li> <li>If the application requires better word-level embeddings, a downstream model can be trained to aggregate subword embeddings effectively.</li> </ol> <p><strong>Best Practice</strong> For general NLP tasks, <strong>averaging subword embeddings</strong> is a common and effective strategy. However, in tasks like machine translation or sentiment analysis, an attention-weighted or task-specific approach may work better.</p> <h2 id=what-is-criteria-of-splitting-a-word>What is criteria of splitting a word?<a class=headerlink href=#what-is-criteria-of-splitting-a-word title="Permanent link">&para;</a></h2> <p>The criteria for splitting a word into multiple tokens in BERT (or similar models) depend on the tokenization algorithm. BERT uses WordPiece tokenization, which follows these rules:</p> <p>simultaneously ‚Üí ['simult', '##aneously'] transformers ‚Üí ['transform', '##ers'] # Splitting Follows the "Longest Match First" Rule<br> Subword Tokens Start with ##</p> <h2 id=what-is-subword-tokenization>What is Subword Tokenization?<a class=headerlink href=#what-is-subword-tokenization title="Permanent link">&para;</a></h2> <p><strong>Subword Tokenization</strong> is a technique that breaks words into smaller, meaningful units (subwords) rather than whole words or individual characters. This helps handle <strong>out-of-vocabulary (OOV) words</strong>, <strong>morphologically rich languages</strong>, and <strong>rare words</strong> efficiently.</p> <p><strong>How It Works:</strong> 1. <strong>Training Phase</strong>:<br> - A vocabulary is built using <strong>frequent words</strong> and <strong>subwords</strong> from a corpus. - Common sequences of characters are merged iteratively.</p> <ol> <li><strong>Tokenization Phase</strong> (at inference): </li> <li>Words are split into <strong>subwords</strong> based on the trained vocabulary.</li> <li>Frequent words remain whole, while rare words are broken into subwords.</li> </ol> <p><strong>Example (Using BPE/WordPiece)</strong> - <strong>Input:</strong> <code>"unhappiness"</code> - <strong>Tokenized as:</strong> <code>["un", "happiness"]</code><br> (if "happiness" is common) - <strong>If not in vocabulary:</strong> <code>["un", "happi", "ness"]</code></p> <p><strong>Popular Subword Tokenization Methods</strong> - <strong>Byte Pair Encoding (BPE)</strong> ‚Äì Used in GPT models. - <strong>WordPiece</strong> ‚Äì Used in BERT. - <strong>SentencePiece (Unigram)</strong> ‚Äì Used in T5, mBERT. - <strong>FastText</strong></p> <p><strong>Key Benefits</strong> ‚úîÔ∏è Handles <strong>rare words</strong><br> ‚úîÔ∏è Reduces <strong>vocabulary size</strong><br> ‚úîÔ∏è Works well for <strong>multilingual models</strong> </p> <h2 id=how-fasttext-tokenization-works>How FastText Tokenization works?<a class=headerlink href=#how-fasttext-tokenization-works title="Permanent link">&para;</a></h2> <p><strong>FastText</strong> uses <strong>subword tokenization</strong>, but in a different way compared to <strong>BPE, WordPiece, or SentencePiece</strong>. </p> <p><strong>How FastText Uses Subwords</strong><br> Instead of breaking words into learned <strong>subword units</strong>, FastText represents words using <strong>character n-grams</strong> (continuous sequences of <code>n</code> characters). </p> <p><strong>How It Works:</strong><br> 1. Each word is split into overlapping <strong>character n-grams</strong> (default: <strong>3-6 characters</strong>).<br> 2. The word embedding is computed as the sum of its <strong>subword embeddings</strong>.<br> 3. This helps handle <strong>OOV words</strong>, <strong>misspellings</strong>, and <strong>morphological variations</strong> better than traditional word embeddings. </p> <p><strong>Example:</strong><br> For the word <strong>"apple"</strong>, with <code>n=3</code>:<br> - Subwords: <code>["&lt;ap", "app", "ppl", "ple", "le&gt;"]</code><br> - Word embedding = sum of all subword embeddings </p> <h2 id=what-is-role-of-cls-token>What is role of [CLS] token?<a class=headerlink href=#what-is-role-of-cls-token title="Permanent link">&para;</a></h2> <p>The [CLS] token embedding is used when you want a context-aware representation of a word within a sentence, rather than just its isolated meaning. In BERT, the first token of every input is [CLS], which learns a summary representation of the entire sequence. This is often useful for sentence-level tasks like classification, but it can also be used as a context-aware word embedding when working with entire sentences.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=n>sentence</span> <span class=o>=</span> <span class=s2>&quot;I saw an elephant in the zoo.&quot;</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=c1># Tokenize the sentence</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>sentence</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&quot;pt&quot;</span><span class=p>)</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=c1># Get embeddings</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>    <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=o>**</span><span class=n>tokens</span><span class=p>)</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a><span class=c1># Convert sentence to token list</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a><span class=n>token_list</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a><span class=nb>print</span><span class=p>(</span><span class=n>token_list</span><span class=p>)</span>  <span class=c1># Example output: [&#39;i&#39;, &#39;saw&#39;, &#39;an&#39;, &#39;elephant&#39;, &#39;in&#39;, &#39;the&#39;, &#39;zoo&#39;, &#39;.&#39;]</span>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a><span class=c1># Find index of &quot;elephant&quot;</span>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a><span class=n>elephant_index</span> <span class=o>=</span> <span class=n>token_list</span><span class=o>.</span><span class=n>index</span><span class=p>(</span><span class=s2>&quot;elephant&quot;</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>  <span class=c1># +1 because BERT adds [CLS] at position 0</span>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a><span class=c1># Extract &quot;elephant&quot; embedding</span>
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a><span class=n>elephant_embedding</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>last_hidden_state</span><span class=p>[:,</span> <span class=n>elephant_index</span><span class=p>,</span> <span class=p>:]</span>
</span><span id=__span-0-19><a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a>
</span><span id=__span-0-20><a id=__codelineno-0-20 name=__codelineno-0-20 href=#__codelineno-0-20></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Elephant Embedding Shape:&quot;</span><span class=p>,</span> <span class=n>elephant_embedding</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>  <span class=c1># Expected: (1, 768)</span>
</span><span id=__span-0-21><a id=__codelineno-0-21 name=__codelineno-0-21 href=#__codelineno-0-21></a>
</span><span id=__span-0-22><a id=__codelineno-0-22 name=__codelineno-0-22 href=#__codelineno-0-22></a><span class=c1># Extract embedding for the entire sentence</span>
</span><span id=__span-0-23><a id=__codelineno-0-23 name=__codelineno-0-23 href=#__codelineno-0-23></a><span class=n>sentence_embedding</span> <span class=o>=</span> <span class=n>outputs</span><span class=o>.</span><span class=n>last_hidden_state</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=p>:]</span>
</span><span id=__span-0-24><a id=__codelineno-0-24 name=__codelineno-0-24 href=#__codelineno-0-24></a>
</span><span id=__span-0-25><a id=__codelineno-0-25 name=__codelineno-0-25 href=#__codelineno-0-25></a><span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Sentence Embedding Shape:&quot;</span><span class=p>,</span> <span class=n>sentence_embedding</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>  <span class=c1># Expected: (1, 768)</span>
</span></code></pre></div> <h2 id=what-is-wordpiece-and-how-it-works>What is WordPiece and how it works?<a class=headerlink href=#what-is-wordpiece-and-how-it-works title="Permanent link">&para;</a></h2> <p><strong>WordPiece</strong> is a <strong>subword tokenization algorithm</strong> used in <strong>BERT, mBERT, and ALBERT</strong>. It helps handle <strong>OOV words</strong>, reduces vocabulary size, and improves efficiency in NLP models. </p> <p><strong>How WordPiece Works:</strong><br> 1. <strong>Build Vocabulary:</strong><br> - Start with a base vocabulary (single characters).<br> - Iteratively merge the most <strong>frequent</strong> adjacent character sequences to form subwords.<br> - Stop when the vocabulary reaches a predefined size (e.g., 30,000 for BERT). </p> <ol> <li><strong>Tokenization Process:</strong> </li> <li>Split words into subwords based on the trained vocabulary. </li> <li>If a word is not in the vocabulary, it is <strong>broken into known subwords</strong>, prefixed with <code>##</code> (to indicate continuation). </li> </ol> <p><strong>Example:</strong><br> - <strong>Input:</strong> <code>"unhappiness"</code><br> - <strong>Tokenized as:</strong> <code>["un", "##happi", "##ness"]</code><br> (if "happiness" isn't frequent, it's split further) </p> <h2 id=what-is-bpe-byte-pair-encoding-and-how-it-works>What is BPE (Byte Pair Encoding), and how it works?<a class=headerlink href=#what-is-bpe-byte-pair-encoding-and-how-it-works title="Permanent link">&para;</a></h2> <p><strong>BPE (Byte Pair Encoding)</strong> is a <strong>subword tokenization algorithm</strong> that replaces the most frequent character pairs with new tokens iteratively. It is widely used in <strong>GPT models, MarianMT, and SentencePiece</strong> to handle <strong>rare words, reduce vocabulary size, and improve text compression</strong>.</p> <hr> <p><strong>Step 1. Training Phase (Vocabulary Learning)</strong> 1. <strong>Start with individual characters</strong> as the base vocabulary.<br> 2. <strong>Count frequent adjacent character pairs</strong> in the corpus.<br> 3. <strong>Merge the most frequent pair</strong> into a new token.<br> 4. <strong>Repeat steps 2-3</strong> until a predefined vocabulary size is reached.</p> <hr> <p><strong>Step 2. Tokenization Phase (Applying Learned Rules)</strong> 1. Break words into <strong>subwords</strong> using the trained vocabulary.<br> 2. If a word is not found in the vocabulary, break it into the longest matching subwords.</p> <hr> <p><strong>Example:</strong><br> <strong>Corpus:</strong> <code>"low", "lowest", "lower"</code><br> 1. Initial tokens: <code>["l", "o", "w", "e", "s", "t", "r"]</code><br> 2. Merge frequent pairs:<br> - <code>"l o"</code> ‚Üí <code>"lo"</code><br> - <code>"lo w"</code> ‚Üí <code>"low"</code><br> - <code>"low e"</code> ‚Üí <code>"lowe"</code><br> - <code>"lowe s"</code> ‚Üí <code>"lowes"</code><br> - <code>"lowes t"</code> ‚Üí <code>"lowest"</code> </p> <p><strong>Final tokenized words:</strong><br> - <code>"low"</code> ‚Üí <code>["low"]</code><br> - <code>"lowest"</code> ‚Üí <code>["low","est"]</code><br> - <code>"lower"</code> ‚Üí <code>["low", "er"]</code> </p> <h2 id=what-is-sentencepiece-and-how-it-works>What is SentencePiece and how it works?<a class=headerlink href=#what-is-sentencepiece-and-how-it-works title="Permanent link">&para;</a></h2> <p><strong>What is SentencePiece?</strong><br> <strong>SentencePiece</strong> is a data-driven <strong>subword tokenization algorithm</strong> used in <strong>T5, mBERT, XLNet, and MarianMT</strong>. Unlike <strong>BPE</strong> or <strong>WordPiece</strong>, it treats text as a <strong>stream of raw bytes</strong> instead of splitting on whitespace or relying on pre-tokenized words. </p> <hr> <p><strong>How SentencePiece Works?</strong><br> 1. <strong>Preprocesses Text Without Whitespace Assumptions</strong><br> - Treats input text as a <strong>continuous stream of characters</strong> (no need for space-based tokenization).<br> - Works well for languages <strong>without clear word boundaries</strong> (e.g., Chinese, Japanese, Thai). </p> <ol> <li><strong>Builds a Token Vocabulary Using One of Two Methods:</strong> </li> <li><strong>Byte Pair Encoding (BPE)</strong> ‚Üí Merges frequent character pairs iteratively. </li> <li> <p><strong>Unigram Model</strong> ‚Üí Uses a <strong>probabilistic approach</strong>, where subword units are scored based on likelihood. </p> </li> <li> <p><strong>Tokenizes New Text Based on Learned Vocabulary</strong> </p> </li> <li>Splits words into <strong>subwords</strong> based on the highest-probability segmentations. </li> </ol> <hr> <p><strong>Example:</strong><br> <strong>Input:</strong> <code>"unhappiness"</code> </p> <p>If using <strong>BPE-based SentencePiece:</strong><br> üîπ <code>["un", "happiness"]</code> (if "happiness" is frequent)<br> üîπ <code>["un", "happi", "ness"]</code> (if "happiness" isn't frequent) </p> <p>If using <strong>Unigram-based SentencePiece:</strong><br> üîπ Multiple segmentations are possible, and the most probable one is selected. </p> <hr> <p><strong>Key Advantages of SentencePiece:</strong><br> ‚úî <strong>Supports raw text</strong> (doesn‚Äôt need pre-tokenization)<br> ‚úî <strong>Works for multiple languages</strong> (even ones without spaces)<br> ‚úî <strong>Can use both BPE and Unigram approaches</strong><br> ‚úî <strong>Highly efficient for Neural Machine Translation (NMT)</strong> </p> <h2 id=for-indian-languages-what-tokenization-schemes-is-the-best>For Indian languages what tokenization schemes is the best ?<a class=headerlink href=#for-indian-languages-what-tokenization-schemes-is-the-best title="Permanent link">&para;</a></h2> <p>For <strong>Indian languages</strong>, tokenization can be challenging due to their diverse scripts, rich morphology, and the fact that many languages have complex compound words, words with complex characters, and inflections. Below are the <strong>best tokenization schemes</strong> and techniques specifically suited for Indian languages:</p> <hr> <p><strong>1. Morpheme-Based Tokenization</strong></p> <ul> <li>Indian languages like <strong>Hindi, Bengali, Tamil, Kannada</strong>, etc., often have complex morphology, and <strong>morpheme-based tokenization</strong> is highly effective here.</li> <li>This approach breaks down words into smaller meaningful units (morphemes), allowing for better handling of compound words and inflections.</li> <li>Indian languages often have <strong>rich inflection</strong> (gender, tense, case), and tokenizing at the morpheme level preserves these nuances.</li> </ul> <p><strong>Example:</strong> - <strong>Hindi:</strong> "‡§µ‡§π ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à" ("He goes to school")<br> Tokenized as: ["‡§µ‡§π", "‡§∏‡•ç‡§ï‡•Ç‡§≤", "‡§ú‡§æ‡§§‡§æ", "‡§π‡•à"]<br> Using morpheme-based tokenization, one could further break down the word "‡§ú‡§æ‡§§‡§æ" (goes) into "‡§ú‡§æ" (go) + "‡§§‡§æ" (tense suffix).</p> <p><strong>Used in Libraries:</strong><br> - <strong>Indic NLP Library</strong> - <strong>KoNLPy</strong> (though more popular for Korean, adaptations exist for Indian languages)</p> <hr> <p><strong>2. Subword Tokenization</strong> <strong>Multilingual models</strong> like <strong>mBERT</strong> support Indian languages, and SentencePiece tokenization is often used in these models. These models break down words into <strong>subwords</strong>, enabling better handling of complex and compound words.</p> <ul> <li>Models like WordPiece, BPE, SentencePiece break down words into <strong>subwords</strong>, enabling better handling of complex and compound words.</li> <li><strong>WordPiece, BPE, or SentencePiece</strong> models are popular choices for tokenizing Indian languages, particularly for <strong>Neural Machine Translation (NMT)</strong> or <strong>multilingual models</strong> like <strong>mBERT</strong> and <strong>XLM-R</strong>.</li> <li>These methods are effective for handling <strong>out-of-vocabulary (OOV)</strong> words in low-resource Indian languages.</li> </ul> <p><strong>Example:</strong><br> - <strong>Hindi:</strong> "‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞‡§§‡§æ ‡§∏‡§Ç‡§ó‡•ç‡§∞‡§æ‡§Æ ‡§∏‡•á‡§®‡§æ‡§®‡•Ä" ("Freedom Fighter")<br> Tokenized into subwords: ["‡§∏‡•ç‡§µ‡§§", "‡§Ç‡§§‡•ç‡§∞", "‡§§‡§æ", "‡§∏‡§Ç", "‡§ó‡•ç‡§∞‡§æ‡§Æ", "‡§∏‡•á", "‡§®‡§æ‡§®‡•Ä"]</p> <p><strong>Used in Libraries:</strong><br> - <strong>mBERT</strong>, <strong>XLM-R</strong>, <strong>IndicBERT</strong>, <strong>T5</strong>, <strong>XLM</strong></p> <hr> <p><strong>3. Hybrid Tokenization (Combining WordPiece + Character-Level)</strong> This approach combines the best of <strong>word-level tokenization</strong> and <strong>character-level tokenization</strong>. It‚Äôs especially useful for <strong>Indian languages</strong> because they have a combination of regular words and highly agglutinative or inflected words. Hybrid tokenization helps model both <strong>frequent words</strong> and <strong>rare or unknown words</strong> by combining word-level tokenization with the ability to handle characters.</p> <p><strong>Example:</strong><br> - <strong>Tamil:</strong> "‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡ØÅ" ("Tamil Nadu")<br> - Could be tokenized as ["‡Æ§", "‡ÆÆ", "‡Æø‡Æ¥", "‡Æ®‡Ææ‡Æü‡ØÅ"] (character-level) or as one word in <strong>pre-trained multilingual models</strong>.</p> <p><strong>Used in Libraries:</strong><br> - <strong>IndicBERT</strong>, <strong>XLM-R</strong> (combines character-level and subword-level tokenization).</p> <hr> <p><strong>4. Sentence-Level Tokenization</strong> For tasks like <strong>translation</strong>, <strong>summarization</strong>, or <strong>document-level tasks</strong> in <strong>Indian languages</strong>, sentence-level tokenization is often used. <strong>Sentence segmentation</strong> helps in <strong>splitting long texts</strong> into meaningful sentences, which can then be processed by NLP models. Indian languages have complex sentence structures, and sentence segmentation ensures that each meaningful unit is tokenized appropriately.</p> <p><strong>Example:</strong><br> - <strong>Hindi:</strong> "‡§Æ‡•à‡§Ç ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ‡§§‡§æ ‡§π‡•Ç‡§Å‡•§" ‚Üí ["‡§Æ‡•à‡§Ç ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ‡§§‡§æ ‡§π‡•Ç‡§Å‡•§"] - a complete sentence. - <strong>Tamil:</strong> "‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æ™‡Øã‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç." ‚Üí ["‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡Æ≥‡Øç‡Æ≥‡Æø‡Æï‡Øç‡Æï‡ØÅ ‡Æ™‡Øã‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç."]</p> <p><strong>Used in Libraries:</strong><br> - <strong>IndicNLP</strong> for segmentation. - <strong>spaCy</strong> with custom models.</p> <hr> <p><strong>5. Grapheme-Based Tokenization (Script-Specific)</strong> <strong>Grapheme-based tokenization</strong> is used for handling text at the character level. Especially useful for handling <strong>script-specific challenges</strong> (e.g., <strong>ligatures</strong> or <strong>complex characters</strong>). It works at the <strong>character level</strong>, breaking down words into individual characters, making it easier to handle scripts with rich diacritics and combinations.</p> <p><strong>Example:</strong><br> - <strong>Kannada:</strong> "‡≤¨‡≥Ü‡≤Ç‡≤ó‡≤≥‡≥Ç‡≤∞‡≥Å" ("Bengaluru")<br> Tokenized at the grapheme level: ["‡≤¨‡≥Ü", "‡≤Ç", "‡≤ó", "‡≤≤", "‡≥Ç", "‡≤∞‡≥Å"]</p> <p><strong>Used in Libraries:</strong><br> - <strong>Indic NLP Library</strong> for preprocessing. <strong>ICU-based tokenizers</strong> (Unicode handling).</p> <hr> <p><strong>6. Character n-grams</strong> <strong>FastText</strong> is a <strong>character-level model</strong> that uses <strong>n-grams</strong> (subsequences of n characters) to generate word embeddings, and it works well for <strong>Indian languages</strong>, especially for <strong>low-resource languages</strong>. It works well for low-resource languages. Useful for text classification, <strong>word embeddings</strong>, and <strong>language identification</strong> in Indian languages. FastText works well with <strong>misspelled words</strong>, <strong>out-of-vocabulary terms</strong>, and <strong>morphologically rich languages</strong> like Hindi, Tamil, or Bengali.</p> <p><strong>Example:</strong><br> - <strong>Hindi:</strong> "‡§∏‡§Æ‡§æ‡§ú‡§µ‡§æ‡§¶‡•Ä" ("Socialist")<br> Tokenized as character n-grams like ["‡§∏‡§Æ", "‡§Æ‡§æ", "‡§æ‡§ú", "‡§µ‡§æ‡§¶", "‡§µ‡•Ä"].</p> <p><strong>Used in Libraries:</strong><br> - <strong>FastText model</strong> for Indian language embeddings.</p> <hr> <p><strong>7. Punctuation-Aware Tokenization</strong> For <strong>sentiment analysis</strong>, <strong>legal text</strong>, and <strong>social media data</strong>, tokenizers need to handle <strong>punctuation</strong> and <strong>emojis</strong> effectively. <strong>Indian languages</strong> with a mix of <strong>Hindi, English (Hinglish)</strong>, and emojis require punctuation-aware tokenization. Keeps punctuation as separate tokens, ensuring that models correctly interpret sentiments or legal structures.</p> <p><strong>Example:</strong><br> - <strong>Hindi-English:</strong> "‡§Æ‡•Å‡§ù‡•á food ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§≤‡§ó‡§æ!" ("I liked food!")<br> Tokenized as: ["‡§Æ‡•Å‡§ù‡•á", "food", "‡§Ö‡§ö‡•ç‡§õ‡§æ", "‡§≤‡§ó‡§æ", "!"]</p> <p><strong>Used in Libraries:</strong><br> - <strong>BERT-based</strong> models like <strong>Hinglish-BERT</strong>. - <strong>Or other Multi-lingual or code-mixed NLP models</strong>.</p> <hr> <p><strong>8. Indic-Specific Models (mBERT, IndicBERT)</strong> Pre-trained models like <strong>mBERT</strong> or <strong>IndicBERT</strong> are trained on <strong>multiple Indian languages</strong>. They rely on <strong>subword tokenization</strong> (BPE/WordPiece) for efficient handling of Indian languages. <strong>IndicBERT</strong> is specifically optimized for <strong>Indian languages</strong> and trained on <strong>11 official languages</strong> of India. These models are designed with <strong>Indian linguistic features</strong> in mind, and they support multiple <strong>scripts</strong> and <strong>languages</strong> without requiring language-specific tokenization.</p> <p><strong>Example:</strong><br> - <strong>Hindi:</strong> "‡§≠‡§æ‡§∞‡§§" ("India")<br> Tokenized as: ["‡§≠‡§æ‡§∞‡§§"]</p> <p><strong>Used in Libraries:</strong><br> - <strong>IndicBERT</strong>, <strong>mBERT</strong>, <strong>XLM-R</strong> for multilingual tasks.</p> <hr> <p><strong>Which Tokenization Scheme is Best for Indian Languages?</strong> - <strong>Morpheme-Based</strong>: Best for <strong>richly inflected languages</strong> (e.g., Hindi, Tamil, Kannada). - <strong>BPE/WordPiece/SentencePiece</strong>: Best for <strong>multilingual models</strong> (e.g., <strong>mBERT</strong>, <strong>IndicBERT</strong>) and <strong>low-resource languages</strong>. - <strong>Hybrid Tokenization</strong>: Effective for <strong>handling both frequent and rare words</strong> (e.g., <strong>IndicBERT</strong>, <strong>XLM-R</strong>). - <strong>FastText</strong>: Best for <strong>word embeddings</strong>, <strong>language identification</strong>, and <strong>low-resource languages</strong>. - <strong>Punctuation-Aware</strong>: Best for <strong>sentiment analysis</strong>, <strong>legal text</strong> or <strong>social media</strong>.</p> <hr> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>