<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/AWS-Sagemaker-Jumpstart-Models.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>AWS SageMaker Jumpstart Models - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta name=author content="Hari Thapliyaal"><meta name=description content="An overview of AWS SageMaker Jumpstart Models, exploring their capabilities, developers, and hosting details."><meta name=keywords content="AWS SageMaker, Jumpstart Models, Machine Learning, AI Models, Model Zoo, Text Generation, Text to Image"><meta property=og:type content=article><meta property=og:locale content=en_US><meta property=og:site_name content=DasarpAI><meta property=og:title content="AWS SageMaker Jumpstart Models"><meta property=og:description content="An overview of AWS SageMaker Jumpstart Models, exploring their capabilities, developers, and hosting details."><meta property=og:url content=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/AWS-Sagemaker-Jumpstart-Models.html><meta property=og:image content=../../assets/images/dspost/dsp6076-AWS-SageMaker-Jumpstart-Models.jpg><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta name=twitter:card content=summary_large_image><meta name=twitter:site content=@dasarpai><meta name=twitter:title content="AWS SageMaker Jumpstart Models"><meta name=twitter:description content="An overview of AWS SageMaker Jumpstart Models, exploring their capabilities, developers, and hosting details."><meta name=twitter:image content=../../assets/images/dspost/dsp6076-AWS-SageMaker-Jumpstart-Models.jpg><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/AWS-Sagemaker-Jumpstart-Models.html><link rel=stylesheet href=../assets/stylesheets/custom.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#aws-sagemaker-jumpstart-models class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@harithapliyal</strong> on <a rel=me href=https://linkedin.com/in/harithapliyal> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/dasarpai> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> AWS SageMaker Jumpstart Models </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#references class=md-nav__link> <span class=md-ellipsis> References </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <article class="md-content__inner md-typeset"> <p><img alt="AWS SageMaker Jumpstart Models" src=../assets/images/dspost/dsp6076-AWS-SageMaker-Jumpstart-Models.jpg> </p> <h1 id=aws-sagemaker-jumpstart-models>AWS SageMaker Jumpstart Models<a class=headerlink href=#aws-sagemaker-jumpstart-models title="Permanent link">&para;</a></h1> <p>As of 17-Jul-23, AWS Sagemaker has 463 models in its Model Zoo. They call these models as Jumstart Models. What are the capabilities of these models, who are the developer of these models, where these models are hosted in given in the table below.</p> <table> <thead> <tr> <th>SNo.</th> <th>Task Type</th> <th>Company</th> <th>Model Description</th> <th>Model ID</th> </tr> </thead> <tbody> <tr> <td>1.</td> <td>Text Generation</td> <td>Huggingface</td> <td>Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize It is ready-to-use chat/instruct model based on Falcon 40B</td> <td>Model ID: huggingface-textgeneration-falcon-40b-instruct-bf16</td> </tr> <tr> <td>2.</td> <td>Text Generation</td> <td>Huggingface</td> <td>This is a Text Generation model built upon a Transformer model from Hugging Face</td> <td>Model ID: huggingface-textgeneration-open-llama</td> </tr> <tr> <td>3.</td> <td>Text to Image</td> <td>StabilityAI</td> <td>Extend beyond just text-to-image prompting. Stable Diffusion XL offers several ways to modify the images: Inpainting - edit inside the image, Outpainting - extend the image outside of the original image, Image-to-image - prompt a new image using a sourced image.</td> <td></td> </tr> <tr> <td>4.</td> <td>Text Generation</td> <td>Cohere</td> <td>Generative model that responds well with instruction-like prompts. This model provides businesses and enterprises with best quality, performance and accuracy in all generative tasks. And with our intuitive SDK, unlocking the full potential of LLMs for your applications has never been easier.</td> <td></td> </tr> <tr> <td>5.</td> <td>Text Generation</td> <td>AI21 Labs</td> <td>Jurassic-2 Ultra is optimized to follow natural language instructions and context, so there is no need to provide it with any examples.</td> <td></td> </tr> <tr> <td>6.</td> <td>Text Generation</td> <td>AI21 Labs</td> <td></td> <td></td> </tr> <tr> <td>7.</td> <td>Text Generation</td> <td>AI21 Labs</td> <td>Condense lengthy texts into short, easy-to-read bites that remain factually consistent with the source. No prompting needed – simply input the text that needs to be summarized. The model is specifically trained to generate summaries that capture the essence and key ideas of the original text.</td> <td></td> </tr> <tr> <td>8.</td> <td>Text Generation</td> <td>AI21 Labs</td> <td>Get the AI21 Paraphrase model, the top-of-the-line paraphrasing engine, and deploy it in your private environment. The model aims to generate 10 alternative suggestions with every activation. It may return fewer suggestions when rewriting very short texts for which it cannot produce as many as 10 sensible paraphrases.</td> <td></td> </tr> <tr> <td>9.</td> <td>Text Generation</td> <td>AI21 Labs</td> <td>Jurassic-2 Mid is optimized to follow natural language instructions and context, so there is no need to provide it with any examples. Pre-trained language model trained by AI21 Labs on a corpus of web text including natural language and computer programs with recent data - updated to mid 2022. This model has a 8192 token context window (i.e. the length of the prompt + completion should be at most 8192 tokens).</td> <td></td> </tr> <tr> <td>10.</td> <td>Text Generation</td> <td>AI21 Labs</td> <td>Detects and suggests corrections for Grammar, Spelling, Punctuation mistakes, as well as word misuse, and accidental repetition or omission.</td> <td></td> </tr> <tr> <td>11.</td> <td>Text to Image</td> <td>StabilityAI</td> <td>Extend beyond just text-to-image prompting. Stable Diffusion XL offers several ways to modify the images: Inpainting - edit inside the image, Outpainting - extend the image outside of the original image, Image-to-image - prompt a new image using a sourced image.</td> <td></td> </tr> <tr> <td>12.</td> <td>Text to Image</td> <td>Stabilityai</td> <td>This is a text-to-image model from Stability AI and downloaded from HuggingFace It takes a textual description as input and returns a generated image from the description</td> <td>Model ID: model-txt2img-stabilityai-stable-diffusion-v2-1-base</td> </tr> <tr> <td>13.</td> <td></td> <td>Huggingface</td> <td>This is a Text2Text Generation model built upon a T5 model from Hugging Face The deployed model can be used for running inference on any input text</td> <td>Model ID: huggingface-text2text-flan-t5-xl</td> </tr> <tr> <td>14.</td> <td></td> <td>Huggingface</td> <td>This is a Text Generation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts next words in the sequence</td> <td>Model ID: huggingface-textgeneration1-gpt-j-6b</td> </tr> <tr> <td>15.</td> <td></td> <td>Huggingface</td> <td>This is a Text2Text Generation model built upon a T5 model from Hugging Face The deployed model can be used for running inference on any input text</td> <td>Model ID: huggingface-text2text-flan-ul2-bf16</td> </tr> <tr> <td>16.</td> <td>Text Generation</td> <td>Pytorch</td> <td>AlexaTM 20B is a multitask, multilingual, large-scale sequence-to-sequence (seq2seq) model, trained on a mixture of Common Crawl (mC4) and Wikipedia data across 12 languages, using denoising and Causal Language Modeling (CLM) tasks</td> <td>Model ID: pytorch-textgeneration1-alexa20b</td> </tr> <tr> <td>17.</td> <td>Text Generation</td> <td>Huggingface</td> <td>This is a Text Generation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts next words in the sequence</td> <td>Model ID: huggingface-textgeneration-bloom-1b7</td> </tr> <tr> <td>18.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>This is an Image Classification model from TensorFlow Hub It takes an image as input and classifies the image to one of the 1001 classes</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v2-100-224-classification-4</td> </tr> <tr> <td>19.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>This is an object detection model from Tensorflow It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: tensorflow-od1-ssd-resnet50-v1-fpn-640x640-coco17-tpu-8</td> </tr> <tr> <td>20.</td> <td>Object Detection</td> <td>Pytorch</td> <td>This is an object detection model from PyTorch Hub It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: pytorch-od1-fasterrcnn-resnet50-fpn</td> </tr> <tr> <td>21.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>This is a Text Classification model built upon a Text Embedding model from TensorFlow Hub It takes a text string as input and classifies the input text as either a positive or negative movie review</td> <td>Model ID: tensorflow-tc-bert-en-uncased-L-12-H-768-A-12-2</td> </tr> <tr> <td>22.</td> <td>Question Answering</td> <td>Huggingface</td> <td>This is an Extractive Question Answering model built on a Transformer model from Hugging Face It takes two strings as inputs: the first string is a question and the second string is the context or any text you want to use to find the answer of the question, and it returns a sub-string from the context as an answer to the question</td> <td>Model ID: huggingface-eqa-distilbert-base-uncased</td> </tr> <tr> <td>23.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>This is Zero Shot Text Classification model built on a Transformer model from Hugging Face It can classify sentences in English language It takes a sequence and a list of candidate labels as inputs and predicts score that the sequence is associated with the particular label</td> <td>Model ID: huggingface-zstc-facebook-bart-large-mnli</td> </tr> <tr> <td>24.</td> <td>Semantic Segmentation</td> <td>Mxnet</td> <td>This is an Semantic Segmentation model from Gluon CV It takes an image as input and returns class label for each pixel in the image</td> <td>Model ID: mxnet-semseg-fcn-resnet101-coco</td> </tr> <tr> <td>25.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>This is a Sentence Pair Classification model built upon a Text Embedding model from Hugging Face It takes a pair of sentences as input and classifies the input pair to 'entailment' or 'no-entailment'</td> <td>Model ID: huggingface-spc-distilbert-base-uncased</td> </tr> <tr> <td>26.</td> <td>Named Entity Recognition</td> <td>Huggingface</td> <td>This is a Named Entity Generation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts named entities in the input text</td> <td>Model ID: huggingface-ner-distilbert-base-cased-finetuned-conll03-english</td> </tr> <tr> <td>27.</td> <td>Text Summarization</td> <td>Huggingface</td> <td>This is a Text Summarization model built upon a Transformer model from Hugging Face It takes a text string as input and returns a summary of the text</td> <td>Model ID: huggingface-summarization-distilbart-xsum-1-1</td> </tr> <tr> <td>28.</td> <td>Machine Translation</td> <td>Huggingface</td> <td>This is a Machine Translation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts its translation</td> <td>Model ID: huggingface-translation-t5-small</td> </tr> <tr> <td>29.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>This is a Text Embedding model from TensorFlow Hub It takes a text string as input and outputs an embedding vector The Text Embedding model is pre-trained on Wikipedia and BookCorpus datasets</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-2-H-128-A-2-2</td> </tr> <tr> <td>30.</td> <td>Text Embedding</td> <td>Mxnet</td> <td>This is a Text Embedding model from GluonNLP pre-trained on the decade (2010-2019) of S&amp;P 500 10-K/10-Q reports It takes a text string as input and outputs an embedding vector For pre-training, the entire text of the 10K/Q filing was used, not just the MD&amp;A (Management Discussion and Analysis) section, so as to ensure that a broader context of financial language is captured Embeddings from the pre-trained modelare then used for fine-tuning specific classifiers</td> <td>Model ID: mxnet-tcembedding-robertafin-base-uncased</td> </tr> <tr> <td>31.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>This is a Sentence Pair Classification model built upon a Text Embedding model from TensorFlow Hub It takes a pair of sentences as input and classifies the input pair to 'entailment' or 'no-entailment'</td> <td>Model ID: tensorflow-spc-bert-en-uncased-L-12-H-768-A-12-2</td> </tr> <tr> <td>32.</td> <td>Instance Segmentation</td> <td>Mxnet</td> <td>This is an Instance Segmentation model from Gluon CV It detects and delineates each distinct object in the image</td> <td>Model ID: mxnet-is-mask-rcnn-fpn-resnet101-v1d-coco</td> </tr> <tr> <td>33.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>This is an Image Feature Vector model from TensorFlow Hub It takes an image as input and returns a feature vector (embedding) of the image</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v2-100-224-featurevector-4</td> </tr> <tr> <td>34.</td> <td>Image Classification</td> <td>Pytorch</td> <td>This is an Image Classification model from PyTorch Hub It takes an image as input and classifies the image to one of the 1000 classes</td> <td>Model ID: pytorch-ic-mobilenet-v2</td> </tr> <tr> <td>35.</td> <td>Object Detection</td> <td>Mxnet</td> <td>This is an object detection model from Gluon CV It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: mxnet-od-ssd-512-mobilenet1-0-coco</td> </tr> <tr> <td>36.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>This is an object detection model from TensorFlow Hub It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: tensorflow-od-ssd-mobilenet-v2-fpnlite-320x320-1</td> </tr> <tr> <td>37.</td> <td>Object Detection</td> <td>Pytorch</td> <td>This is an object detection model from PyTorch Hub It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: pytorch-od-nvidia-ssd</td> </tr> <tr> <td>38.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>This is an Image Classification model from TensorFlow Hub It takes an image as input and classifies the image to one of the 1001 classes</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v2-075-224-classification-4</td> </tr> <tr> <td>39.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v2-050-224-classification-4</td> </tr> <tr> <td>40.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v2-035-224-classification-4</td> </tr> <tr> <td>41.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v2-140-224-classification-4</td> </tr> <tr> <td>42.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v2-130-224-classification-4</td> </tr> <tr> <td>43.</td> <td>Object Detection</td> <td>Pytorch</td> <td>This is an object detection model from PyTorch Hub It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: pytorch-od1-fasterrcnn-mobilenet-v3-large-320-fpn</td> </tr> <tr> <td>44.</td> <td>Object Detection</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-od1-fasterrcnn-mobilenet-v3-large-fpn</td> </tr> <tr> <td>45.</td> <td>Semantic Segmentation</td> <td>Mxnet</td> <td>This is an Semantic Segmentation model from Gluon CV It takes an image as input and returns class label for each pixel in the image</td> <td>Model ID: mxnet-semseg-fcn-resnet101-voc</td> </tr> <tr> <td>46.</td> <td>Semantic Segmentation</td> <td>Mxnet</td> <td>Same as above</td> <td>Model ID: mxnet-semseg-fcn-resnet101-ade</td> </tr> <tr> <td>47.</td> <td>Instance Segmentation</td> <td>Mxnet</td> <td>Same as above</td> <td>Model ID: mxnet-semseg-fcn-resnet50-ade</td> </tr> <tr> <td>48.</td> <td>Image Classification</td> <td>Pytorch</td> <td>This is an Image Classification model from PyTorch Hub It takes an image as input and classifies the image to one of the 1000 classes</td> <td>Model ID: pytorch-ic-resnet18</td> </tr> <tr> <td>49.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-resnet34</td> </tr> <tr> <td>50.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-resnet50</td> </tr> <tr> <td>51.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-resnet101</td> </tr> <tr> <td>52.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-resnet152</td> </tr> <tr> <td>53.</td> <td>Object Detection</td> <td>Mxnet</td> <td>This is an object detection model from Gluon CV It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: mxnet-od-ssd-512-mobilenet1-0-voc</td> </tr> <tr> <td>54.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-ssd-512-resnet50-v1-coco</td> </tr> <tr> <td>55.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-ssd-512-resnet50-v1-voc</td> </tr> <tr> <td>56.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-ssd-300-vgg16-atrous-coco</td> </tr> <tr> <td>57.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-ssd-300-vgg16-atrous-voc</td> </tr> <tr> <td>58.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-efficientdet-d0-512x512-coco17-tpu-8</td> </tr> <tr> <td>59.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-efficientdet-d1-640x640-coco17-tpu-8</td> </tr> <tr> <td>60.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-efficientdet-d2-768x768-coco17-tpu-8</td> </tr> <tr> <td>61.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-efficientdet-d3-896x896-coco17-tpu-32</td> </tr> <tr> <td>62.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-mobilenet-v1-fpn-640x640-coco17-tpu-8</td> </tr> <tr> <td>63.</td> <td>Instance Segmentation</td> <td>Mxnet</td> <td>This is an Instance Segmentation model from Gluon CV It detects and delineates each distinct object in the image</td> <td>Model ID: mxnet-is-mask-rcnn-fpn-resnet50-v1b-coco</td> </tr> <tr> <td>64.</td> <td>Instance Segmentation</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-is-mask-rcnn-fpn-resnet18-v1b-coco</td> </tr> <tr> <td>65.</td> <td></td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-is-mask-rcnn-resnet18-v1b-coco</td> </tr> <tr> <td>66.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>This is an Image Feature Vector model from TensorFlow Hub It takes an image as input and returns a feature vector (embedding) of the image</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v2-075-224-featurevector-4</td> </tr> <tr> <td>67.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v2-050-224-featurevector-4</td> </tr> <tr> <td>68.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v2-035-224-featurevector-4</td> </tr> <tr> <td>69.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v2-140-224-featurevector-4</td> </tr> <tr> <td>70.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v2-130-224-featurevector-4</td> </tr> <tr> <td>71.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>This is an object detection model from TensorFlow Hub It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: tensorflow-od-ssd-mobilenet-v2-fpnlite-640x640-1</td> </tr> <tr> <td>72.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-ssd-mobilenet-v2-2</td> </tr> <tr> <td>73.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-ssd-mobilenet-v1-fpn-640x640-1</td> </tr> <tr> <td>74.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-resnet50-v1-640x640-1</td> </tr> <tr> <td>75.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-resnet50-v1-800x1333-1</td> </tr> <tr> <td>76.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>This is Zero Shot Text Classification model built on a Transformer model from Hugging Face It can classify sentences in English language It takes a sequence and a list of candidate labels as inputs and predicts score that the sequence is associated with the particular label</td> <td>Model ID: huggingface-zstc-narsil-deberta-large-mnli-zero-cls</td> </tr> <tr> <td>77.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-moritzlaurer-deberta-v3-large-mnli-fever-anli-ling-wanli</td> </tr> <tr> <td>78.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-cross-encoder-nli-distilroberta-base</td> </tr> <tr> <td>79.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-recognai-bert-base-spanish-wwm-cased-xnli</td> </tr> <tr> <td>80.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-moritzlaurer-mdeberta-v3-base-xnli-multilingual-nli-2mil7</td> </tr> <tr> <td>81.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-cross-encoder-nli-roberta-base</td> </tr> <tr> <td>82.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-cross-encoder-nli-deberta-base</td> </tr> <tr> <td>83.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-cross-encoder-nli-minilm2-l6-h768</td> </tr> <tr> <td>84.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-recognai-zeroshot-selectra-medium</td> </tr> <tr> <td>85.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-navteca-bart-large-mnli</td> </tr> <tr> <td>86.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-jiva-xlm-roberta-large-it-mnli</td> </tr> <tr> <td>87.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-digitalepidemiologylab-covid-twitter-bert-v2-mnli</td> </tr> <tr> <td>88.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-recognai-zeroshot-selectra-small</td> </tr> <tr> <td>89.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-distilbert-base-turkish-cased-snli-tr</td> </tr> <tr> <td>90.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-bert-base-turkish-cased-allnli-tr</td> </tr> <tr> <td>91.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-bert-base-turkish-cased-snli-tr</td> </tr> <tr> <td>92.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-bert-base-multilingual-cased-allnli-tr</td> </tr> <tr> <td>93.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-narsil-bart-large-mnli-opti</td> </tr> <tr> <td>94.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-convbert-base-turkish-mc4-cased-allnli-tr</td> </tr> <tr> <td>95.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-lighteternal-nli-xlm-r-greek</td> </tr> <tr> <td>96.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-distilbert-base-turkish-cased-allnli-tr</td> </tr> <tr> <td>97.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-bert-base-multilingual-cased-multinli-tr</td> </tr> <tr> <td>98.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-eleldar-theme-classification</td> </tr> <tr> <td>99.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-bert-base-turkish-cased-multinli-tr</td> </tr> <tr> <td>100.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-bert-base-multilingual-cased-snli-tr</td> </tr> <tr> <td>101.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-convbert-base-turkish-mc4-cased-multinli-tr</td> </tr> <tr> <td>102.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-distilbert-base-turkish-cased-multinli-tr</td> </tr> <tr> <td>103.</td> <td>Zero-Shot Text Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-zstc-emrecan-convbert-base-turkish-mc4-cased-snli-tr</td> </tr> <tr> <td>104.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>This is an Image Classification model from TensorFlow Hub It takes an image as input and classifies the image to one of the 1001 classes</td> <td>Model ID: tensorflow-ic-tf2-preview-mobilenet-v2-classification-4</td> </tr> <tr> <td>105.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-inception-v3-classification-4</td> </tr> <tr> <td>106.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-inception-v2-classification-4</td> </tr> <tr> <td>107.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-inception-v1-classification-4</td> </tr> <tr> <td>108.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-tf2-preview-inception-v3-classification-4</td> </tr> <tr> <td>109.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-inception-resnet-v2-classification-4</td> </tr> <tr> <td>110.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-resnet-v2-50-classification-4</td> </tr> <tr> <td>111.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-resnet-v2-101-classification-4</td> </tr> <tr> <td>112.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-resnet-v2-152-classification-4</td> </tr> <tr> <td>113.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-resnet-v1-50-classification-4</td> </tr> <tr> <td>114.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-resnet-v1-101-classification-4</td> </tr> <tr> <td>115.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-resnet-v1-152-classification-4</td> </tr> <tr> <td>116.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-resnet-50-classification-1</td> </tr> <tr> <td>117.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-b0-classification-1</td> </tr> <tr> <td>118.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-b1-classification-1</td> </tr> <tr> <td>119.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-b2-classification-1</td> </tr> <tr> <td>120.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-b3-classification-1</td> </tr> <tr> <td>121.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-b4-classification-1</td> </tr> <tr> <td>122.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-b5-classification-1</td> </tr> <tr> <td>123.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-b6-classification-1</td> </tr> <tr> <td>124.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-b7-classification-1</td> </tr> <tr> <td>125.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-lite0-classification-2</td> </tr> <tr> <td>126.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-lite1-classification-2</td> </tr> <tr> <td>127.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-lite2-classification-2</td> </tr> <tr> <td>128.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-lite3-classification-2</td> </tr> <tr> <td>129.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-efficientnet-lite4-classification-2</td> </tr> <tr> <td>130.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-100-224-classification-4</td> </tr> <tr> <td>131.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-100-192-classification-4</td> </tr> <tr> <td>132.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-100-160-classification-4</td> </tr> <tr> <td>133.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-100-128-classification-4</td> </tr> <tr> <td>134.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-075-224-classification-4</td> </tr> <tr> <td>135.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-075-192-classification-4</td> </tr> <tr> <td>136.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-075-160-classification-4</td> </tr> <tr> <td>137.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-075-128-classification-4</td> </tr> <tr> <td>138.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-050-224-classification-4</td> </tr> <tr> <td>139.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-050-192-classification-4</td> </tr> <tr> <td>140.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-050-160-classification-4</td> </tr> <tr> <td>141.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-050-128-classification-4</td> </tr> <tr> <td>142.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-025-224-classification-4</td> </tr> <tr> <td>143.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-025-192-classification-4</td> </tr> <tr> <td>144.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-025-160-classification-4</td> </tr> <tr> <td>145.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-imagenet-mobilenet-v1-025-128-classification-4</td> </tr> <tr> <td>146.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-s-r50x1-ilsvrc2012-classification-1</td> </tr> <tr> <td>147.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-s-r50x3-ilsvrc2012-classification-1</td> </tr> <tr> <td>148.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-s-r101x1-ilsvrc2012-classification-1</td> </tr> <tr> <td>149.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-s-r101x3-ilsvrc2012-classification-1</td> </tr> <tr> <td>150.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-m-r50x1-ilsvrc2012-classification-1</td> </tr> <tr> <td>151.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-m-r50x3-ilsvrc2012-classification-1</td> </tr> <tr> <td>152.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-m-r101x1-ilsvrc2012-classification-1</td> </tr> <tr> <td>153.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-m-r101x3-ilsvrc2012-classification-1</td> </tr> <tr> <td>154.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-m-r50x1-imagenet21k-classification-1</td> </tr> <tr> <td>155.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-m-r50x3-imagenet21k-classification-1</td> </tr> <tr> <td>156.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-m-r101x1-imagenet21k-classification-1</td> </tr> <tr> <td>157.</td> <td>Image Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-ic-bit-m-r101x3-imagenet21k-classification-1</td> </tr> <tr> <td>158.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-alexnet</td> </tr> <tr> <td>159.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-densenet121</td> </tr> <tr> <td>160.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-densenet169</td> </tr> <tr> <td>161.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-densenet201</td> </tr> <tr> <td>162.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-densenet161</td> </tr> <tr> <td>163.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-resnext50-32x4d</td> </tr> <tr> <td>164.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-resnext101-32x8d</td> </tr> <tr> <td>165.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-shufflenet-v2-x1-0</td> </tr> <tr> <td>166.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-squeezenet1-0</td> </tr> <tr> <td>167.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-squeezenet1-1</td> </tr> <tr> <td>168.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-vgg11</td> </tr> <tr> <td>169.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-vgg11-bn</td> </tr> <tr> <td>170.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-vgg13</td> </tr> <tr> <td>171.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-vgg13-bn</td> </tr> <tr> <td>172.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-vgg16</td> </tr> <tr> <td>173.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-vgg16-bn</td> </tr> <tr> <td>174.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-vgg19</td> </tr> <tr> <td>175.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-vgg19-bn</td> </tr> <tr> <td>176.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-wide-resnet50-2</td> </tr> <tr> <td>177.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-wide-resnet101-2</td> </tr> <tr> <td>178.</td> <td>Image Classification</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-ic-googlenet</td> </tr> <tr> <td>179.</td> <td>Object Detection</td> <td>Mxnet</td> <td>This is an object detection model from Gluon CV It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: mxnet-od-ssd-512-vgg16-atrous-coco</td> </tr> <tr> <td>180.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-ssd-512-vgg16-atrous-voc</td> </tr> <tr> <td>181.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-yolo3-darknet53-voc</td> </tr> <tr> <td>182.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-yolo3-mobilenet1-0-voc</td> </tr> <tr> <td>183.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-yolo3-darknet53-coco</td> </tr> <tr> <td>184.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-yolo3-mobilenet1-0-coco</td> </tr> <tr> <td>185.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-faster-rcnn-resnet50-v1b-voc</td> </tr> <tr> <td>186.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-faster-rcnn-resnet50-v1b-coco</td> </tr> <tr> <td>187.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-faster-rcnn-resnet101-v1d-coco</td> </tr> <tr> <td>188.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-faster-rcnn-fpn-resnet50-v1b-coco</td> </tr> <tr> <td>189.</td> <td>Object Detection</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-od-faster-rcnn-fpn-resnet101-v1d-coco</td> </tr> <tr> <td>190.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-mobilenet-v2-fpnlite-320x320-coco17-tpu-8</td> </tr> <tr> <td>191.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-mobilenet-v2-fpnlite-640x640-coco17-tpu-8</td> </tr> <tr> <td>192.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-resnet50-v1-fpn-1024x1024-coco17-tpu-8</td> </tr> <tr> <td>193.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-resnet101-v1-fpn-640x640-coco17-tpu-8</td> </tr> <tr> <td>194.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-resnet101-v1-fpn-1024x1024-coco17-tpu-8</td> </tr> <tr> <td>195.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-resnet152-v1-fpn-640x640-coco17-tpu-8</td> </tr> <tr> <td>196.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od1-ssd-resnet152-v1-fpn-1024x1024-coco17-tpu-8</td> </tr> <tr> <td>197.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>This is an Image Feature Vector model from TensorFlow Hub It takes an image as input and returns a feature vector (embedding) of the image</td> <td>Model ID: tensorflow-icembedding-imagenet-inception-v3-featurevector-4</td> </tr> <tr> <td>198.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-inception-v2-featurevector-4</td> </tr> <tr> <td>199.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-inception-v1-featurevector-4</td> </tr> <tr> <td>200.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-tf2-preview-inception-v3-featurevector-4</td> </tr> <tr> <td>201.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-tf2-preview-mobilenet-v2-featurevector-4</td> </tr> <tr> <td>202.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-resnet-v2-50-featurevector-4</td> </tr> <tr> <td>203.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-resnet-v2-101-featurevector-4</td> </tr> <tr> <td>204.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-resnet-v2-152-featurevector-4</td> </tr> <tr> <td>205.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-resnet-v1-50-featurevector-4</td> </tr> <tr> <td>206.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-resnet-v1-101-featurevector-4</td> </tr> <tr> <td>207.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-resnet-v1-152-featurevector-4</td> </tr> <tr> <td>208.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-resnet-50-featurevector-1</td> </tr> <tr> <td>209.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-b0-featurevector-1</td> </tr> <tr> <td>210.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-b1-featurevector-1</td> </tr> <tr> <td>211.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-b2-featurevector-1</td> </tr> <tr> <td>212.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-b3-featurevector-1</td> </tr> <tr> <td>213.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-b6-featurevector-1</td> </tr> <tr> <td>214.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-lite0-featurevector-2</td> </tr> <tr> <td>215.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-lite1-featurevector-2</td> </tr> <tr> <td>216.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-lite2-featurevector-2</td> </tr> <tr> <td>217.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-lite3-featurevector-2</td> </tr> <tr> <td>218.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-efficientnet-lite4-featurevector-2</td> </tr> <tr> <td>219.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-100-224-featurevector-4</td> </tr> <tr> <td>220.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-100-192-featurevector-4</td> </tr> <tr> <td>221.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-100-160-featurevector-4</td> </tr> <tr> <td>222.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-100-128-featurevector-4</td> </tr> <tr> <td>223.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-075-224-featurevector-4</td> </tr> <tr> <td>224.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-075-192-featurevector-4</td> </tr> <tr> <td>225.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-075-160-featurevector-4</td> </tr> <tr> <td>226.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-075-128-featurevector-4</td> </tr> <tr> <td>227.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-050-224-featurevector-4</td> </tr> <tr> <td>228.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-050-192-featurevector-4</td> </tr> <tr> <td>229.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-050-160-featurevector-4</td> </tr> <tr> <td>230.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-050-128-featurevector-4</td> </tr> <tr> <td>231.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-025-224-featurevector-4</td> </tr> <tr> <td>232.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-025-192-featurevector-4</td> </tr> <tr> <td>233.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-025-160-featurevector-4</td> </tr> <tr> <td>234.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-imagenet-mobilenet-v1-025-128-featurevector-4</td> </tr> <tr> <td>235.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-bit-s-r50x1-ilsvrc2012-featurevector-1</td> </tr> <tr> <td>236.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-bit-s-r50x3-ilsvrc2012-featurevector-1</td> </tr> <tr> <td>237.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-bit-s-r101x1-ilsvrc2012-featurevector-1</td> </tr> <tr> <td>238.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-bit-s-r101x3-ilsvrc2012-featurevector-1</td> </tr> <tr> <td>239.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-bit-m-r50x1-ilsvrc2012-featurevector-1</td> </tr> <tr> <td>240.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-bit-m-r50x3-imagenet21k-featurevector-1</td> </tr> <tr> <td>241.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-bit-m-r101x1-ilsvrc2012-featurevector-1</td> </tr> <tr> <td>242.</td> <td>Image Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-icembedding-bit-m-r101x3-imagenet21k-featurevector-1</td> </tr> <tr> <td>243.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>This is an object detection model from TensorFlow Hub It takes an image as input and returns bounding boxes for the objects in the image</td> <td>Model ID: tensorflow-od-faster-rcnn-resnet50-v1-1024x1024-1</td> </tr> <tr> <td>244.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-resnet101-v1-640x640-1</td> </tr> <tr> <td>245.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-resnet101-v1-800x1333-1</td> </tr> <tr> <td>246.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-resnet101-v1-1024x1024-1</td> </tr> <tr> <td>247.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-resnet152-v1-640x640-1</td> </tr> <tr> <td>248.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-resnet152-v1-800x1333-1</td> </tr> <tr> <td>249.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-resnet152-v1-1024x1024-1</td> </tr> <tr> <td>250.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-inception-resnet-v2-640x640-1</td> </tr> <tr> <td>251.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-faster-rcnn-inception-resnet-v2-1024x1024-1</td> </tr> <tr> <td>252.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-efficientdet-d0-1</td> </tr> <tr> <td>253.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-efficientdet-d1-1</td> </tr> <tr> <td>254.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-efficientdet-d2-1</td> </tr> <tr> <td>255.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-efficientdet-d3-1</td> </tr> <tr> <td>256.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-efficientdet-d4-1</td> </tr> <tr> <td>257.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-efficientdet-d5-1</td> </tr> <tr> <td>258.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-retinanet-resnet50-v1-fpn-640x640-1</td> </tr> <tr> <td>259.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-retinanet-resnet50-v1-fpn-1024x1024-1</td> </tr> <tr> <td>260.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-retinanet-resnet101-v1-fpn-640x640-1</td> </tr> <tr> <td>261.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-retinanet-resnet101-v1-fpn-1024x1024-1</td> </tr> <tr> <td>262.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-retinanet-resnet152-v1-fpn-640x640-1</td> </tr> <tr> <td>263.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-retinanet-resnet152-v1-fpn-1024x1024-1</td> </tr> <tr> <td>264.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-centernet-hourglass-512x512-1</td> </tr> <tr> <td>265.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-centernet-hourglass-512x512-kpts-1</td> </tr> <tr> <td>266.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-centernet-hourglass-1024x1024-1</td> </tr> <tr> <td>267.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-centernet-hourglass-1024x1024-kpts-1</td> </tr> <tr> <td>268.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-centernet-resnet50v1-fpn-512x512-1</td> </tr> <tr> <td>269.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-centernet-resnet50v1-fpn-512x512-kpts-1</td> </tr> <tr> <td>270.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-centernet-resnet50v2-512x512-1</td> </tr> <tr> <td>271.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-centernet-resnet50v2-512x512-kpts-1</td> </tr> <tr> <td>272.</td> <td>Object Detection</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-od-centernet-resnet101v1-fpn-512x512-1</td> </tr> <tr> <td>273.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>This is a Text Classification model built upon a Text Embedding model from TensorFlow Hub It takes a text string as input and classifies the input text as either a positive or negative movie review</td> <td>Model ID: tensorflow-tc-bert-en-cased-L-12-H-768-A-12-2</td> </tr> <tr> <td>274.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-tc-bert-multi-cased-L-12-H-768-A-12-2</td> </tr> <tr> <td>275.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-2-H-128-A-2</td> </tr> <tr> <td>276.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-2-H-256-A-4</td> </tr> <tr> <td>277.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-2-H-512-A-8</td> </tr> <tr> <td>278.</td> <td>Question Answering</td> <td>Huggingface</td> <td>This is an Extractive Question Answering model built on a Transformer model from Hugging Face It takes two strings as inputs: the first string is a question and the second string is the context or any text you want to use to find the answer of the question, and it returns a sub-string from the context as an answer to the question</td> <td>Model ID: huggingface-eqa-distilbert-base-cased</td> </tr> <tr> <td>279.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-eqa-distilbert-base-multilingual-cased</td> </tr> <tr> <td>280.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-eqa-bert-base-uncased</td> </tr> <tr> <td>281.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-eqa-bert-base-cased</td> </tr> <tr> <td>282.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-eqa-bert-base-multilingual-uncased</td> </tr> <tr> <td>283.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>This is a Sentence Pair Classification model built upon a Text Embedding model from TensorFlow Hub It takes a pair of sentences as input and classifies the input pair to 'entailment' or 'no-entailment'</td> <td>Model ID: tensorflow-spc-bert-en-cased-L-12-H-768-A-12-2</td> </tr> <tr> <td>284.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-spc-bert-multi-cased-L-12-H-768-A-12-2</td> </tr> <tr> <td>285.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-spc-bert-en-uncased-L-24-H-1024-A-16-2</td> </tr> <tr> <td>286.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-spc-electra-small-1</td> </tr> <tr> <td>287.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-spc-electra-base-1</td> </tr> <tr> <td>288.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-spc-distilbert-base-cased</td> </tr> <tr> <td>289.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-spc-distilbert-base-multilingual-cased</td> </tr> <tr> <td>290.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-spc-bert-base-uncased</td> </tr> <tr> <td>291.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-spc-bert-base-cased</td> </tr> <tr> <td>292.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-spc-bert-base-multilingual-uncased</td> </tr> <tr> <td>293.</td> <td>Named Entity Recognition</td> <td>Huggingface</td> <td>This is a Named Entity Generation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts named entities in the input text</td> <td>Model ID: huggingface-ner-distilbert-base-uncased-finetuned-conll03-english</td> </tr> <tr> <td>294.</td> <td>Text Generation</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-textgeneration-bloom-1b1</td> </tr> <tr> <td>295.</td> <td>Text Generation</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-textgeneration-bloom-560m</td> </tr> <tr> <td>296.</td> <td>Text Generation</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-textgeneration-gpt2</td> </tr> <tr> <td>297.</td> <td>Text Generation</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-textgeneration-distilgpt2</td> </tr> <tr> <td>298.</td> <td>Text Summarization</td> <td>Huggingface</td> <td>This is a Text Summarization model built upon a Transformer model from Hugging Face It takes a text string as input and returns a summary of the text</td> <td>Model ID: huggingface-summarization-bert-small2bert-small-finetuned-cnn-daily-mail-summarization</td> </tr> <tr> <td>299.</td> <td>Text Summarization</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-summarization-distilbart-cnn-6-6</td> </tr> <tr> <td>300.</td> <td>Text Summarization</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-summarization-distilbart-xsum-12-3</td> </tr> <tr> <td>301.</td> <td>Text Summarization</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-summarization-distilbart-cnn-12-6</td> </tr> <tr> <td>302.</td> <td>Text Summarization</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-summarization-bart-large-cnn-samsum</td> </tr> <tr> <td>303.</td> <td>Machine Translation</td> <td>Huggingface</td> <td>This is a Machine Translation model built upon a Transformer model from Hugging Face It takes a text string as input and predicts its translation</td> <td>Model ID: huggingface-translation-t5-base</td> </tr> <tr> <td>304.</td> <td>Machine Translation</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-translation-t5-large</td> </tr> <tr> <td>305.</td> <td>Machine Translation</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-translation-opus-mt-en-es</td> </tr> <tr> <td>306.</td> <td>Machine Translation</td> <td>Huggingface</td> <td>Same</td> <td>Model ID: huggingface-translation-opus-mt-en-vi</td> </tr> <tr> <td>307.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>This is a Text Embedding model from TensorFlow Hub It takes a text string as input and outputs an embedding vector The Text Embedding model is pre-trained on Wikipedia and BookCorpus datasets</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-2-H-256-A-4</td> </tr> <tr> <td>308.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>same</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-2-H-512-A-8-2</td> </tr> <tr> <td>309.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>same</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-2-H-768-A-12-2</td> </tr> <tr> <td>310.</td> <td>Text to Image</td> <td>Stabilityai</td> <td>This is a text-to-image model from Stability AI and downloaded from HuggingFace It takes a textual description as input and returns a generated image from the description</td> <td>Model ID: model-txt2img-stabilityai-stable-diffusion-v2</td> </tr> <tr> <td>311.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>This is a Text Embedding model from TensorFlow Hub It takes a text string as input and outputs an embedding vector The Text Embedding model is pre-trained on Wikipedia and BookCorpus datasets</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-4-H-128-A-2-2</td> </tr> <tr> <td>312.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-4-H-256-A-4-2</td> </tr> <tr> <td>313.</td> <td>Text Embedding</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-tcembedding-robertafin-base-wiki-uncased</td> </tr> <tr> <td>314.</td> <td>Text Embedding</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-tcembedding-robertafin-large-uncased</td> </tr> <tr> <td>315.</td> <td>Text Embedding</td> <td>Mxnet</td> <td>Same</td> <td>Model ID: mxnet-tcembedding-robertafin-large-wiki-uncased</td> </tr> <tr> <td>316.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>This is a Text Classification model built upon a Text Embedding model from TensorFlow Hub It takes a text string as input and classifies the input text as either a positive or negative movie review</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-2-H-768-A-12</td> </tr> <tr> <td>317.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-4-H-128-A-2</td> </tr> <tr> <td>318.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-4-H-256-A-4</td> </tr> <tr> <td>319.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-4-H-512-A-8</td> </tr> <tr> <td>320.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-4-H-768-A-12</td> </tr> <tr> <td>321.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-6-H-128-A-2</td> </tr> <tr> <td>322.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-6-H-256-A-4</td> </tr> <tr> <td>323.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-6-H-512-A-8</td> </tr> <tr> <td>324.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-6-H-768-A-12</td> </tr> <tr> <td>325.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-8-H-128-A-2</td> </tr> <tr> <td>326.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-8-H-256-A-4</td> </tr> <tr> <td>327.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-8-H-512-A-8</td> </tr> <tr> <td>328.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-8-H-768-A-12</td> </tr> <tr> <td>329.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-10-H-128-A-2</td> </tr> <tr> <td>330.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-10-H-256-A-4</td> </tr> <tr> <td>331.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-10-H-512-A-8</td> </tr> <tr> <td>332.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-10-H-768-A-12</td> </tr> <tr> <td>333.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-12-H-128-A-2</td> </tr> <tr> <td>334.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-12-H-256-A-4</td> </tr> <tr> <td>335.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-12-H-512-A-8</td> </tr> <tr> <td>336.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-small-bert-bert-en-uncased-L-12-H-768-A-12</td> </tr> <tr> <td>337.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-bert-en-uncased-L-24-H-1024-A-16-2</td> </tr> <tr> <td>338.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-bert-en-cased-L-24-H-1024-A-16-2</td> </tr> <tr> <td>339.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-bert-en-wwm-uncased-L-24-H-1024-A-16-2</td> </tr> <tr> <td>340.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-bert-en-wwm-cased-L-24-H-1024-A-16-2</td> </tr> <tr> <td>341.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-albert-en-base</td> </tr> <tr> <td>342.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-electra-small-1</td> </tr> <tr> <td>343.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-electra-base-1</td> </tr> <tr> <td>344.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-experts-bert-wiki-books-1</td> </tr> <tr> <td>345.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-experts-bert-pubmed-1</td> </tr> <tr> <td>346.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-talking-heads-base</td> </tr> <tr> <td>347.</td> <td>Text Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tc-talking-heads-large</td> </tr> <tr> <td>348.</td> <td>Question Answering</td> <td>Huggingface</td> <td>This is an Extractive Question Answering model built on a Transformer model from Hugging Face It takes two strings as inputs: the first string is a question and the second string is the context or any text you want to use to find the answer of the question, and it returns a sub-string from the context as an answer to the question</td> <td>Model ID: huggingface-eqa-bert-base-multilingual-cased</td> </tr> <tr> <td>349.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-eqa-bert-large-uncased</td> </tr> <tr> <td>350.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-eqa-bert-large-cased</td> </tr> <tr> <td>351.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-eqa-bert-large-uncased-whole-word-masking</td> </tr> <tr> <td>352.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-eqa-bert-large-cased-whole-word-masking</td> </tr> <tr> <td>353.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-eqa-distilroberta-base</td> </tr> <tr> <td>354.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-eqa-roberta-base</td> </tr> <tr> <td>355.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-eqa-roberta-base-openai-detector</td> </tr> <tr> <td>356.</td> <td>Question Answering</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-eqa-roberta-large</td> </tr> <tr> <td>357.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>This is a Sentence Pair Classification model built upon a Text Embedding model from TensorFlow Hub It takes a pair of sentences as input and classifies the input pair to 'entailment' or 'no-entailment'</td> <td>Model ID: tensorflow-spc-bert-en-wwm-uncased-L-24-H-1024-A-16-2</td> </tr> <tr> <td>358.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-spc-bert-en-wwm-cased-L-24-H-1024-A-16-2</td> </tr> <tr> <td>359.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-spc-experts-bert-wiki-books-1</td> </tr> <tr> <td>360.</td> <td>Sentence Pair Classification</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-spc-experts-bert-pubmed-1</td> </tr> <tr> <td>361.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-bert-base-multilingual-cased</td> </tr> <tr> <td>362.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-bert-large-uncased</td> </tr> <tr> <td>363.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-bert-large-cased</td> </tr> <tr> <td>364.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-bert-large-uncased-whole-word-masking</td> </tr> <tr> <td>365.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-bert-large-cased-whole-word-masking</td> </tr> <tr> <td>366.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-distilroberta-base</td> </tr> <tr> <td>367.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-roberta-base</td> </tr> <tr> <td>368.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-roberta-base-openai-detector</td> </tr> <tr> <td>369.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-roberta-large</td> </tr> <tr> <td>370.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-roberta-large-openai-detector</td> </tr> <tr> <td>371.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-xlm-mlm-ende-1024</td> </tr> <tr> <td>372.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-xlm-mlm-enro-1024</td> </tr> <tr> <td>373.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-xlm-mlm-xnli15-1024</td> </tr> <tr> <td>374.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-xlm-mlm-tlm-xnli15-1024</td> </tr> <tr> <td>375.</td> <td>Sentence Pair Classification</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-spc-xlm-clm-ende-1024</td> </tr> <tr> <td>376.</td> <td>Text Summarization</td> <td>Huggingface</td> <td>This is a Text Summarization model built upon a Transformer model from Hugging Face It takes a text string as input and returns a summary of the text</td> <td>Model ID: huggingface-summarization-bigbird-pegasus-large-arxiv</td> </tr> <tr> <td>377.</td> <td>Text Summarization</td> <td>Huggingface</td> <td>Same as above</td> <td>Model ID: huggingface-summarization-bigbird-pegasus-large-pubmed</td> </tr> <tr> <td>378.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>This is a Text Embedding model from TensorFlow Hub It takes a text string as input and outputs an embedding vector The Text Embedding model is pre-trained on Wikipedia and BookCorpus datasets</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-4-H-512-A-8-2</td> </tr> <tr> <td>379.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-4-H-768-A-12-2</td> </tr> <tr> <td>380.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-6-H-128-A-2-2</td> </tr> <tr> <td>381.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-6-H-256-A-4</td> </tr> <tr> <td>382.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-6-H-512-A-8-2</td> </tr> <tr> <td>383.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-6-H-768-A-12-2</td> </tr> <tr> <td>384.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-8-H-256-A-4-2</td> </tr> <tr> <td>385.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-8-H-512-A-8-2</td> </tr> <tr> <td>386.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-8-H-768-A-12-2</td> </tr> <tr> <td>387.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-10-H-128-A-2-2</td> </tr> <tr> <td>388.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-10-H-256-A-4-2</td> </tr> <tr> <td>389.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-10-H-512-A-8-2</td> </tr> <tr> <td>390.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-10-H-768-A-12-2</td> </tr> <tr> <td>391.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-12-H-128-A-2-2</td> </tr> <tr> <td>392.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-12-H-256-A-4</td> </tr> <tr> <td>393.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-12-H-512-A-8-2</td> </tr> <tr> <td>394.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-12-H-768-A-12-2</td> </tr> <tr> <td>395.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-en-uncased-L-12-H-768-A-12-4</td> </tr> <tr> <td>396.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-wiki-books-sst2</td> </tr> <tr> <td>397.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-bert-wiki-books-mnli-2</td> </tr> <tr> <td>398.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-universal-sentence-encoder-cmlm-en-large-1</td> </tr> <tr> <td>399.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-universal-sentence-encoder-cmlm-en-base-1</td> </tr> <tr> <td>400.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-talkheads-ggelu-bert-en-base-2</td> </tr> <tr> <td>401.</td> <td>Text Embedding</td> <td>Tensorflow</td> <td>Same as above</td> <td>Model ID: tensorflow-tcembedding-talkheads-ggelu-bert-en-large-2</td> </tr> <tr> <td>402.</td> <td>Tabular Classification</td> <td></td> <td>This is the LightGBM algorithm for tabular classification task LightGBM is a gradient boosting framework that uses tree based learning algorithms</td> <td>Model ID: lightgbm-classification-model</td> </tr> <tr> <td>403.</td> <td>Tabular Classification</td> <td>Catboost</td> <td>This is the CatBoost algorithm for tabular classification task CatBoost is a machine learning algorithm that uses gradient boosting on decision trees</td> <td>Model ID: catboost-classification-model</td> </tr> <tr> <td>404.</td> <td>Tabular Classification</td> <td></td> <td>This is the AutoGluon-Tabular algorithm for tabular classification task AutoGluon-Tabular is an open-source AutoML framework that trains highly accurate machine learning models on an unprocessed tabular dataset Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers</td> <td>Model ID: autogluon-classification-ensemble</td> </tr> <tr> <td>405.</td> <td>Tabular Classification</td> <td></td> <td>This is the TabTransformer algorithm for tabular classification task TabTransformer is a deep tabular data modeling architecture that built upon self-attention based Transformers</td> <td>Model ID: pytorch-tabtransformerclassification-model</td> </tr> <tr> <td>406.</td> <td>Tabular Classification</td> <td>Sklearn</td> <td>This is the scikit-learn linear algorithm for tabular classification task Linear Classification is a linear approach to classify data into labels (targets) based on a linear combination of its input features (predictors)</td> <td>Model ID: sklearn-classification-linear</td> </tr> <tr> <td>407.</td> <td>Tabular Classification</td> <td>Xgboost</td> <td>This is the XGBoost algorithm for tabular classification task XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable It implements machine learning algorithms under the Gradient Boosting framework</td> <td>Model ID: xgboost-classification-model</td> </tr> <tr> <td>408.</td> <td>Tabular Regression</td> <td></td> <td>This is the LightGBM algorithm for tabular regression task LightGBM is a gradient boosting framework that uses tree based learning algorithms</td> <td>Model ID: lightgbm-regression-model</td> </tr> <tr> <td>409.</td> <td>Tabular Regression</td> <td>Catboost</td> <td>This is the CatBoost algorithm for tabular regression task CatBoost is a machine learning algorithm that uses gradient boosting on decision trees</td> <td>Model ID: catboost-regression-model</td> </tr> <tr> <td>410.</td> <td>Tabular Regression</td> <td></td> <td>This is the AutoGluon-Tabular algorithm for tabular regression task AutoGluon-Tabular is an open-source AutoML framework that trains highly accurate machine learning models on an unprocessed tabular dataset Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers</td> <td>Model ID: autogluon-regression-ensemble</td> </tr> <tr> <td>411.</td> <td>Tabular Regression</td> <td></td> <td>This is the TabTransformer algorithm for tabular regression task TabTransformer is a deep tabular data modeling architecture that built upon self-attention based Transformers</td> <td>Model ID: pytorch-tabtransformerregression-model</td> </tr> <tr> <td>412.</td> <td>Tabular Regression</td> <td>Sklearn</td> <td>This is the scikit-learn linear algorithm for tabular regression task Linear Regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables</td> <td>Model ID: sklearn-regression-linear</td> </tr> <tr> <td>413.</td> <td>Tabular Regression</td> <td>Xgboost</td> <td>This is the XGBoost algorithm for tabular regression task XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable It implements machine learning algorithms under the Gradient Boosting framework</td> <td>Model ID: xgboost-regression-model</td> </tr> <tr> <td>414.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-distilbert-base-uncased</td> </tr> <tr> <td>415.</td> <td>Question Answering</td> <td>Pytorch</td> <td>Same as above</td> <td>Model ID: pytorch-eqa-bert-large-uncased-whole-word-masking</td> </tr> <tr> <td>416.</td> <td>Question Answering</td> <td>Pytorch</td> <td>Same as above</td> <td>Model ID: pytorch-eqa-bert-large-uncased</td> </tr> <tr> <td>417.</td> <td>Question Answering</td> <td>Pytorch</td> <td>Same as above</td> <td>Model ID: pytorch-eqa-bert-large-cased</td> </tr> <tr> <td>418.</td> <td>Question Answering</td> <td>Pytorch</td> <td>Same as above</td> <td>Model ID: pytorch-eqa-roberta-base</td> </tr> <tr> <td>419.</td> <td>Question Answering</td> <td>Pytorch</td> <td>Same as above</td> <td>Model ID: pytorch-eqa-distilbert-base-multilingual-cased</td> </tr> <tr> <td>420.</td> <td>Object detection</td> <td>SageMaker</td> <td>Identify birds species in a scene using a SageMaker object detection model.</td> <td></td> </tr> <tr> <td>421.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-distilroberta-base</td> </tr> <tr> <td>422.</td> <td>Audio Embedding</td> <td>Tensorflow</td> <td>This is an audio embedding model from Tensorflow Hub It takes a wav (audio file format) file as input and outputs an embedding vector</td> <td>Model ID: tensorflow-audioembedding-trill-distilled-3</td> </tr> <tr> <td>423.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-roberta-large-openai-detector</td> </tr> <tr> <td>424.</td> <td>Object detection</td> <td>SageMaker</td> <td>Identify defective regions in product images either by training an object detection model from scratch or fine-tuning pretrained SageMaker models.</td> <td></td> </tr> <tr> <td>425.</td> <td>Audio Embedding</td> <td>Tensorflow</td> <td>This is an audio embedding model from Tensorflow Hub It takes a wav (audio file format) file as input and outputs an embedding vector</td> <td>Model ID: tensorflow-audioembedding-trillsson2-1</td> </tr> <tr> <td>426.</td> <td>Tabular classification</td> <td>SageMaker</td> <td>Automatically detect potentially fraudulent activity in transactions using SageMaker XGBoost with the over-sampling technique Synthetic Minority Over-sampling (SMOTE).</td> <td></td> </tr> <tr> <td>427.</td> <td>Feature importance using shap</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>428.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-distilbert-base-cased</td> </tr> <tr> <td>429.</td> <td>Graph neural network classification</td> <td>SageMaker</td> <td>Detect fraud in financial transactions by training a graph convolutional network with the deep graph library and a SageMaker XGBoost model.</td> <td></td> </tr> <tr> <td>430.</td> <td>Tabular classification</td> <td>SageMaker</td> <td>Classify financial payments based on transaction information using SageMaker XGBoost. Use this solution template as an intermediate step in fraud detection, personalization, or anomaly detection.</td> <td></td> </tr> <tr> <td>431.</td> <td>Tabular classification</td> <td>SageMaker</td> <td>Identify unhappy mobile phone customers using SageMaker XGBoost.</td> <td></td> </tr> <tr> <td>432.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-bert-base-cased</td> </tr> <tr> <td>433.</td> <td>RL</td> <td>SageMaker</td> <td>Distributed reinforcement learning starter kit for NeurIPS 2020 Procgen Reinforcement learning challenge.</td> <td></td> </tr> <tr> <td>434.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-bert-large-cased-whole-word-masking-finetuned-squad</td> </tr> <tr> <td>435.</td> <td>Tabular classification</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>436.</td> <td>RL</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>437.</td> <td>Entity resolution</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>438.</td> <td>Tabular classification</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>439.</td> <td>Tabular and text classification</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>440.</td> <td>Text classification</td> <td>SageMaker</td> <td>Anonymize text to better preserve user privacy in sentiment classification.</td> <td></td> </tr> <tr> <td>441.</td> <td>Tabular, image, and text classification.</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>442.</td> <td>Tabular classification</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>443.</td> <td>Text to Image</td> <td>Stabilityai</td> <td>This is a text-to-image model from Stability AI and downloaded from HuggingFace It takes a textual description as input and returns a generated image from the description</td> <td>Model ID: model-txt2img-stabilityai-stable-diffusion-v2-fp16</td> </tr> <tr> <td>444.</td> <td>Text to Image</td> <td>Stabilityai</td> <td>Same</td> <td>Model ID: model-txt2img-stabilityai-stable-diffusion-v1-4-fp16</td> </tr> <tr> <td>445.</td> <td>ext to Image</td> <td>Stabilityai</td> <td>Same</td> <td>Model ID: model-txt2img-stabilityai-stable-diffusion-v1-4</td> </tr> <tr> <td>446.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-bert-base-multilingual-cased</td> </tr> <tr> <td>447.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-roberta-large</td> </tr> <tr> <td>448.</td> <td>Audio Embedding</td> <td>Tensorflow</td> <td>This is an audio embedding model from Tensorflow Hub It takes a wav (audio file format) file as input and outputs an embedding vector</td> <td>Model ID: tensorflow-audioembedding-frill-1</td> </tr> <tr> <td>449.</td> <td>Audio Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-audioembedding-trillsson3-1</td> </tr> <tr> <td>450.</td> <td>Audio Embedding</td> <td>Tensorflow</td> <td>Same</td> <td>Model ID: tensorflow-audioembedding-trill-3</td> </tr> <tr> <td>451.</td> <td>Tabular and text classification</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>452.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-roberta-base-openai-detector</td> </tr> <tr> <td>453.</td> <td>Question Answering</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-eqa-bert-large-cased-whole-word-masking</td> </tr> <tr> <td>454.</td> <td>Time series</td> <td>SageMaker</td> <td>Demand forecasting for multivariate time series data using three state-of-the-art time series forecasting algorithms: LSTNet, Prophet, and SageMaker DeepAR.</td> <td></td> </tr> <tr> <td>455.</td> <td>Question Answering</td> <td>Pytorch</td> <td>This is a Extractive Question Answering model built upon a Text Embedding model from PyTorch Hub It takes as input a pair of question-context strings, and returns a sub-string from the context as a answer to the question</td> <td>Model ID: pytorch-eqa-bert-large-uncased-whole-word-masking-finetuned-squad</td> </tr> <tr> <td>456.</td> <td>Question Answering</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-eqa-bert-base-multilingual-uncased</td> </tr> <tr> <td>457.</td> <td>Question Answering</td> <td>Pytorch</td> <td>Same</td> <td>Model ID: pytorch-eqa-bert-base-uncased</td> </tr> <tr> <td>458.</td> <td>Audio Embedding</td> <td>Tensorflow</td> <td>This is an audio embedding model from Tensorflow Hub It takes a wav (audio file format) file as input and outputs an embedding vector</td> <td>Model ID: tensorflow-audioembedding-trillsson1-1</td> </tr> <tr> <td>459.</td> <td>Object detection</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>460.</td> <td>Causal inference</td> <td>SageMaker</td> <td>Generate a counterfactual analysis of corn response to nitrogen. This solution learns the crop phenology cycle in its entirety using multi-spectral satellite imagery and ground-level observations.</td> <td></td> </tr> <tr> <td>461.</td> <td>Price optimization</td> <td>SageMaker</td> <td>Estimate price elasticity using Double Machine Learning (ML) for causal inference and the Prophet forecasting procedure. Use these estimates to optimize daily prices.</td> <td></td> </tr> <tr> <td>462.</td> <td>Tabular and text classification</td> <td>SageMaker</td> <td></td> <td></td> </tr> <tr> <td>463.</td> <td>Upscaling</td> <td>Stabilityai</td> <td>This is a upscaling model from Stability AI downloaded from HuggingFace with FP16 precision Given a low resolution image and a textual prompt, it generates a higher resolution image with size up to four times the original image size</td> <td>Model ID: model-upscaling-stabilityai-stable-diffusion-x4-upscaler-fp16</td> </tr> </tbody> </table> <h2 id=references>References<a class=headerlink href=#references title="Permanent link">&para;</a></h2> <ul> <li><a href=https://aws.amazon.com/sagemaker/jumpstart/getting-started/ >https://aws.amazon.com/sagemaker/jumpstart/getting-started/</a></li> </ul> <p><strong>Author</strong> <br> Dr. Hari Thapliyaal <br> dasarpai.com <br> linkedin.com/in/harithapliyal </p> <div class=author-bio style="margin-top: 2rem; display: flex; gap: 1rem;"> <img src=../assets/images/myphotos/Profilephoto1.jpg alt="Hari Thapliyaal" style="border-radius: 50%; width: 80px; height: 80px;"> <div> <strong>Dr. Hari Thapliyaal</strong><br> <small>Dr. Hari Thapliyal is a prolific blogger and seasoned professional with an extensive background in Data Science, Project Management, and Advait-Vedanta Philosophy. He holds a Doctorate in AI/NLP from SSBM, Geneva, along with Master’s degrees in Computers, Business Management, Data Science, and Economics. With over three decades of experience in management and leadership, Hari has extensive expertise in training, consulting, and coaching within the technology sector. His specializations include Data Science, AI, Computer Vision, NLP, and machine learning. Hari is also passionate about meditation and nature, often retreating to secluded places for reflection and peace.</small> </div> </div> <div class=share-buttons style="margin-top: 2rem;"> <strong>Share this article:</strong><br> <a href="https://twitter.com/intent/tweet?text=AWS%20SageMaker%20Jumpstart%20Models&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/AWS-Sagemaker-Jumpstart-Models.html" target=_blank>Twitter</a> | <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/AWS-Sagemaker-Jumpstart-Models.html" target=_blank>Facebook</a> | <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//dasarpai.github.io/dasarpai-mkdocs/dsblog/AWS-Sagemaker-Jumpstart-Models.html" target=_blank>LinkedIn</a> </div> </article> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.js></script> </body> </html>