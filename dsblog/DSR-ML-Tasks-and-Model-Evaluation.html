<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/DSR-ML-Tasks-and-Model-Evaluation.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Machine Learning Tasks and Model Evaluation - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#machine-learning-tasks-and-model-evaluation class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Machine Learning Tasks and Model Evaluation </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=#what-is-bleu-benchmark class=md-nav__link> <span class=md-ellipsis> What is BLEU Benchmark? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-glue-benchmark class=md-nav__link> <span class=md-ellipsis> What is GLUE Benchmark? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-the-tasks-of-glue-benchmark class=md-nav__link> <span class=md-ellipsis> What are the Tasks of GLUE Benchmark? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-superglue-benchmark class=md-nav__link> <span class=md-ellipsis> What is SuperGLUE Benchmark? </span> </a> </li> <li class=md-nav__item> <a href=#glue-superglue-tasks class=md-nav__link> <span class=md-ellipsis> GLUE &amp; SuperGLUE tasks </span> </a> <nav class=md-nav aria-label="GLUE & SuperGLUE tasks"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-the-difference-between-bleu-and-glue class=md-nav__link> <span class=md-ellipsis> What is the difference between BLEU and GLUE? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-is-meteor-score class=md-nav__link> <span class=md-ellipsis> What is METEOR Score? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-xtreme-benchmark class=md-nav__link> <span class=md-ellipsis> What is XTREME Benchmark? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-rouge-score class=md-nav__link> <span class=md-ellipsis> What is ROUGE Score? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-big-bench class=md-nav__link> <span class=md-ellipsis> What is BIG-Bench? </span> </a> </li> <li class=md-nav__item> <a href=#deep-learning-tasks-models-on-huggingface-100k-models class=md-nav__link> <span class=md-ellipsis> Deep Learning Tasks &amp; Models on Huggingface (100K Models) </span> </a> <nav class=md-nav aria-label="Deep Learning Tasks & Models on Huggingface (100K Models)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#computer-vision-models-6000-models class=md-nav__link> <span class=md-ellipsis> Computer Vision Models, 6000+ Models </span> </a> </li> <li class=md-nav__item> <a href=#natural-language-processing-models-65000-models class=md-nav__link> <span class=md-ellipsis> Natural Language Processing Models, 65000+ Models </span> </a> </li> <li class=md-nav__item> <a href=#audio-models-10000-models class=md-nav__link> <span class=md-ellipsis> Audio Models, 10000+ Models </span> </a> </li> <li class=md-nav__item> <a href=#multimodal-models-9000-models class=md-nav__link> <span class=md-ellipsis> Multimodal Models, 9000+ Models </span> </a> </li> <li class=md-nav__item> <a href=#reinforcement-learning-22000-models class=md-nav__link> <span class=md-ellipsis> Reinforcement Learning, 22000+ Models </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/DSR-ML-Tasks-and-Model-Evaluation.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/DSR-ML-Tasks-and-Model-Evaluation.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="Deep Learning Tasks and Models" src=../assets/images/dsresources/dsr114-ml-tasks-and-model-evaluation.jpg> </p> <h1 id=machine-learning-tasks-and-model-evaluation>Machine Learning Tasks and Model Evaluation<a class=headerlink href=#machine-learning-tasks-and-model-evaluation title="Permanent link">&para;</a></h1> <h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h2> <p>Machine learning is a subject where we study how to create &amp; evaluate machine learning models. To create these models, we need different types of data. We build models which can help us do various kinds of tasks. There are hundreds of model building techniques and researchers keep adding new techniques, and architectures as when need arises. But, the question is how do you evaluate these models which are output of the model trainings? To evaluate the performance of a model on structured data, or classification/regression/clustering models, we require one kind of metrics. But this becomes complicated when we are dealing with voice, text and audio data. How do you evaluate ten models which are responsible for translation, or locating an object in the image, transcribing voice into text, captioning an image? To solve this problem, standard databases are created and everyone needs to demonstrate the performance of their model, architecture, or approach against that dataset. But, even if you have a baseline dataset, how will you evaluate various NLP or deep learning tasks? For that GLUE, SuperGLUE benchmarks are created.</p> <h2 id=what-is-bleu-benchmark>What is BLEU Benchmark?<a class=headerlink href=#what-is-bleu-benchmark title="Permanent link">&para;</a></h2> <p>BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of machine translation. It is based on the n-gram precision between the machine translation and the reference translation.</p> <p>BLEU-1 measures the unigram precision, which is the fraction of unigrams in the machine translation that also appear in the reference translation. A BLEU-1 score of 1 means that the machine translation is a perfect match for the reference translation.</p> <p>BLEU-1 is a simple and easy-to-understand metric, but it can be biased towards shorter translations. For example, a machine translation that is twice as long as the reference translation will have a lower BLEU-1 score, even if it is more accurate.</p> <p>To address this, BLEU-n metrics were developed, which measure the n-gram precision for n&gt;1. BLEU-4 is the most commonly used BLEU-n metric. A BLEU-4 score of 1 means that the machine translation is a perfect match for the reference translation. However, BLEU-4 scores are typically much lower than 1, and a score of 0.5 or higher is considered to be good.</p> <p>BLEU-4 is a simple and easy-to-understand metric, but it can be biased towards shorter translations. For example, a machine translation that is twice as long as the reference translation will have a lower BLEU-4 score, even if it is more accurate.</p> <p>Steps to calculate BlLEU score are as following. 1. Step1: Calculate N-gram Precision: BLEU calculates precision scores based on matching n-grams (sequences of n words) between the candidate translation and the reference translations. It computes precision scores for 1-gram (unigram), 2-gram (bigram), 3-gram, and 4-gram matches. 2. Step2: Brevity Penalty: BLEU also takes into account the length of the candidate translation compared to the reference translations. This is because shorter translations tend to have an advantage in n-gram matching, but they might not convey the full meaning of the source text. BLEU applies a brevity penalty to avoid favoring overly short translations. 3. Step3: Compute Geometric Mean: BLEU combines the precision scores of different n-gram matches using a geometric mean. This is done to give a balanced consideration to different n-gram orders. This metric helps in capturing both local and global translation quality. 4. Step4: Score Calculation: The BLEU score is calculated by multiplying the geometric mean of the n-gram precisions by the brevity penalty. The result is a value between 0 and 1, where a higher score indicates a better match between the candidate translation and the reference translations.</p> <p>It's important to note that while BLEU is widely used for evaluating machine translation systems, it has some limitations. For instance, BLEU relies solely on n-gram matching and does not capture higher-level semantic or syntactic aspects of translation quality. As a result, it may not always align well with human judgments of translation quality, especially when dealing with creative or complex translations.</p> <p>Despite its limitations, BLEU remains a widely used and easily computable metric for quick and automated evaluation of machine translation outputs. Researchers often combine BLEU with other metrics or use it as a starting point for evaluation, but more advanced metrics have been developed over time to address its limitations.</p> <p>Advantages of using BLEU score: - It is a simple and easy-to-understand metric. - It is relatively insensitive to changes in word order. - It has been shown to be effective in evaluating the performance of machine translation.</p> <p>Disadvantages of using BLEU score: - It can be biased towards shorter translations. - It does not take into account the semantic similarity between the machine translation and the reference translation. - It can be difficult to interpret the results of BLEU score for different tasks.</p> <h2 id=what-is-glue-benchmark>What is GLUE Benchmark?<a class=headerlink href=#what-is-glue-benchmark title="Permanent link">&para;</a></h2> <p>The <strong>GLUE (General Language Understanding Evaluation)</strong> benchmark is a collection of diverse natural language processing (NLP) tasks designed to evaluate and compare the performance of various machine learning models and techniques in understanding and processing human language. It serves as a standard evaluation framework for assessing the general language understanding capabilities of different models.</p> <p>The GLUE benchmark was <strong>introduced in 2018</strong> and consists of a set of <strong>nine different NLP tasks</strong>, covering a wide range of language understanding tasks including sentence classification, sentence similarity, natural language inference, and question answering. Some of the tasks included in GLUE are the Stanford Sentiment Treebank, Multi-Genre Natural Language Inference, and the Recognizing Textual Entailment dataset.</p> <p>The primary goal of the GLUE benchmark is to encourage the development of models that can perform well across multiple NLP tasks, thus demonstrating a more comprehensive understanding of human language. The performance of models is measured using a single metric called the GLUE score, which is computed by aggregating the performance of models on individual tasks.</p> <p>The GLUE benchmark has been instrumental in advancing the field of NLP and has served as a benchmark for many state-of-the-art models, including various transformer-based architectures like BERT (Bidirectional Encoder Representations from Transformers) and RoBERTa.</p> <p>It's worth noting that since the introduction of the GLUE benchmark, other benchmarks like SuperGLUE and XTREME have been developed to address some limitations and provide more challenging evaluation tasks for further advancing the state of NLP research. <br> - <a href=https://gluebenchmark.com/ >https://gluebenchmark.com/</a> - <a href=https://gluebenchmark.com/leaderboard>https://gluebenchmark.com/leaderboard</a> - <a href=https://paperswithcode.com/dataset/glue>https://paperswithcode.com/dataset/glue</a></p> <p>The GLUE benchmark is used by companies to evaluate the performance of their NLU models. For example, Google uses the GLUE benchmark to evaluate the performance of its BERT model. The higher the GLUE score, the better the overall performance of the model on various NLP tasks.</p> <h2 id=what-are-the-tasks-of-glue-benchmark>What are the Tasks of GLUE Benchmark?<a class=headerlink href=#what-are-the-tasks-of-glue-benchmark title="Permanent link">&para;</a></h2> <p>The current version of the GLUE benchmark includes the following 9 tasks - CoLA: A sentence-level grammaticality task. - SST-2: A binary sentiment classification task. - MRPC: A sentence-pair similarity task. - STS-B: A sentence-pair similarity task that measures the semantic relatedness of two sentences. - QQP: A question-answering task. - MNLI: A natural language inference task that measures whether a sentence entails another sentence. - QNLI: A natural language inference task that measures whether a sentence entails a question. - RTE: A natural language inference task that measures whether a sentence contradicts another sentence. - WNLI: A word similarity task that measures the similarity between two words.</p> <h2 id=what-is-superglue-benchmark>What is SuperGLUE Benchmark?<a class=headerlink href=#what-is-superglue-benchmark title="Permanent link">&para;</a></h2> <p>The SuperGLUE (Super General Language Understanding Evaluation) benchmark is an enhanced version of the GLUE benchmark introduced to address its limitations and provide more challenging language understanding tasks for evaluating and comparing the performance of natural language processing (NLP) models. SuperGLUE builds upon the success of GLUE and aims to push the boundaries of NLP research further.</p> <p>SuperGLUE was <strong>introduced in 2019</strong> as an extension of GLUE, consisting of a more diverse and difficult set of language understanding tasks. It includes a <strong>total of eight challenging tasks</strong>, including tasks like BoolQ (Boolean Questions), COPA (Choice of Plausible Alternatives), and RTE (Recognizing Textual Entailment), among others. These tasks are carefully designed to require more advanced reasoning and understanding abilities from models.</p> <p>The primary objective of SuperGLUE is to evaluate models on a more comprehensive set of tasks that demand higher levels of language comprehension and reasoning capabilities. It provides a broader and more challenging evaluation platform to assess the progress and performance of NLP models beyond what was covered by the original GLUE benchmark.</p> <p>Similar to GLUE, SuperGLUE also utilizes a single evaluation metric called the SuperGLUE score to assess model performance across the different tasks. The SuperGLUE benchmark has spurred further research and development in the field of NLP, pushing for advancements in model architectures, training techniques, and performance improvements.</p> <p>SuperGLUE has become a prominent benchmark for evaluating the state-of-the-art NLP models, building on the success of GLUE and encouraging the development of more sophisticated models that can tackle complex language understanding tasks.</p> <p>It's important to note that the SuperGLUE benchmark, while providing more challenging tasks, is still evolving, and researchers continue to work on expanding and refining the benchmark to further push the boundaries of NLP research. <br> - <a href=https://super.gluebenchmark.com/ >https://super.gluebenchmark.com/</a> - <a href=https://super.gluebenchmark.com/leaderboard>https://super.gluebenchmark.com/leaderboard</a> - <a href=https://paperswithcode.com/dataset/superglue>https://paperswithcode.com/dataset/superglue</a></p> <h2 id=glue-superglue-tasks>GLUE &amp; SuperGLUE tasks<a class=headerlink href=#glue-superglue-tasks title="Permanent link">&para;</a></h2> <p>Below is list of different NLP and Deep Learning tasks for which different benchmark datasets are created and model's perormance is measured against those tasks.</p> <table> <thead> <tr> <th>Sno</th> <th>Task</th> <th>Corpus</th> <th>Paper</th> <th>GLUE</th> <th>SuperGLUE</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>Sentence acceptability judgment</td> <td><a href=https://nyu-mll.github.io/CoLA/ >CoLA (Corpus of Linguistic Acceptability)</a></td> <td><a href=https://arxiv.org/abs/1805.12471>CoLA Warstadt et al., 2018</a></td> <td>Yes</td> <td>No</td> </tr> <tr> <td>2</td> <td>Sentiment analysis</td> <td><a href=https://nlp.stanford.edu/sentiment/index.html>SST-2 (Stanford Sentiment Treebank)</a></td> <td><a href=https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf>SST-2 Socher et al., 2013</a></td> <td>Yes</td> <td>No</td> </tr> <tr> <td>3</td> <td>Paraphrasing/sentence similarity</td> <td><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52398">MRPC (Microsoft Research Paraphrase Corpus)</a></td> <td><a href=https://aclanthology.org/I05-5002>MRPC Dolan and Brockett, 2005</a></td> <td>Yes</td> <td>No</td> </tr> <tr> <td>4</td> <td></td> <td><a href=http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark>STS-B (Semantic Textual Similarity Benchmark)</a></td> <td><a href=https://arxiv.org/abs/1708.00055>STS-B Ceret al., 2017</a></td> <td>Yes</td> <td>No</td> </tr> <tr> <td>5</td> <td></td> <td><a href=https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs>QQP (Quora Question Pairs)</a></td> <td><a href=https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs>QQP Iyer et al., 2017</a></td> <td>Yes</td> <td>No</td> </tr> <tr> <td>6</td> <td>Natural language inference</td> <td><a href=https://arxiv.org/abs/1704.05426>MNLI (Multi-Genre Natural Language Inference)</a></td> <td><a href=https://arxiv.org/abs/1704.05426>MNLI Williams et al., 2017</a></td> <td>Yes</td> <td>No</td> </tr> <tr> <td>7</td> <td></td> <td><a href=https://rajpurkar.github.io/SQuAD-explorer/ >QNLI (Question-answering Natural Language Inference)</a></td> <td><a href=https://arxiv.org/abs/1606.05250>QNLI Rajpurkar et al.,2016</a></td> <td>Yes</td> <td>No</td> </tr> <tr> <td>8</td> <td></td> <td><a href=https://aclweb.org/aclwiki/Recognizing_Textual_Entailment>RTE (Recognizing Textual Entailment)</a></td> <td><a href=https://link.springer.com/chapter/10.1007/11736790_9>RTE Dagan et al., 2005</a></td> <td>Yes</td> <td>Yes</td> </tr> <tr> <td>9</td> <td></td> <td><a href=https://github.com/mcdm/CommitmentBank>CB (CommitmentBank)</a></td> <td><a href=https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf>CB De Marneff et al., 2019</a></td> <td>No</td> <td>Yes</td> </tr> <tr> <td>10</td> <td>Sentence completion</td> <td><a href=https://people.ict.usc.edu/~gordon/copa.html>COPA (Choice of Plausible Alternatives)</a></td> <td><a href=https://www.researchgate.net/publication/221251392_Choice_of_Plausible_Alternatives_An_Evaluation_of_Commonsense_Causal_Reasoning>COPA Roemmele et al., 2011</a></td> <td>No</td> <td>Yes</td> </tr> <tr> <td>11</td> <td>Word sense disambiguation</td> <td><a href=https://pilehvar.github.io/wic/ >WiC (Word-in-Context)</a></td> <td><a href=https://arxiv.org/abs/1808.09121>WIC Pilehvar and Camacho-Collados, 2018</a></td> <td>No</td> <td>Yes</td> </tr> <tr> <td>12</td> <td></td> <td><a href=https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html>WNLI (Winograd Natural Language Inference)</a></td> <td><a href></a></td> <td>Yes</td> <td>No</td> </tr> <tr> <td>13</td> <td>Question answering</td> <td><a href=https://cogcomp.org/multirc/ >MultiRC (Multi-sentence Reading Comprehension)</a></td> <td><a href=https://aclanthology.org/N18-1023>MultiRC Khashabi et al., 2018</a></td> <td>No</td> <td>Yes</td> </tr> <tr> <td>14</td> <td></td> <td><a href=https://sheng-z.github.io/ReCoRD-explorer/ >ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset)</a></td> <td><a href=https://arxiv.org/abs/1810.12885>ReCoRD Zhang et al., 2018</a></td> <td>No</td> <td>Yes</td> </tr> <tr> <td>15</td> <td></td> <td><a href=https://github.com/google-research-datasets/boolean-questions>BoolQ (Boolean Questions)</a></td> <td><a href=https://arxiv.org/abs/1905.10044>BoolQ Clark et al., 2019</a></td> <td>No</td> <td>Yes</td> </tr> <tr> <td>16</td> <td>Common Sense Reasoning</td> <td><a href=https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html>WSC (Winograd Schema Challenge)</a></td> <td><a href=https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html></a></td> <td>No</td> <td>Yes</td> </tr> <tr> <td>17</td> <td></td> <td><a href>AX-b : Broadcoverage Diagnostic - Mathew's Corr</a></td> <td><a href></a></td> <td>No</td> <td>Yes</td> </tr> <tr> <td>18</td> <td></td> <td><a href>AX-g : Winogender Shema Diagnostic Gender Parity - Accuracy</a></td> <td><a href></a></td> <td>No</td> <td>Yes</td> </tr> </tbody> </table> <h3 id=what-is-the-difference-between-bleu-and-glue>What is the difference between BLEU and GLUE?<a class=headerlink href=#what-is-the-difference-between-bleu-and-glue title="Permanent link">&para;</a></h3> <p>The <strong>BLEU (Bilingual Evaluation Understudy)</strong> score and the <strong>GLUE (General Language Understanding Evaluation)</strong> score are two different evaluation metrics used in the field of natural language processing (NLP), and they serve different purposes and evaluate different aspects of NLP models.</p> <p>BLEU is a metric commonly used to evaluate the <strong>quality of machine translation</strong> systems. It measures the similarity between the machine-generated translations and reference translations provided by human translators. BLEU score is <strong>based on n-gram precision</strong>, where it compares the n-gram sequences (typically up to 4-grams) between the machine-generated output and the reference translations. It assigns a score between 0 and 1, with a higher score indicating better translation quality.</p> <p>GLUE, on the other hand, is a benchmark for evaluating the performance of NLP models on a range of language understanding tasks. It provides a single-number evaluation metric that aggregates the performance of a model across multiple tasks, including textual entailment, sentiment analysis, question-answering, and more. The GLUE score is a weighted average of the model's performance on each of these tasks, with higher scores indicating better overall language understanding capabilities.</p> <h2 id=what-is-meteor-score>What is METEOR Score?<a class=headerlink href=#what-is-meteor-score title="Permanent link">&para;</a></h2> <p><strong>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</strong> is a popular automatic evaluation metric used in the field of Natural Language Processing (NLP) to assess the quality of machine translation outputs. METEOR is designed to measure the overall similarity and alignment between a generated translation and a reference (human-generated) translation, taking into account multiple levels of linguistic analysis.</p> <p>Unlike simpler metrics such as BLEU (Bilingual Evaluation Understudy), which primarily focus on measuring n-gram overlaps between the generated and reference translations, METEOR incorporates more sophisticated linguistic features and alignments to provide a more comprehensive evaluation.</p> <p>Key features of the METEOR metric include:</p> <ul> <li>Tokenization and Stemming: METEOR tokenizes and stems both the generated and reference translations to handle variations in word forms and improve alignment.</li> <li>Exact Matching and Stem Matching: METEOR calculates the precision and recall of exact word matches as well as matches based on stemmed forms of words, considering synonyms and related words.</li> <li>Alignment and Synonymy: METEOR uses a paraphrase database to identify synonyms and related words, which helps in capturing semantically equivalent terms.</li> <li>Word Order: METEOR explicitly considers word order and alignment between words in the generated and reference translations.</li> <li>Chunk-based Matching: METEOR evaluates matches at the chunk level, which allows for partial credit for translations that have word-level differences but still capture the same meaning.</li> <li>Punctuation and Function Words: METEOR takes into account punctuation and function words and their alignment, as they play a role in overall sentence coherence.</li> <li>Precision and Recall: METEOR calculates precision and recall scores for each of the above components and combines them to compute the final metric.</li> <li>Normalization and Penalty: METEOR applies normalization to the precision and recall scores and incorporates penalty terms to balance precision and recall contributions.</li> </ul> <p>The final METEOR score is a combination of these individual components, reflecting the quality of the generated translation in terms of semantic content, syntax, word choice, and alignment with the reference translation. METEOR is designed to address some of the limitations of BLEU and provide a more holistic evaluation of translation quality, considering linguistic variations and nuances.</p> <p>METEOR has been widely used in machine translation research and competitions, serving as an important tool for comparing and benchmarking different translation systems and approaches. It provides a more comprehensive and linguistically informed perspective on translation quality, making it a valuable addition to the arsenal of NLP evaluation metrics.</p> <h2 id=what-is-xtreme-benchmark>What is XTREME Benchmark?<a class=headerlink href=#what-is-xtreme-benchmark title="Permanent link">&para;</a></h2> <p>The XTREME (Cross-lingual TRansfer Evaluation of Multilingual Encoders) benchmark is a comprehensive evaluation framework <strong>introduced in 2020</strong> for assessing the performance of multilingual models in natural language understanding (NLU) tasks across multiple languages. It aims to evaluate the generalization and transfer capabilities of models in cross-lingual settings.</p> <p>XTREME was developed as an extension of previous benchmarks like GLUE and SuperGLUE, with a specific focus on evaluating models' <strong>abilities to understand and process languages beyond English</strong>. It includes a diverse range of tasks spanning multiple languages, such as named entity recognition, part-of-speech tagging, machine translation, sentence classification, and question answering, among others. Tasks on <a href=https://sites.research.google/xtreme>xtreme bechmark</a> </p> <ul> <li>Sentence-pair Classification </li> <li>Structured Prediction </li> <li>Question Answering </li> <li>Sentence Retrieval</li> </ul> <p>The main objective of the XTREME benchmark is to encourage the <strong>development of models that can effectively transfer knowledge across different languages</strong>, leveraging pretraining on large-scale multilingual data. By evaluating models on a wide range of languages and tasks, XTREME provides insights into the cross-lingual transfer capabilities and identifies areas for improvement in multilingual NLU.</p> <p>Similar to GLUE and SuperGLUE, XTREME utilizes a single metric called the XTREME score to assess the performance of models across the various tasks and languages. The XTREME benchmark serves as an important evaluation platform for advancing research and development in multilingual NLU, fostering the development of models that can effectively handle language diversity and facilitate cross-lingual understanding.</p> <p>XTREME has gained significant attention and has been instrumental in driving progress in multilingual NLU, pushing researchers to develop models that exhibit strong cross-lingual transfer capabilities and perform well across a wide range of languages and tasks. The benchmark continues to evolve and expand to include additional languages, tasks, and evaluation metrics to further enhance the evaluation of multilingual models. <br> - <a href=https://sites.research.google/xtreme>https://sites.research.google/xtreme</a> - <a href=https://arxiv.org/abs/2003.11080>https://arxiv.org/abs/2003.11080</a> - <a href=https://paperswithcode.com/dataset/xtreme>https://paperswithcode.com/dataset/xtreme</a></p> <h2 id=what-is-rouge-score>What is ROUGE Score?<a class=headerlink href=#what-is-rouge-score title="Permanent link">&para;</a></h2> <p>Recall-Oriented Understudy for Gisting Evaluation is a set of metrics for evaluating the quality of automatic summaries and machine translation. It measures the similarity between a machine-generated summary and a reference summary using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summary.</p> <p>The most common n-grams used are unigrams, bigrams, and trigrams. ROUGE score calculates the recall of n-grams in the machine-generated summary by comparing them to the reference summaries.</p> <p>Formula for calculating ROUGE-N:</p> <p><span class=arithmatex>\(ROUGE-N = \frac{\sum_{i=1}^{m} \text{card}(S_i \cap R_i)}{\sum_{i=1}^{m} \text{card}(R_i)}\)</span></p> <p>where:</p> <p><span class=arithmatex>\(S_i\)</span> is the set of n-grams in the machine-generated summary <br> <span class=arithmatex>\(R_i\)</span> is the set of n-grams in the reference summary</p> <p>m is the maximum n-gram length</p> <p>For example, ROUGE-1 measures the overlap of unigrams, ROUGE-2 measures the overlap of bigrams, and ROUGE-L measures the longest common subsequence of the machine-generated summary and the reference summary.</p> <p>Advantages of using ROUGE score: - It is a simple and easy-to-understand metric. - It is relatively insensitive to changes in word order. - It has been shown to be effective in evaluating the performance of automatic summaries and machine translation.</p> <p>Disadvantages of using ROUGE score: - It does not take into account the semantic similarity between the machine-generated summary and the reference summary. - It can be biased towards longer summaries. - It can be difficult to interpret the results of ROUGE score for different tasks.</p> <h2 id=what-is-big-bench>What is BIG-Bench?<a class=headerlink href=#what-is-big-bench title="Permanent link">&para;</a></h2> <p>BIG-Bench is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. It is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. <a href=https://dasarpai.com/dsblog/nlp-tasks#214-nlp-tasks-from-big-benchmark>214 tasks</a> included in BIG-bench are summarized by keyword.</p> <h2 id=deep-learning-tasks-models-on-huggingface-100k-models>Deep Learning Tasks &amp; Models on Huggingface (100K Models)<a class=headerlink href=#deep-learning-tasks-models-on-huggingface-100k-models title="Permanent link">&para;</a></h2> <p>There are many tasks like below for different modalities. And there are different metrics to measure the performance of a model against those tasks. In future I will expend this article, which will contain the metrics for the tasks mentioned below.</p> <h3 id=computer-vision-models-6000-models><a href="https://huggingface.co/models?pipeline_tag=translation">Computer Vision Models</a>, 6000+ Models<a class=headerlink href=#computer-vision-models-6000-models title="Permanent link">&para;</a></h3> <p>1 <a href=https://huggingface.co/tasks/depth-estimation>Depth Estimation</a> <br> 2 <a href=https://huggingface.co/tasks/image-classification>Image Classification</a> <br> 3 <a href=https://huggingface.co/tasks/image-segmentation>Image Segmentation</a> <br> 4 <a href=https://huggingface.co/tasks/image-to-image>Image-to-Image</a> <br> 5 <a href=https://huggingface.co/tasks/object-detection>Object Detection</a> <br> 6 <a href=https://huggingface.co/tasks/video-classification>Video Classification</a> <br> 7 <a href=https://huggingface.co/tasks/unconditional-image-generation>Unconditional Image Generation</a> <br> 8 <a href=https://huggingface.co/tasks/zero-shot-image-classification>Zero-Shot Image Classification</a> </p> <h3 id=natural-language-processing-models-65000-models><a href="https://huggingface.co/models?pipeline_tag=text-generation">Natural Language Processing Models</a>, 65000+ Models<a class=headerlink href=#natural-language-processing-models-65000-models title="Permanent link">&para;</a></h3> <p>1 <a href=https://huggingface.co/tasks/conversational>Conversational</a> <br> 2 <a href=https://huggingface.co/tasks/fill-mask>Fill-Mask</a> <br> 3 <a href=https://huggingface.co/tasks/question-answering>Question Answering</a> <br> 4 <a href=https://huggingface.co/tasks/sentence-similarity>Sentence Similarity</a> <br> 5 <a href=https://huggingface.co/tasks/summarization>Summarization</a> <br> 6 <a href=https://huggingface.co/tasks/table-question-answering>Table Question Answering</a> <br> 7 <a href=https://huggingface.co/tasks/text-classification>Text Classification</a> <br> 8 <a href=https://huggingface.co/tasks/text-generation>Text Generation</a> <br> 9 <a href=https://huggingface.co/tasks/token-classification>Token Classification</a> <br> 10 <a href=https://huggingface.co/tasks/translation>Translation</a> <br> 11 <a href=https://huggingface.co/tasks/zero-shot-classification>Zero-Shot Classification</a> </p> <h3 id=audio-models-10000-models><a href="https://huggingface.co/models?pipeline_tag=voice-activity-detection">Audio Models</a>, 10000+ Models<a class=headerlink href=#audio-models-10000-models title="Permanent link">&para;</a></h3> <p>1 <a href=https://huggingface.co/tasks/audio-classification>Audio Classification</a> <br> 2 <a href=https://huggingface.co/tasks/audio-to-audio>Audio-to-Audio</a> <br> 3 <a href=https://huggingface.co/tasks/automatic-speech-recognition>Automatic Speech Recognition</a> <br> 4 <a href=https://huggingface.co/tasks/text-to-speech>Text-to-Speech</a> <br> 5 <a href="https://huggingface.co/models?pipeline_tag=tabular-classification">Tabular</a> <br> 6 <a href=https://huggingface.co/tasks/tabular-classification>Tabular Classification</a> <br> 7 <a href=https://huggingface.co/tasks/tabular-regression>Tabular Regression</a> </p> <h3 id=multimodal-models-9000-models><a href="https://huggingface.co/models?pipeline_tag=reinforcement-learning">Multimodal Models</a>, 9000+ Models<a class=headerlink href=#multimodal-models-9000-models title="Permanent link">&para;</a></h3> <p>1 <a href=https://huggingface.co/tasks/document-question-answering>Document Question Answering</a> <br> 2 <a href=https://huggingface.co/tasks/feature-extraction>Feature Extraction</a> <br> 3 <a href=https://huggingface.co/tasks/image-to-text>​Image-to-Text</a> <br> 4 <a href=https://huggingface.co/tasks/text-to-image>Text-to-Image</a> <br> 5 <a href=https://huggingface.co/tasks/text-to-video>Text-to-Video Contribute</a> <br> 6 <a href=https://huggingface.co/tasks/visual-question-answering>Visual Question Answering</a></p> <h3 id=reinforcement-learning-22000-models><a href=https://huggingface.co/tasks/reinforcement-learning>Reinforcement Learning</a>, 22000+ Models<a class=headerlink href=#reinforcement-learning-22000-models title="Permanent link">&para;</a></h3> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>