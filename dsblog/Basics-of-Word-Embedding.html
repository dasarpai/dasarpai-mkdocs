<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Basics-of-Word-Embedding.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Basics of Word Embedding - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#basics-of-word-embedding class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Basics of Word Embedding </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-context-target-and-window class=md-nav__link> <span class=md-ellipsis> What is Context, target and window? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-skipgram class=md-nav__link> <span class=md-ellipsis> What is Skipgram? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cbow-continuous-bag-of-words class=md-nav__link> <span class=md-ellipsis> What is CBOW (Continuous Bag of Words) </span> </a> </li> <li class=md-nav__item> <a href=#how-cbow-works class=md-nav__link> <span class=md-ellipsis> How CBOW works? </span> </a> <nav class=md-nav aria-label="How CBOW works?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#finalize-the-corpus-step-1 class=md-nav__link> <span class=md-ellipsis> Finalize the corpus (Step 1) </span> </a> </li> <li class=md-nav__item> <a href=#create-skipgram-step-234 class=md-nav__link> <span class=md-ellipsis> Create Skipgram (Step 2+3+4 ) </span> </a> </li> <li class=md-nav__item> <a href=#create-neural-network-step-5 class=md-nav__link> <span class=md-ellipsis> Create Neural Network (Step 5) </span> </a> </li> <li class=md-nav__item> <a href=#training-forward-propagation-step-678910 class=md-nav__link> <span class=md-ellipsis> Training - Forward propagation (Step 6+7+8+9+10) </span> </a> </li> <li class=md-nav__item> <a href=#training-backward-propagation-step-1112 class=md-nav__link> <span class=md-ellipsis> Training - backward propagation (Step 11+12) </span> </a> </li> <li class=md-nav__item> <a href=#update-embedding-step-1314 class=md-nav__link> <span class=md-ellipsis> Update Embedding (Step 13+14 ) </span> </a> </li> <li class=md-nav__item> <a href=#complete-the-training class=md-nav__link> <span class=md-ellipsis> Complete the Training </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#other-methods-of-embedding class=md-nav__link> <span class=md-ellipsis> Other Methods of Embedding </span> </a> <nav class=md-nav aria-label="Other Methods of Embedding"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tf-idf class=md-nav__link> <span class=md-ellipsis> TF-IDF </span> </a> <nav class=md-nav aria-label=TF-IDF> <ul class=md-nav__list> <li class=md-nav__item> <a href=#how-tf-idf-works class=md-nav__link> <span class=md-ellipsis> How TF-IDF works? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#glove-global-vectors class=md-nav__link> <span class=md-ellipsis> GloVe (Global Vectors) </span> </a> <nav class=md-nav aria-label="GloVe (Global Vectors)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#how-glove-embedding-works class=md-nav__link> <span class=md-ellipsis> How GloVe embedding works? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bert-bidirectional-encoder-representations-from-transformers class=md-nav__link> <span class=md-ellipsis> BERT (Bidirectional Encoder Representations from Transformers) </span> </a> <nav class=md-nav aria-label="BERT (Bidirectional Encoder Representations from Transformers)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#how-bert-does-embedding class=md-nav__link> <span class=md-ellipsis> How BERT does embedding? </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#fasttext-fast-text class=md-nav__link> <span class=md-ellipsis> FastText (Fast Text) </span> </a> </li> <li class=md-nav__item> <a href=#elmo-embeddings-from-language-models class=md-nav__link> <span class=md-ellipsis> ELMo (Embeddings from Language Models) </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/Basics-of-Word-Embedding.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/Basics-of-Word-Embedding.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="Basics of Word Embedding" src=../assets/images/dspost/dsp6101-Basics-of-Word-Embedding.jpg></p> <h1 id=basics-of-word-embedding>Basics of Word Embedding<a class=headerlink href=#basics-of-word-embedding title="Permanent link">&para;</a></h1> <h2 id=what-is-context-target-and-window>What is Context, target and window?<a class=headerlink href=#what-is-context-target-and-window title="Permanent link">&para;</a></h2> <ul> <li>The "context" word is the surrounding word. </li> <li>The "target" word is the middle word. </li> <li>The "window distance" is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right.</li> </ul> <p>Let's take a sentence</p> <blockquote> <p>The quick brown fox jump over a lazy dog.</p> </blockquote> <p>R- Right, L - Left</p> <table> <thead> <tr> <th>target</th> <th>context 1 window</th> <th>context 2 window</th> </tr> </thead> <tbody> <tr> <td>the</td> <td>quick (R)</td> <td>quick(R), brown(R)</td> </tr> <tr> <td>quick</td> <td>the(L), brown(R)</td> <td>the(L), brown(R), fox(R)</td> </tr> <tr> <td>brown</td> <td>quick(L), fox(R)</td> <td>the(L), quick(L), fox(R), jump(R)</td> </tr> <tr> <td>fox</td> <td>brown(L), jump(R)</td> <td>quick(L), brown(L), jump(R), over(R)</td> </tr> </tbody> </table> <p>When creating dataset you don't write multiple words in one row, but you create multiple rows, as below. </p> <table> <thead> <tr> <th>target</th> <th>context 2 window</th> </tr> </thead> <tbody> <tr> <td>the</td> <td>quick</td> </tr> <tr> <td>the</td> <td>brown</td> </tr> <tr> <td>quick</td> <td>the</td> </tr> <tr> <td>quick</td> <td>brown</td> </tr> <tr> <td>quick</td> <td>fox</td> </tr> </tbody> </table> <h2 id=what-is-skipgram>What is Skipgram?<a class=headerlink href=#what-is-skipgram title="Permanent link">&para;</a></h2> <p>Skipgram: <strong>With the help of target word</strong> we want to predict the context/surrounding word. From above example predicting "quick", "brown", "the", "brown" etc with target word "the", "quick"</p> <h2 id=what-is-cbow-continuous-bag-of-words>What is CBOW (Continuous Bag of Words)<a class=headerlink href=#what-is-cbow-continuous-bag-of-words title="Permanent link">&para;</a></h2> <p>CBOW : <strong>With the help of context</strong> we want to predict target. From above example, predicting "the", "quick" when context words are "quick" or "brown", "the", "fox".</p> <h2 id=how-cbow-works>How CBOW works?<a class=headerlink href=#how-cbow-works title="Permanent link">&para;</a></h2> <p>For both, CBOW and Skipgram networks works in the same way as mentioned below. Only difference is when we are using CBOW we want to predict target word from context word. If you are using Skipgram then we want to predict context word from a target word. </p> <h3 id=finalize-the-corpus-step-1>Finalize the corpus (Step 1)<a class=headerlink href=#finalize-the-corpus-step-1 title="Permanent link">&para;</a></h3> <p>In reality corpus is extremely huge size, it is like entire wikipedia text or entire stakeoverflow text or entire quora text. For the illustration of skipgram we are taking a small example.</p> <p><strong>Corpus</strong> : The quick brown fox jump over the dog </p> <h3 id=create-skipgram-step-234>Create Skipgram (Step 2+3+4 )<a class=headerlink href=#create-skipgram-step-234 title="Permanent link">&para;</a></h3> <p>As discussed earlier created 1 or 2 or 3 window skipgram from the corpus.</p> <table> <thead> <tr> <th>Word</th> <th>(Step 3) onehot encoding for each word in the corpus</th> <th>(Step 4) random initial embedding, 4 dimensional</th> </tr> </thead> <tbody> <tr> <td>the</td> <td>[1,0,0,0,0,0,0,0]</td> <td>[0.11,0.12,0.14,0.15]</td> </tr> <tr> <td>quick</td> <td>[0,1,0,0,0,0,0,0]</td> <td>[0.21,0.23,0.24,0.26]</td> </tr> <tr> <td>brown</td> <td>[0,0,1,0,0,0,0,0]</td> <td>[0.31,0.34,0.36,0.38]</td> </tr> <tr> <td>fox</td> <td>[0,0,0,1,0,0,0,0]</td> <td>[0.51,0.12,0.14,0.15]</td> </tr> <tr> <td>jump</td> <td>[0,0,0,0,1,0,0,0]</td> <td>[0.21,0.63,0.24,0.26]</td> </tr> <tr> <td>over</td> <td>[0,0,0,0,0,1,0,0]</td> <td>[0.31,0.34,0.86,0.38]</td> </tr> <tr> <td>the</td> <td>[0,0,0,0,0,0,1,0]</td> <td>[0.71,0.12,0.14,0.15]</td> </tr> <tr> <td>dog</td> <td>[0,0,0,0,0,0,0,1]</td> <td>[0.21,0.93,0.24,0.26]</td> </tr> </tbody> </table> <h3 id=create-neural-network-step-5>Create Neural Network (Step 5)<a class=headerlink href=#create-neural-network-step-5 title="Permanent link">&para;</a></h3> <p>Create a neural network for learning embedding. - One input layer which can accept token/words. Convert token (context and target words) into onehot encoding - One embedding layer, for example sake we are taking 4 dimensional embedding of words. These embedding are randomingly intiated number initally (there are other ways also). - One dense layer of 5 neuron (example) - Softmax function - Output layer (to predict the probability of the predicted word. If vocabulary size of the corpus is 10,000 words, then softmax will predict 10,000 probabilities) - Loss function - Cross entropy loss function. L = <span class=arithmatex>\(<span class=arithmatex>\(- \sum_{i=1}^{N} y_{i} \cdot \log(p_{i})\)</span>\)</span>, N is vocab size. - 4 numbers from embedding will go to each of the 5 neuron, Each neuron will have 4 weights to embedding layer. 5*4 = 20 weights are learned + 5 biases learned - Learning Rate LR = .0002</p> <h3 id=training-forward-propagation-step-678910>Training - Forward propagation (Step 6+7+8+9+10)<a class=headerlink href=#training-forward-propagation-step-678910 title="Permanent link">&para;</a></h3> <ul> <li>Randomly initialize all the weights and biases of the network.</li> <li>Pass target and context word to the network.</li> </ul> <table> <thead> <tr> <th>Step 6</th> <th>-</th> <th>-</th> <th>Step 7</th> <th>Step 8</th> <th>-</th> <th>Step 9</th> </tr> </thead> <tbody> <tr> <td>Input layer</td> <td>Embedding layer</td> <td>Hidden layer (5 neuron, random init w\&amp;b), dense layer</td> <td>matmul between weights and inputs (embedding)</td> <td>softmax (8 vocab size)</td> <td>actual vector for "quick"</td> <td>cross entropy loss</td> </tr> <tr> <td>The (context), quick (target)</td> <td>context (The) = [.11,.12,.14,.15]</td> <td>n1=[.11,.12,.13,.14]</td> <td>0.0657</td> <td>0.1867</td> <td>0</td> <td>0.0897</td> </tr> <tr> <td></td> <td>target (quick) = [.21,.23,.24,.26]</td> <td>n2=[.13,.14,.15,.16]</td> <td>0.0761</td> <td>0.1886</td> <td>1</td> <td>0.7244</td> </tr> <tr> <td></td> <td></td> <td>n3=[.21..22,.23,.24]</td> <td>0.1177</td> <td>0.1966</td> <td>0</td> <td>0.0951</td> </tr> <tr> <td></td> <td></td> <td>n4=[.32,.33,.34,.35]</td> <td>0.1749</td> <td>0.2082</td> <td>0</td> <td>0.1014</td> </tr> <tr> <td></td> <td></td> <td>n5=[.42,.43,.45,.46]</td> <td>0.2298</td> <td>0.2199</td> <td>0</td> <td>0.1079</td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td><strong>Step 10</strong></td> </tr> <tr> <td></td> <td></td> <td></td> <td></td> <td></td> <td>Total Loss</td> <td>1.1185</td> </tr> </tbody> </table> <h3 id=training-backward-propagation-step-1112>Training - backward propagation (Step 11+12)<a class=headerlink href=#training-backward-propagation-step-1112 title="Permanent link">&para;</a></h3> <p>Updating weights of network neurons</p> <table> <thead> <tr> <th>Step 11 (Gradient Calculation for 20 weights)</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> </tr> </thead> <tbody> <tr> <td>dL/dw1</td> <td>10.17</td> <td>9.32</td> <td>8.60</td> <td>7.99</td> </tr> <tr> <td>dL/dw2</td> <td>8.60</td> <td>7.99</td> <td>7.46</td> <td>6.99</td> </tr> <tr> <td>dL/dw3</td> <td>5.33</td> <td>5.08</td> <td>4.66</td> <td>4.66</td> </tr> <tr> <td>dL/dw4</td> <td>3.50</td> <td>3.39</td> <td>3.20</td> <td>3.20</td> </tr> <tr> <td>dL/dw5</td> <td>2.66</td> <td>2.60</td> <td>2.43</td> <td>2.43</td> </tr> </tbody> </table> <table> <thead> <tr> <th>Step 12 Updated Weights</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> </tr> </thead> <tbody> <tr> <td>new w1</td> <td>0.11</td> <td>0.12</td> <td>0.13</td> <td>0.14</td> </tr> <tr> <td>new w2</td> <td>0.13</td> <td>0.14</td> <td>0.15</td> <td>0.16</td> </tr> <tr> <td>new w3</td> <td>0.21</td> <td>0.22</td> <td>0.23</td> <td>0.24</td> </tr> <tr> <td>new d4</td> <td>0.32</td> <td>0.33</td> <td>0.34</td> <td>0.35</td> </tr> <tr> <td>new w5</td> <td>0.42</td> <td>0.43</td> <td>0.45</td> <td>0.46</td> </tr> </tbody> </table> <p>new weight = old weight - Learning Rate * DL/dW</p> <h3 id=update-embedding-step-1314>Update Embedding (Step 13+14 )<a class=headerlink href=#update-embedding-step-1314 title="Permanent link">&para;</a></h3> <table> <thead> <tr> <th>Old Embedding (Vector)</th> <th>1</th> <th>2</th> <th>3</th> <th>4</th> </tr> </thead> <tbody> <tr> <td>context (The) = [.11,.12,.14,.15]</td> <td>0.110</td> <td>0.120</td> <td>0.140</td> <td>0.150</td> </tr> <tr> <td>target (quick) = [.21,.23,.24,.26]</td> <td>0.210</td> <td>0.230</td> <td>0.240</td> <td>0.260</td> </tr> </tbody> </table> <table> <thead> <tr> <th>Step 13 (Gradient of Old Embedding)</th> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <td>dL/context</td> <td>10.17</td> <td>9.32</td> <td>7.99</td> </tr> <tr> <td>dL/target</td> <td>5.33</td> <td>4.86</td> <td>4.66</td> </tr> </tbody> </table> <table> <thead> <tr> <th>Step 14 (Updated Embedding)</th> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <td>context (The)</td> <td>0.108</td> <td>0.118</td> <td>0.138</td> </tr> <tr> <td>target (quick)</td> <td>0.209</td> <td>0.229</td> <td>0.239</td> </tr> </tbody> </table> <h3 id=complete-the-training>Complete the Training<a class=headerlink href=#complete-the-training title="Permanent link">&para;</a></h3> <ul> <li>Perform training forward and backword propagation in batch, multiple words at a time. </li> <li>Everytime update w&amp;b and also update embedding.</li> <li>Trained embedding can be used in future without these training steps.</li> <li>Let entire dataset of paired words go through this network. One it goes through it is called one epoch.</li> <li>Let embedding get updated over multiple epoch say 50 or 100. More epoch, will cause better embedding. It will cost more money.</li> <li>More dimentional vector will have better represenation but will cost more computation and more money.</li> </ul> <h2 id=other-methods-of-embedding>Other Methods of Embedding<a class=headerlink href=#other-methods-of-embedding title="Permanent link">&para;</a></h2> <h3 id=tf-idf>TF-IDF<a class=headerlink href=#tf-idf title="Permanent link">&para;</a></h3> <p>TF-IDF - Term Frequency - Inverse Document Frequency, is an old, traditional, frequency based text embedding technique. It is not based on neural network architecture therefore does not need expensive hardware to create these embedding and use TF-IDF embedding. Like skipgram or CBOW it is not vector based but frequency based, therefore understandign symantic of the text is not possible with TF-IDF. There is no use of pretrained embedding, everytime we have a corpus we need to create embedding for that and it is used only for that. We cannot use TF-IDF embedding, which was created using news text for something else, say history or enterainment. Thus, embedding transfer is meaninless but task transfer can be done. It means TF-IDF embedding which is used for classficittion purpose can be used for other task like topic modelling, sentiment analysis etc. Obviously there is a limit, we cannot use it for other task like translation or summarization.</p> <h4 id=how-tf-idf-works>How TF-IDF works?<a class=headerlink href=#how-tf-idf-works title="Permanent link">&para;</a></h4> <ul> <li>Term frequency (TF): The number of times a word appears in a document.</li> <li>Inverse document frequency (IDF): The logarithm of the number of documents in the collection divided by the number of documents that contain the word.</li> <li>The TF-IDF score for a word in a document is calculated as follows:</li> <li>TF-IDF = TF * IDF (The higher the TF-IDF score, the more important the word is to the document.)</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=n>Document_1</span><span class=p>:</span> <span class=s2>&quot;The quick brown fox jumps over the lazy dog.&quot;</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=n>Document_2</span><span class=p>:</span> <span class=s2>&quot;The dog is lazy, but the fox is quick.&quot;</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=c1># Term frequency for the word &quot;quick&quot; in Document 1</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=n>TF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=c1># Inverse document frequency for the word &quot;quick&quot;</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a><span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>)</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a><span class=c1># TF-IDF score for the word &quot;quick&quot; in Document 1</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a><span class=n>TF</span><span class=o>-</span><span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>=</span> <span class=n>TF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>*</span> <span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a><span class=c1># Term frequency for the word &quot;quick&quot; in Document 2</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a><span class=n>TF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a><span class=c1># Inverse document frequency for the word &quot;quick&quot;</span>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a><span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>)</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a>
</span><span id=__span-0-19><a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a><span class=c1># TF-IDF score for the word &quot;quick&quot; in Document 2</span>
</span><span id=__span-0-20><a id=__codelineno-0-20 name=__codelineno-0-20 href=#__codelineno-0-20></a><span class=n>TF</span><span class=o>-</span><span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>=</span> <span class=n>TF</span><span class=p>(</span><span class=n>quick</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>*</span> <span class=n>IDF</span><span class=p>(</span><span class=n>quick</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-21><a id=__codelineno-0-21 name=__codelineno-0-21 href=#__codelineno-0-21></a>
</span><span id=__span-0-22><a id=__codelineno-0-22 name=__codelineno-0-22 href=#__codelineno-0-22></a><span class=c1># Term frequency for the word &quot;lazy&quot; in Document 1</span>
</span><span id=__span-0-23><a id=__codelineno-0-23 name=__codelineno-0-23 href=#__codelineno-0-23></a><span class=n>TF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span>
</span><span id=__span-0-24><a id=__codelineno-0-24 name=__codelineno-0-24 href=#__codelineno-0-24></a>
</span><span id=__span-0-25><a id=__codelineno-0-25 name=__codelineno-0-25 href=#__codelineno-0-25></a><span class=c1># Inverse document frequency for the word &quot;lazy&quot;</span>
</span><span id=__span-0-26><a id=__codelineno-0-26 name=__codelineno-0-26 href=#__codelineno-0-26></a><span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>)</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-27><a id=__codelineno-0-27 name=__codelineno-0-27 href=#__codelineno-0-27></a>
</span><span id=__span-0-28><a id=__codelineno-0-28 name=__codelineno-0-28 href=#__codelineno-0-28></a><span class=c1># TF-IDF score for the word &quot;lazy&quot; in Document 1</span>
</span><span id=__span-0-29><a id=__codelineno-0-29 name=__codelineno-0-29 href=#__codelineno-0-29></a><span class=n>TF</span><span class=o>-</span><span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>=</span> <span class=n>TF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_1</span><span class=p>)</span> <span class=o>*</span> <span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-30><a id=__codelineno-0-30 name=__codelineno-0-30 href=#__codelineno-0-30></a>
</span><span id=__span-0-31><a id=__codelineno-0-31 name=__codelineno-0-31 href=#__codelineno-0-31></a><span class=c1># Term frequency for the word &quot;lazy&quot; in Document 2</span>
</span><span id=__span-0-32><a id=__codelineno-0-32 name=__codelineno-0-32 href=#__codelineno-0-32></a><span class=n>TF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span>
</span><span id=__span-0-33><a id=__codelineno-0-33 name=__codelineno-0-33 href=#__codelineno-0-33></a>
</span><span id=__span-0-34><a id=__codelineno-0-34 name=__codelineno-0-34 href=#__codelineno-0-34></a><span class=c1># Inverse document frequency for the word &quot;lazy&quot;</span>
</span><span id=__span-0-35><a id=__codelineno-0-35 name=__codelineno-0-35 href=#__codelineno-0-35></a><span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>)</span> <span class=o>=</span> <span class=n>log</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-36><a id=__codelineno-0-36 name=__codelineno-0-36 href=#__codelineno-0-36></a>
</span><span id=__span-0-37><a id=__codelineno-0-37 name=__codelineno-0-37 href=#__codelineno-0-37></a><span class=c1># TF-IDF score for the word &quot;lazy&quot; in Document 2</span>
</span><span id=__span-0-38><a id=__codelineno-0-38 name=__codelineno-0-38 href=#__codelineno-0-38></a><span class=n>TF</span><span class=o>-</span><span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>=</span> <span class=n>TF</span><span class=p>(</span><span class=n>lazy</span><span class=p>,</span> <span class=n>Document_2</span><span class=p>)</span> <span class=o>*</span> <span class=n>IDF</span><span class=p>(</span><span class=n>lazy</span><span class=p>)</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>*</span> <span class=mi>0</span> <span class=o>=</span> <span class=mi>0</span>
</span></code></pre></div> <h3 id=glove-global-vectors>GloVe (Global Vectors)<a class=headerlink href=#glove-global-vectors title="Permanent link">&para;</a></h3> <p>GloVe is a method that learns word embeddings from global word-word co-occurrence statistics. It is similar to Skipgram and CBOW, but it is better at capturing long-range semantic relationships between words. GloVe embedding is good for text classification, and machine translation (MT).</p> <h4 id=how-glove-embedding-works>How GloVe embedding works?<a class=headerlink href=#how-glove-embedding-works title="Permanent link">&para;</a></h4> <ul> <li>Tokenize the corpus: Split the corpus into individual words and punctuation marks.</li> <li>Count word co-occurrences: For each word in the vocabulary, count how many times it co-occurs with other words in a given window size.</li> <li>Build a word-word co-occurrence matrix: The word-word co-occurrence matrix is a square matrix, where each row and column represents a word in the vocabulary. The value at each cell in the matrix represents the number of times the two corresponding words co-occur in the corpus.</li> <li>Factorize the word-word co-occurrence matrix: Factorize the word-word co-occurrence matrix into two lower-dimensional matrices, one for <strong>word embeddings</strong> (relationship between words) and one for <strong>context embeddings</strong> (relationship between words in the context). We can factorize the word-word co-occurrence matrix using a variety of matrix factorization techniques, such as singular value decomposition (SVD) or nonnegative matrix factorization (NMF).</li> <li>Normalize the word embeddings: Normalize the word embeddings so that they have a unit length. We can normalize the word embeddings by dividing each embedding by its L2 norm. This will ensure that all of the embeddings have a unit length.</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=kn>from</span><span class=w> </span><span class=nn>gensim.models</span><span class=w> </span><span class=kn>import</span> <span class=n>KeyedVectors</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=c1># Load the corpus</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=n>corpus</span> <span class=o>=</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&quot;corpus.txt&quot;</span><span class=p>,</span> <span class=s2>&quot;r&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=c1># Tokenize the corpus</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>corpus</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=c1># Count word co-occurrences</span>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a><span class=n>word_co_occurrences</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)))</span>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12 href=#__codelineno-1-12></a><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)):</span>
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13 href=#__codelineno-1-13></a>    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)):</span>
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14 href=#__codelineno-1-14></a>        <span class=k>if</span> <span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>!=</span> <span class=n>tokens</span><span class=p>[</span><span class=n>j</span><span class=p>]:</span>
</span><span id=__span-1-15><a id=__codelineno-1-15 name=__codelineno-1-15 href=#__codelineno-1-15></a>            <span class=n>word_co_occurrences</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>tokens</span><span class=o>.</span><span class=n>count</span><span class=p>(</span><span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=s2>&quot; &quot;</span> <span class=o>+</span> <span class=n>tokens</span><span class=p>[</span><span class=n>j</span><span class=p>])</span>
</span><span id=__span-1-16><a id=__codelineno-1-16 name=__codelineno-1-16 href=#__codelineno-1-16></a>
</span><span id=__span-1-17><a id=__codelineno-1-17 name=__codelineno-1-17 href=#__codelineno-1-17></a><span class=c1># Factorize the word-word co-occurrence matrix</span>
</span><span id=__span-1-18><a id=__codelineno-1-18 name=__codelineno-1-18 href=#__codelineno-1-18></a><span class=n>glove_model</span> <span class=o>=</span> <span class=n>KeyedVectors</span><span class=p>(</span><span class=n>word_vectors</span><span class=o>=</span><span class=n>word_co_occurrences</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span><span id=__span-1-19><a id=__codelineno-1-19 name=__codelineno-1-19 href=#__codelineno-1-19></a>
</span><span id=__span-1-20><a id=__codelineno-1-20 name=__codelineno-1-20 href=#__codelineno-1-20></a><span class=c1># Save the word embeddings</span>
</span><span id=__span-1-21><a id=__codelineno-1-21 name=__codelineno-1-21 href=#__codelineno-1-21></a><span class=n>glove_model</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&quot;glove_embeddings.txt&quot;</span><span class=p>)</span>
</span></code></pre></div> <p>How SVD (Singular Value Decomposition) works?</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=kn>import</span><span class=w> </span><span class=nn>numpy</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=nn>np</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=c1># Create a word-word co-occurrence matrix</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=n>word_co_occurrences</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a>    <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a>    <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=p>])</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=c1># Perform SVD</span>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a><span class=n>U</span><span class=p>,</span> <span class=n>S</span><span class=p>,</span> <span class=n>Vh</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>svd</span><span class=p>(</span><span class=n>word_co_occurrences</span><span class=p>)</span>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a>
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a><span class=c1># Truncate the singular values</span>
</span><span id=__span-2-14><a id=__codelineno-2-14 name=__codelineno-2-14 href=#__codelineno-2-14></a><span class=n>S_truncated</span> <span class=o>=</span> <span class=n>S</span><span class=p>[:</span><span class=mi>2</span><span class=p>]</span>
</span><span id=__span-2-15><a id=__codelineno-2-15 name=__codelineno-2-15 href=#__codelineno-2-15></a>
</span><span id=__span-2-16><a id=__codelineno-2-16 name=__codelineno-2-16 href=#__codelineno-2-16></a><span class=c1># Reconstruct the word-word co-occurrence matrix</span>
</span><span id=__span-2-17><a id=__codelineno-2-17 name=__codelineno-2-17 href=#__codelineno-2-17></a><span class=n>word_co_occurrences_reconstructed</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>U</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>2</span><span class=p>],</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>S_truncated</span><span class=p>,</span> <span class=n>Vh</span><span class=p>[:,</span> <span class=p>:</span><span class=mi>2</span><span class=p>]))</span>
</span><span id=__span-2-18><a id=__codelineno-2-18 name=__codelineno-2-18 href=#__codelineno-2-18></a>
</span><span id=__span-2-19><a id=__codelineno-2-19 name=__codelineno-2-19 href=#__codelineno-2-19></a><span class=c1># Print the reconstructed word-word co-occurrence matrix</span>
</span><span id=__span-2-20><a id=__codelineno-2-20 name=__codelineno-2-20 href=#__codelineno-2-20></a><span class=nb>print</span><span class=p>(</span><span class=n>word_co_occurrences_reconstructed</span><span class=p>)</span>
</span><span id=__span-2-21><a id=__codelineno-2-21 name=__codelineno-2-21 href=#__codelineno-2-21></a>
</span><span id=__span-2-22><a id=__codelineno-2-22 name=__codelineno-2-22 href=#__codelineno-2-22></a><span class=c1># Results</span>
</span><span id=__span-2-23><a id=__codelineno-2-23 name=__codelineno-2-23 href=#__codelineno-2-23></a><span class=p>[[</span><span class=o>-</span><span class=mf>0.50578521</span> <span class=o>-</span><span class=mf>0.25523155</span><span class=p>]</span>
</span><span id=__span-2-24><a id=__codelineno-2-24 name=__codelineno-2-24 href=#__codelineno-2-24></a> <span class=p>[</span><span class=o>-</span><span class=mf>0.58437383</span> <span class=o>-</span><span class=mf>0.60130182</span><span class=p>]</span>
</span><span id=__span-2-25><a id=__codelineno-2-25 name=__codelineno-2-25 href=#__codelineno-2-25></a> <span class=p>[</span><span class=o>-</span><span class=mf>0.63457746</span>  <span class=mf>0.75716113</span><span class=p>]]</span>
</span><span id=__span-2-26><a id=__codelineno-2-26 name=__codelineno-2-26 href=#__codelineno-2-26></a><span class=p>[[</span><span class=o>-</span><span class=mf>0.50578521</span> <span class=o>-</span><span class=mf>0.58437383</span><span class=p>]</span>
</span><span id=__span-2-27><a id=__codelineno-2-27 name=__codelineno-2-27 href=#__codelineno-2-27></a> <span class=p>[</span> <span class=mf>0.25523155</span>  <span class=mf>0.60130182</span><span class=p>]</span>
</span><span id=__span-2-28><a id=__codelineno-2-28 name=__codelineno-2-28 href=#__codelineno-2-28></a> <span class=p>[</span> <span class=mf>0.82403773</span> <span class=o>-</span><span class=mf>0.54492509</span><span class=p>]]</span>
</span></code></pre></div> <h3 id=bert-bidirectional-encoder-representations-from-transformers>BERT (Bidirectional Encoder Representations from Transformers)<a class=headerlink href=#bert-bidirectional-encoder-representations-from-transformers title="Permanent link">&para;</a></h3> <p>BERT is a transformer-based language model that can learn word embeddings from unlabeled text, we need not to create skipgram pairs. BERT embeddings are particularly good at capturing <strong>contextual information</strong>. BERT embedding is good for MT, QA, Classification tasks.</p> <h4 id=how-bert-does-embedding>How BERT does embedding?<a class=headerlink href=#how-bert-does-embedding title="Permanent link">&para;</a></h4> <ul> <li>Tokenization: The first step is to tokenize the sentence into words. This means splitting the sentence into individual words, including punctuation marks. The tokenized sentence is then represented as a sequence of integers (we create ids), where each integer represents a word in the vocabulary.</li> <li>Word embedding lookup: BERT uses a pre-trained word embedding table to convert each word in the sequence into a vector of numbers. This vector represents the meaning of the word in a distributed manner.</li> <li>Segment embedding lookup: BERT also uses a segment embedding table to encode the position of each word in the sentence. This is necessary because BERT is a bidirectional language model, and it needs to know the context of each word in order to learn meaningful embeddings.</li> <li>Positional embedding lookup: BERT also uses a positional embedding table to encode the absolute position of each word in the sentence. This is necessary because BERT needs to know the order of the words in the sentence in order to learn meaningful embeddings.</li> <li>Transformer encoding: The encoded sequence of word embeddings, segment embeddings, and positional embeddings is then passed to the transformer encoder. The transformer encoder is a neural network architecture that learns long-range dependencies between words in a sentence.</li> <li>Output embedding: The output of the transformer encoder is a sequence of vectors, where each vector represents the embedding of the corresponding word in the sentence. These embeddings are then used for downstream natural language processing tasks, such as machine translation, text classification, and question answering.</li> </ul> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=c1># Tokenize the sentence</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=n>sentence</span> <span class=o>=</span> <span class=s2>&quot;The quick brown fox jump over the lazy fox&quot;</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a><span class=n>tokens</span> <span class=o>=</span> <span class=n>sentence</span><span class=o>.</span><span class=n>split</span><span class=p>()</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a><span class=c1># Convert each word to a word embedding vector</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a><span class=n>word_embeddings</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a><span class=k>for</span> <span class=n>token</span> <span class=ow>in</span> <span class=n>tokens</span><span class=p>:</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>    <span class=n>word_embeddings</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>bert_model</span><span class=o>.</span><span class=n>get_word_embedding</span><span class=p>(</span><span class=n>token</span><span class=p>))</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a><span class=c1># Create segment embeddings</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a><span class=n>segment_embeddings</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)):</span>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a>    <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span><span class=p>:</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a>        <span class=n>segment_embeddings</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>bert_model</span><span class=o>.</span><span class=n>get_segment_embedding</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-3-16><a id=__codelineno-3-16 name=__codelineno-3-16 href=#__codelineno-3-16></a>        <span class=n>segment_embeddings</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>bert_model</span><span class=o>.</span><span class=n>get_segment_embedding</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
</span><span id=__span-3-17><a id=__codelineno-3-17 name=__codelineno-3-17 href=#__codelineno-3-17></a>
</span><span id=__span-3-18><a id=__codelineno-3-18 name=__codelineno-3-18 href=#__codelineno-3-18></a><span class=c1># Create positional embeddings</span>
</span><span id=__span-3-19><a id=__codelineno-3-19 name=__codelineno-3-19 href=#__codelineno-3-19></a><span class=n>positional_embeddings</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-3-20><a id=__codelineno-3-20 name=__codelineno-3-20 href=#__codelineno-3-20></a><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)):</span>
</span><span id=__span-3-21><a id=__codelineno-3-21 name=__codelineno-3-21 href=#__codelineno-3-21></a>    <span class=n>positional_embeddings</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>bert_model</span><span class=o>.</span><span class=n>get_positional_embedding</span><span class=p>(</span><span class=n>i</span><span class=p>))</span>
</span><span id=__span-3-22><a id=__codelineno-3-22 name=__codelineno-3-22 href=#__codelineno-3-22></a>
</span><span id=__span-3-23><a id=__codelineno-3-23 name=__codelineno-3-23 href=#__codelineno-3-23></a><span class=c1># Encode the sentence</span>
</span><span id=__span-3-24><a id=__codelineno-3-24 name=__codelineno-3-24 href=#__codelineno-3-24></a><span class=n>encoded_sentence</span> <span class=o>=</span> <span class=n>bert_model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>word_embeddings</span><span class=p>,</span> <span class=n>segment_embeddings</span><span class=p>,</span> <span class=n>positional_embeddings</span><span class=p>)</span>
</span><span id=__span-3-25><a id=__codelineno-3-25 name=__codelineno-3-25 href=#__codelineno-3-25></a>
</span><span id=__span-3-26><a id=__codelineno-3-26 name=__codelineno-3-26 href=#__codelineno-3-26></a><span class=c1># Output embeddings</span>
</span><span id=__span-3-27><a id=__codelineno-3-27 name=__codelineno-3-27 href=#__codelineno-3-27></a><span class=n>output_embeddings</span> <span class=o>=</span> <span class=n>encoded_sentence</span>
</span></code></pre></div> <h3 id=fasttext-fast-text>FastText (Fast Text)<a class=headerlink href=#fasttext-fast-text title="Permanent link">&para;</a></h3> <p>FastText is a modification of Skipgram that can learn embeddings for words and subwords. This makes it better at representing rare words and out-of-vocabulary words. FastText is good for name-entity-recognition (NER) &amp; Question Answering (QA) tasks.</p> <h3 id=elmo-embeddings-from-language-models>ELMo (Embeddings from Language Models)<a class=headerlink href=#elmo-embeddings-from-language-models title="Permanent link">&para;</a></h3> <p>ELMo (Embeddings from Language Models) is a deep contextual word embedding technique that uses a bidirectional language model (biLM) to learn word representations. A biLM is a type of neural network that can learn to predict the next word in a sentence, as well as the previous word. Unlike skipgram, which predicts next words, biLM is bidirectional. From a target word biLM can predict next and previous words.</p> <h1 id=resources>Resources<a class=headerlink href=#resources title="Permanent link">&para;</a></h1> <p>If you want to understand all skipgram/cbow caluclation with excel and then you can use this <a href="https://docs.google.com/spreadsheets/d/1eU4EVtUzD1w_ILcpJVTc6oK2KH9vEDK7OuXFtyv1_gU/edit?usp=sharing">calculation sheet</a></p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>