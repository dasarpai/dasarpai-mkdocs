<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Cost Functions and Optimizers in Machine Learning - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#cost-functions-and-optimizers-in-machine-learning class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Cost Functions and Optimizers in Machine Learning </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#what-is-machine-learning class=md-nav__link> <span class=md-ellipsis> What is machine learning? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-cost-function class=md-nav__link> <span class=md-ellipsis> What is cost function? </span> </a> </li> <li class=md-nav__item> <a href=#what-is-the-difference-between-loss-function-and-cost-function class=md-nav__link> <span class=md-ellipsis> What is the difference between loss function and cost function? </span> </a> <nav class=md-nav aria-label="What is the difference between loss function and cost function?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-loss-function class=md-nav__link> <span class=md-ellipsis> 1. Loss Function: </span> </a> </li> <li class=md-nav__item> <a href=#2-cost-function class=md-nav__link> <span class=md-ellipsis> 2. Cost Function: </span> </a> </li> <li class=md-nav__item> <a href=#summary-of-differences class=md-nav__link> <span class=md-ellipsis> Summary of Differences: </span> </a> </li> <li class=md-nav__item> <a href=#example-in-practice class=md-nav__link> <span class=md-ellipsis> Example in Practice: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-are-different-cost-functions class=md-nav__link> <span class=md-ellipsis> What are different cost functions? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-different-cost-functions-for-different-machine-learning-goals class=md-nav__link> <span class=md-ellipsis> What are different cost functions for different Machine learning goals? </span> </a> <nav class=md-nav aria-label="What are different cost functions for different Machine learning goals?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#cost-function-for-regression class=md-nav__link> <span class=md-ellipsis> Cost Function for Regression </span> </a> </li> <li class=md-nav__item> <a href=#cost-function-for-classification class=md-nav__link> <span class=md-ellipsis> Cost Function for Classification </span> </a> </li> <li class=md-nav__item> <a href=#cost-function-for-clustering class=md-nav__link> <span class=md-ellipsis> Cost Function for Clustering </span> </a> </li> <li class=md-nav__item> <a href=#cost-function-for-sementic-segmenration class=md-nav__link> <span class=md-ellipsis> Cost Function for Sementic Segmenration </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cost-function-for-text-genration class=md-nav__link> <span class=md-ellipsis> Cost Function for Text Genration </span> </a> <nav class=md-nav aria-label="Cost Function for Text Genration"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-cross-entropy-loss-negative-log-likelihood-loss class=md-nav__link> <span class=md-ellipsis> 1. Cross-Entropy Loss (Negative Log-Likelihood Loss) </span> </a> </li> <li class=md-nav__item> <a href=#2-reinforcement-learning-based-loss-policy-gradient class=md-nav__link> <span class=md-ellipsis> 2. Reinforcement Learning-Based Loss (Policy Gradient) </span> </a> </li> <li class=md-nav__item> <a href=#3-maximum-likelihood-estimation-mle-loss class=md-nav__link> <span class=md-ellipsis> 3. Maximum Likelihood Estimation (MLE) Loss </span> </a> </li> <li class=md-nav__item> <a href=#4-perplexity class=md-nav__link> <span class=md-ellipsis> 4. Perplexity </span> </a> </li> <li class=md-nav__item> <a href=#5-kl-divergence-kullback-leibler-divergence class=md-nav__link> <span class=md-ellipsis> 5. KL Divergence (Kullback-Leibler Divergence) </span> </a> </li> <li class=md-nav__item> <a href=#6-bleu-score-based-loss class=md-nav__link> <span class=md-ellipsis> 6. BLEU Score-Based Loss </span> </a> </li> <li class=md-nav__item> <a href=#7-gan-discriminator-loss class=md-nav__link> <span class=md-ellipsis> 7. GAN Discriminator Loss </span> </a> </li> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> Summary: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#can-you-explain-with-example-how-kl-divergence-loss-function-works class=md-nav__link> <span class=md-ellipsis> Can you explain with example how KL Divergence loss function works? </span> </a> <nav class=md-nav aria-label="Can you explain with example how KL Divergence loss function works?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-a-probability-distribution class=md-nav__link> <span class=md-ellipsis> What is a Probability Distribution? </span> </a> </li> <li class=md-nav__item> <a href=#kl-divergence-explained class=md-nav__link> <span class=md-ellipsis> KL Divergence Explained: </span> </a> </li> <li class=md-nav__item> <a href=#example class=md-nav__link> <span class=md-ellipsis> Example: </span> </a> <nav class=md-nav aria-label=Example:> <ul class=md-nav__list> <li class=md-nav__item> <a href=#scenario class=md-nav__link> <span class=md-ellipsis> Scenario: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#kl-divergence-calculation class=md-nav__link> <span class=md-ellipsis> KL Divergence Calculation: </span> </a> </li> <li class=md-nav__item> <a href=#interpretation class=md-nav__link> <span class=md-ellipsis> Interpretation: </span> </a> </li> <li class=md-nav__item> <a href=#summary_1 class=md-nav__link> <span class=md-ellipsis> Summary: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-is-optimizer class=md-nav__link> <span class=md-ellipsis> What is Optimizer? </span> </a> </li> <li class=md-nav__item> <a href=#what-are-the-various-optimization-algorithms class=md-nav__link> <span class=md-ellipsis> What are the various optimization algorithms? </span> </a> </li> <li class=md-nav__item> <a href=#can-you-explain-with-example-how-these-optimizers-update-the-model-parameters class=md-nav__link> <span class=md-ellipsis> Can you explain with example how these optimizers update the model parameters? </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt=Cost-Functions-and-Optimizers-in-Machine-Learning src=../assets/images/dspost/dsp6045-Cost-Functions-and-Optimizers-in-Machine-Learning.jpg> </p> <h1 id=cost-functions-and-optimizers-in-machine-learning>Cost-Functions-and-Optimizers-in-Machine-Learning<a class=headerlink href=#cost-functions-and-optimizers-in-machine-learning title="Permanent link">&para;</a></h1> <h2 id=what-is-machine-learning>What is machine learning?<a class=headerlink href=#what-is-machine-learning title="Permanent link">&para;</a></h2> <p>Machine learning is a subfield of artificial intelligence that focuses on the <strong>development of algorithms and statistical models</strong> that enable computers to improve their performance on a specific task through experience.</p> <p>In machine learning, the goal is to develop models that can <strong>automatically learn patterns and relationships in data, and use that knowledge to make predictions or take actions</strong>. The models are trained on a large dataset, and the learning process involves <strong>optimizing the parameters of the model to minimize the prediction error</strong>. For this purpose every algorithms uses some <strong>cost function or loss function</strong>.</p> <p>There are various types of machine learning, including supervised learning, unsupervised learning, semi-supervised learning, and reinforcement learning. These approaches are used in a wide range of applications, including image classification, speech recognition, natural language processing, recommendation systems, and predictive analytics.</p> <h2 id=what-is-cost-function>What is cost function?<a class=headerlink href=#what-is-cost-function title="Permanent link">&para;</a></h2> <p>A cost function, also known as a loss function or objective function, is a <strong>mathematical function that measures the difference between the predicted output of a model and the actual output</strong>. The cost function is used to evaluate the performance of a machine learning model and <strong>guide the optimization process during training</strong>.</p> <p>The goal of training a machine learning model is to minimize the value of the cost function. This is achieved by adjusting the parameters of the model to reduce the prediction error. The choice of cost function will depend on the type of problem being solved and the type of model being used.</p> <p>For example, in a binary classification problem, a common cost function is the cross-entropy loss, which measures the difference between the predicted probabilities and the actual class labels. In a regression problem, a common cost function is the mean squared error, which measures the average squared difference between the predicted values and the actual values.</p> <p>The cost function provides a measure of the model's performance, and the optimization process aims to find the values of the model's parameters that minimize the cost function. The optimization process is usually performed using gradient descent or other optimization algorithms, which iteratively update the parameters to reduce the value of the cost function.</p> <h2 id=what-is-the-difference-between-loss-function-and-cost-function>What is the difference between loss function and cost function?<a class=headerlink href=#what-is-the-difference-between-loss-function-and-cost-function title="Permanent link">&para;</a></h2> <p>The terms "cost function" and "loss function" are often used interchangeably, but they do have subtle differences depending on the context:</p> <h3 id=1-loss-function>1. <strong>Loss Function:</strong><a class=headerlink href=#1-loss-function title="Permanent link">&para;</a></h3> <ul> <li><strong>Definition</strong>: A loss function measures the error for a single training example. It quantifies how well the model's prediction matches the actual target value for that particular example.</li> <li><strong>Use Case</strong>: Typically used in contexts where you're evaluating or updating the model on a per-example basis.</li> <li><strong>Example</strong>: In a binary classification task, if your model predicts the probability of a sample belonging to the positive class, the loss function might be binary cross-entropy, which compares this prediction to the actual class label.</li> </ul> <h3 id=2-cost-function>2. <strong>Cost Function:</strong><a class=headerlink href=#2-cost-function title="Permanent link">&para;</a></h3> <ul> <li><strong>Definition</strong>: A cost function is generally the average (or sum) of the loss functions over an entire dataset. It provides a measure of the overall model performance across all training examples.</li> <li><strong>Use Case</strong>: Used during the training process to evaluate and minimize the overall error of the model.</li> <li><strong>Example</strong>: The cost function could be the Mean Squared Error (MSE) for a regression task, which is the average of the squared errors (individual losses) over all the training examples.</li> </ul> <h3 id=summary-of-differences>Summary of Differences:<a class=headerlink href=#summary-of-differences title="Permanent link">&para;</a></h3> <ul> <li><strong>Scope</strong>: </li> <li><strong>Loss function</strong> is usually focused on a single data point.</li> <li> <p><strong>Cost function</strong> aggregates the loss across all data points in the dataset.</p> </li> <li> <p><strong>Usage</strong>: </p> </li> <li>The term "loss function" is more commonly used when referring to the error for individual predictions.</li> <li>The term "cost function" is often used when referring to the total error used to train the model (e.g., in optimization algorithms).</li> </ul> <h3 id=example-in-practice>Example in Practice:<a class=headerlink href=#example-in-practice title="Permanent link">&para;</a></h3> <ul> <li><strong>Loss Function</strong>: If you're using Mean Squared Error as your loss function, then for each training example, the loss might be calculated as: $$ \text{Loss} = (y_{\text{true}} - y_{\text{pred}})^2 $$</li> <li><strong>Cost Function</strong>: The cost function would then be the average of these losses across all examples: $$ \text{Cost} = \frac{1}{n} \sum_{i=1}^{n} (y_{\text{true, i}} - y_{\text{pred, i}})^2 $$</li> </ul> <p>In summary, the loss function is a measure of error on a single data point, while the cost function is a measure of error across the entire dataset.</p> <p>The specific form of the cost function will depend on the type of problem being solved and the nature of the output variables. For example, in regression problems, the mean squared error (MSE) is often used as the cost function, while in classification problems, the cross-entropy loss is commonly used.</p> <h2 id=what-are-different-cost-functions>What are different cost functions?<a class=headerlink href=#what-are-different-cost-functions title="Permanent link">&para;</a></h2> <p>Cost functions, also known as loss functions, are mathematical functions used in machine learning to quantify the difference between the predicted output of a model and the actual target values. The goal of training a model is to minimize this cost function, thereby improving the model's performance. Common Cost Functions:</p> <ol> <li><strong>Mean Squared Error (MSE)</strong></li> <li><strong>Description</strong>: Measures the average squared difference between predicted values and actual values.</li> <li><strong>Formula</strong>: $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$ where $$ y_i $$ is the actual value, $$ \hat{y}_i $$ is the predicted value, and $$ n $$ is the number of data points.</li> <li> <p><strong>Use Case</strong>: Commonly used in regression tasks.</p> </li> <li> <p><strong>Mean Absolute Error (MAE)</strong></p> </li> <li><strong>Description</strong>: Measures the average absolute difference between predicted values and actual values.</li> <li><strong>Formula</strong>: $$ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $$</li> <li> <p><strong>Use Case</strong>: Used in regression tasks, particularly when outliers are less influential.</p> </li> <li> <p><strong>Binary Cross-Entropy (Log Loss)</strong></p> </li> <li><strong>Description</strong>: Measures the difference between predicted probabilities and actual binary labels. It penalizes incorrect predictions more heavily.</li> <li><strong>Formula</strong>: $$ \text{Binary Cross-Entropy} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right] $$</li> <li> <p><strong>Use Case</strong>: Used in binary classification tasks.</p> </li> <li> <p><strong>Categorical Cross-Entropy</strong></p> </li> <li><strong>Description</strong>: Measures the difference between the predicted probability distribution and the actual one-hot encoded target distribution.</li> <li><strong>Formula</strong>: $$ \text{Categorical Cross-Entropy} = -\sum_{i=1}^{n} \sum_{j=1}^{k} y_{ij} \log(\hat{y}<em ij=ij>{ij}) $$ where $$ k $$ is the number of classes, and $$ y</em> $$ is the actual one-hot encoded value.</li> <li> <p><strong>Use Case</strong>: Used in multi-class classification tasks.</p> </li> <li> <p><strong>Huber Loss</strong></p> </li> <li><strong>Description</strong>: Combines MSE and MAE, offering a balance between the two. It is less sensitive to outliers than MSE.</li> <li><strong>Formula</strong>: $$ \text{Huber Loss} = \begin{cases} \frac{1}{2}(y_i - \hat{y}_i)^2 &amp; \text{for } |y_i - \hat{y}_i| \leq \delta \ \delta \cdot |y_i - \hat{y}_i| - \frac{1}{2} \delta^2 &amp; \text{otherwise} \end{cases} $$ where $$ \delta $$ is a threshold.</li> <li> <p><strong>Use Case</strong>: Used in regression tasks, especially when dealing with outliers.</p> </li> <li> <p><strong>Hinge Loss</strong></p> </li> <li><strong>Description</strong>: Used for "maximum-margin" classification, such as SVMs. It penalizes predictions that are on the wrong side of the decision boundary or within a margin.</li> <li><strong>Formula</strong>: $$ \text{Hinge Loss} = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \cdot \hat{y}_i) $$ where $$ y_i $$ is the true label (-1 or 1) and $$ \hat{y}_i $$ is the predicted output.</li> <li> <p><strong>Use Case</strong>: Used in Support Vector Machines (SVM) for binary classification.</p> </li> <li> <p><strong>Kullback-Leibler (KL) Divergence</strong></p> </li> <li><strong>Description</strong>: Measures the divergence between two probability distributions, often used as a regularization term in models like VAEs.</li> <li><strong>Formula</strong>: $$ \text{KL Divergence} = \sum_{i} P(x_i) \log \frac{P(x_i)}{Q(x_i)} $$</li> <li> <p><strong>Use Case</strong>: Used in probabilistic models, VAEs, and reinforcement learning.</p> </li> <li> <p><strong>Poisson Loss</strong></p> </li> <li><strong>Description</strong>: Used for count data and assumes that the target variable follows a Poisson distribution.</li> <li><strong>Formula</strong>: $$ \text{Poisson Loss} = \hat{y}_i - y_i \log(\hat{y}_i) $$</li> <li> <p><strong>Use Case</strong>: Used in models where the target variable represents counts.</p> </li> <li> <p><strong>Cosine Similarity Loss</strong></p> </li> <li><strong>Description</strong>: Measures the cosine of the angle between two non-zero vectors, indicating their similarity.</li> <li><strong>Formula</strong>: $$ \text{Cosine Similarity Loss} = 1 - \frac{\sum_{i=1}^{n} y_i \hat{y}<em i=1>i}{\sqrt{\sum</em> $$}^{n} y_i^2} \cdot \sqrt{\sum_{i=1}^{n} \hat{y}_i^2}</li> <li> <p><strong>Use Case</strong>: Used in tasks like text similarity, recommendation systems, and face recognition.</p> </li> <li> <p><strong>Wasserstein Loss</strong></p> <ul> <li><strong>Description</strong>: Used in Wasserstein GANs (WGANs) to measure the distance between the real data distribution and the generated data distribution.</li> <li><strong>Formula</strong>: Depends on the specific WGAN implementation but generally involves the Earth Mover's Distance (EMD).</li> <li><strong>Use Case</strong>: Used in GANs to improve training stability and address issues like mode collapse.</li> </ul> </li> </ol> <p>Different cost functions are chosen based on the type of problem (regression, classification, etc.) and the specific characteristics of the data (e.g., presence of outliers). The goal is to find a cost function that aligns well with the task and helps the model converge to an optimal solution during training.</p> <h2 id=what-are-different-cost-functions-for-different-machine-learning-goals>What are different cost functions for different Machine learning goals?<a class=headerlink href=#what-are-different-cost-functions-for-different-machine-learning-goals title="Permanent link">&para;</a></h2> <h3 id=cost-function-for-regression>Cost Function for Regression<a class=headerlink href=#cost-function-for-regression title="Permanent link">&para;</a></h3> <p>If the goal is Regression, we want to predict a continuous number, then we can use use these cost/loss functions. - Mean Squared Error (MSE): used for regression problems, measures the average squared difference between the predicted output and the actual output. - Mean Absolute Error (MAE): also used for regression problems, measures the average absolute difference between the predicted output and the actual output. - Huber Loss: a combination of mean squared error and mean absolute error, used for robust regression. - Smooth L1 Loss: also known as the Huber Loss, a combination of mean squared error and mean absolute error, used for object detection in computer vision. - Log-Cosh Loss: a smooth approximation of the mean absolute error, used for regression problems.</p> <h3 id=cost-function-for-classification>Cost Function for Classification<a class=headerlink href=#cost-function-for-classification title="Permanent link">&para;</a></h3> <p>If the goal is Classification, we want to predict a class/category, then we can use use these cost/loss functions. - Binary Cross-Entropy Loss: a variation of cross-entropy loss, used for binary classification problems. - Cross-Entropy Loss: measures the difference between the predicted class probabilities and the actual class label, used for multi-class classification problems. - Hinge Loss: used for maximum-margin classification problems, measures the margin between the predicted class and the incorrect class. - Squared Hinge Loss: a variation of hinge loss, used for maximum-margin classification problems. - Multi-Class Logarithmic Loss: used for multi-class classification problems, measures the average log loss across all classes. - Focal Loss: used for object detection in computer vision, adds a term that modulates the cross-entropy loss based on the prediction confidence. - Categorical Cross-Entropy Loss: another variation of cross-entropy loss for multi-class classification problems.</p> <h3 id=cost-function-for-clustering>Cost Function for Clustering<a class=headerlink href=#cost-function-for-clustering title="Permanent link">&para;</a></h3> <p>If the goal is Clustering, we want to group samples/records/examples, then we can use use these cost/loss functions. - Sum of Squared Errors (SSE): measures the sum of squared differences between each data point and its nearest cluster center. - Within-Cluster Sum of Squared Errors (WCSS): similar to SSE, but measures the sum of squared differences between each data point and its nearest cluster center, averaged across all clusters. - Davies-Bouldin Index: measures the similarity between each pair of clusters, based on the distance between their cluster centers and the size of their cluster. - Silhouette Score: measures the similarity between each data point and its own cluster compared to other clusters. - Calinski-Harabasz Index: measures the ratio of between-cluster variance to within-cluster variance, based on the sum of squared differences from the cluster centers.</p> <h3 id=cost-function-for-sementic-segmenration>Cost Function for Sementic Segmenration<a class=headerlink href=#cost-function-for-sementic-segmenration title="Permanent link">&para;</a></h3> <p>If the goal is Semantic Segmentation (a type of image analysis task in computer vision where the goal is to classify each pixel in an image into a predefined category or class.), then we can use use these cost/loss functions. - Cross-Entropy Loss: measures the difference between the predicted class probabilities and the actual class label. - Dice Loss: measures the overlap between the predicted and ground-truth segmentation masks. - Intersection over Union (IoU) Loss: similar to Dice loss, measures the overlap between the predicted and ground-truth segmentation masks, but normalizes the overlap based on the size of the masks. - Jaccard Loss: a variant of IoU loss, which measures the overlap between the predicted and ground-truth segmentation masks. - Focal Loss: adds a term that modulates the cross-entropy loss based on the prediction confidence, which can improve the performance of the model on the hard examples.</p> <h2 id=cost-function-for-text-genration>Cost Function for Text Genration<a class=headerlink href=#cost-function-for-text-genration title="Permanent link">&para;</a></h2> <p>Can you tell what different loss functions are available to update the parameters during learning in Text Generation models?</p> <p>In text generation models, various loss functions are used to update the model's parameters and improve performance. The choice of loss function depends on the model architecture and the specific text generation task. Here are some common loss functions used in text generation:</p> <h3 id=1-cross-entropy-loss-negative-log-likelihood-loss>1. <strong>Cross-Entropy Loss (Negative Log-Likelihood Loss)</strong><a class=headerlink href=#1-cross-entropy-loss-negative-log-likelihood-loss title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Measures the difference between the predicted probability distribution over the vocabulary and the actual target distribution (one-hot encoded vector).</li> <li><strong>Use Case</strong>: Used extensively in sequence-to-sequence models, language models, and other text generation tasks where the goal is to predict the next token in a sequence.</li> <li><strong>Formula</strong>: $$ \text{Cross-Entropy Loss} = -\sum_{i=1}^{N} \log P(y_i | x) $$ where $$ y_i $$ is the actual token and $$ P(y_i | x) $$ is the predicted probability of the token given the input $$ x $$.</li> </ul> <h3 id=2-reinforcement-learning-based-loss-policy-gradient>2. <strong>Reinforcement Learning-Based Loss (Policy Gradient)</strong><a class=headerlink href=#2-reinforcement-learning-based-loss-policy-gradient title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Used when the text generation model is trained using reinforcement learning techniques, such as the REINFORCE algorithm. The loss is based on the reward signal received from the environment or from a discriminator in GANs.</li> <li><strong>Use Case</strong>: Used in models like SeqGAN, where text is generated in sequence and evaluated based on a reward function rather than direct supervised labels.</li> <li><strong>Formula</strong>: $$ \text{Policy Gradient Loss} = - \mathbb{E}[R(\tau) \log \pi_\theta(\tau)] $$ where $$ R(\tau) $$ is the reward associated with the generated sequence $$ \tau $$, and $$ \pi_\theta $$ is the policy (model) with parameters $$ \theta $$.</li> </ul> <h3 id=3-maximum-likelihood-estimation-mle-loss>3. <strong>Maximum Likelihood Estimation (MLE) Loss</strong><a class=headerlink href=#3-maximum-likelihood-estimation-mle-loss title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: A loss function that maximizes the likelihood of the observed data given the model parameters. It’s closely related to cross-entropy loss in the context of text generation.</li> <li><strong>Use Case</strong>: Common in sequence generation tasks where the objective is to maximize the probability of the training sequences.</li> <li><strong>Formula</strong>: $$ \text{MLE Loss} = -\sum_{t=1}^{T} \log P(y_t | y_{&lt;t}, x) $$ where $$ y_t $$ is the target token at time step $$ t $$, and $$ y_{&lt;t} $$ represents the previous tokens.</li> </ul> <h3 id=4-perplexity>4. <strong>Perplexity</strong><a class=headerlink href=#4-perplexity title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Perplexity is not a direct loss function but a metric derived from cross-entropy loss. It measures how well a probability distribution or probability model predicts a sample.</li> <li><strong>Use Case</strong>: Used as an evaluation metric for language models, lower perplexity indicates a better model.</li> <li><strong>Formula</strong>: $$ \text{Perplexity} = 2^{\text{Cross-Entropy Loss}} $$</li> </ul> <h3 id=5-kl-divergence-kullback-leibler-divergence>5. <strong>KL Divergence (Kullback-Leibler Divergence)</strong><a class=headerlink href=#5-kl-divergence-kullback-leibler-divergence title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: Measures the divergence between the predicted distribution and a target distribution. It’s often used in variational autoencoders (VAEs) for text generation.</li> <li><strong>Use Case</strong>: Regularizes models to ensure that the generated distribution doesn’t diverge too far from a prior distribution.</li> <li><strong>Formula</strong>: $$ \text{KL Divergence} = \sum_{i=1}^{N} P(y_i | x) \log \frac{P(y_i | x)}{Q(y_i)} $$ where $$ P(y_i | x) $$ is the predicted probability and $$ Q(y_i) $$ is the target probability.</li> </ul> <h3 id=6-bleu-score-based-loss>6. <strong>BLEU Score-Based Loss</strong><a class=headerlink href=#6-bleu-score-based-loss title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of text generated by comparing it with reference texts. Some approaches involve directly optimizing BLEU or using it as a reward in reinforcement learning.</li> <li><strong>Use Case</strong>: Used in machine translation and other generation tasks where the quality of the generated text relative to reference text is important.</li> <li><strong>Formula</strong>: BLEU is not directly a loss function, but can be used as a reward signal: $$ \text{BLEU} = \text{exp} \left( \min \left( 1 - \frac{\text{length of reference}}{\text{length of hypothesis}}, 0 \right) + \sum_{n=1}^{N} w_n \log p_n \right) $$ where $$ p_n $$ is the precision of n-grams and $$ w_n $$ is the weight for the n-gram order.</li> </ul> <h3 id=7-gan-discriminator-loss>7. <strong>GAN Discriminator Loss</strong><a class=headerlink href=#7-gan-discriminator-loss title="Permanent link">&para;</a></h3> <ul> <li><strong>Description</strong>: In GANs like SeqGAN or TextGAN, the discriminator provides a loss signal that reflects how well the generator’s text resembles real text. The generator’s loss is typically the inverse of the discriminator’s success.</li> <li><strong>Use Case</strong>: Used in GAN-based text generation models to refine the generator's output to be more realistic.</li> <li><strong>Formula</strong>: $$ \text{Generator Loss} = -\log(D(G(z))) $$ where $$ D(G(z)) $$ is the discriminator's probability that the generated text $$ G(z) $$ is real.</li> </ul> <h3 id=summary>Summary:<a class=headerlink href=#summary title="Permanent link">&para;</a></h3> <p>Different loss functions serve different purposes in updating the parameters of text generation models. Cross-Entropy Loss and MLE Loss are standard for sequence modeling, while reinforcement learning-based losses and GAN discriminator losses are used for more advanced techniques like GANs and policy gradient methods. Each loss function is chosen based on the specific needs of the text generation task and the model architecture.</p> <h2 id=can-you-explain-with-example-how-kl-divergence-loss-function-works>Can you explain with example how KL Divergence loss function works?<a class=headerlink href=#can-you-explain-with-example-how-kl-divergence-loss-function-works title="Permanent link">&para;</a></h2> <p>When we say "measures the divergence between the predicted distribution and a target distribution," we're referring to how different or similar two probability distributions are from each other. Specifically, it’s about comparing the distribution of the model's predictions (the predicted distribution) with the actual or expected distribution (the target distribution).</p> <h3 id=what-is-a-probability-distribution>What is a Probability Distribution?<a class=headerlink href=#what-is-a-probability-distribution title="Permanent link">&para;</a></h3> <p>A probability distribution assigns probabilities to different outcomes or events. For example, in the context of text generation, a probability distribution might tell us the likelihood of each word or token being the next in a sequence.</p> <h3 id=kl-divergence-explained>KL Divergence Explained:<a class=headerlink href=#kl-divergence-explained title="Permanent link">&para;</a></h3> <p><strong>Kullback-Leibler (KL) Divergence</strong> is a measure of how one probability distribution diverges from a second, expected probability distribution. It's not symmetric, meaning $$ (\text{KL}P | Q) $$ is not necessarily the same as $$ \text{KL}(Q | P) $$. The KL divergence is used to quantify how much information is lost when we use the predicted distribution instead of the target distribution.</p> <h3 id=example>Example:<a class=headerlink href=#example title="Permanent link">&para;</a></h3> <p>Let's consider a simple example where we have a model that is predicting the next word in a sentence.</p> <h4 id=scenario>Scenario:<a class=headerlink href=#scenario title="Permanent link">&para;</a></h4> <p>Suppose we're trying to predict the next word after "The cat is". The target distribution might look like this based on real-world data:</p> <ul> <li>Target distribution $$ Q $$:</li> <li>"sleeping": 0.7</li> <li>"running": 0.2</li> <li>"eating": 0.1</li> </ul> <p>This distribution reflects the true probabilities of each word occurring next based on our training data.</p> <p>Now, suppose our model predicts the following distribution:</p> <ul> <li>Predicted distribution $$ P $$:</li> <li>"sleeping": 0.5</li> <li>"running": 0.3</li> <li>"eating": 0.2</li> </ul> <h3 id=kl-divergence-calculation>KL Divergence Calculation:<a class=headerlink href=#kl-divergence-calculation title="Permanent link">&para;</a></h3> <p>The KL divergence between the target distribution $$ Q $$ and the predicted distribution $$ P $$ can be calculated as:</p> <div class=arithmatex>\[ \text{KL}(Q \| P) = \sum_{i} Q(i) \log \frac{Q(i)}{P(i)} \]</div> <p>For our example:</p> <div class=arithmatex>\[ \text{KL}(Q \| P) = (0.7) \log \frac{0.7}{0.5} + (0.2) \log \frac{0.2}{0.3} + (0.1) \log \frac{0.1}{0.2} \]</div> <p>Let's compute this step by step:</p> <ol> <li> <p>For "sleeping": $$ 0.7 \log \frac{0.7}{0.5} = 0.7 \times \log(1.4) \approx 0.7 \times 0.3365 = 0.2356 $$</p> </li> <li> <p>For "running": $$ 0.2 \log \frac{0.2}{0.3} = 0.2 \times \log(0.6667) \approx 0.2 \times (-0.1761) = -0.0352 $$</p> </li> <li> <p>For "eating": $$ 0.1 \log \frac{0.1}{0.2} = 0.1 \times \log(0.5) \approx 0.1 \times (-0.3010) = -0.0301 $$</p> </li> </ol> <p>Finally, summing these values gives us:</p> <div class=arithmatex>\[ \text{KL}(Q \| P) = 0.2356 - 0.0352 - 0.0301 = 0.1703 \]</div> <h3 id=interpretation>Interpretation:<a class=headerlink href=#interpretation title="Permanent link">&para;</a></h3> <ul> <li><strong>KL Divergence Value</strong>: The KL divergence value of 0.1703 indicates that there's some divergence between the predicted and target distributions. If the predicted distribution $$ P $$ were exactly the same as the target distribution $$ Q $$, the KL divergence would be 0, indicating no divergence.</li> <li><strong>Minimizing KL Divergence</strong>: In practice, during training, we try to minimize the KL divergence to make the model’s predictions as close as possible to the target distribution.</li> </ul> <h3 id=summary_1>Summary:<a class=headerlink href=#summary_1 title="Permanent link">&para;</a></h3> <p>KL Divergence measures how much the predicted probability distribution diverges from the actual, expected distribution. In the context of our example, it quantifies how different the model’s predicted probabilities for the next word in a sentence are from the true probabilities based on training data. Minimizing KL divergence during training helps improve the model’s accuracy in making predictions.</p> <h2 id=what-is-optimizer>What is Optimizer?<a class=headerlink href=#what-is-optimizer title="Permanent link">&para;</a></h2> <p>Optimizers play a crucial role in deep neural network training. They are responsible for updating the model's parameters in order to minimise the loss function, and ultimately improve the performance of the model. There are many different optimisers available, each with their own strengths and weaknesses, and choosing the right optimiser can make a significant impact on the training process. </p> <h2 id=what-are-the-various-optimization-algorithms>What are the various optimization algorithms?<a class=headerlink href=#what-are-the-various-optimization-algorithms title="Permanent link">&para;</a></h2> <p>Some popular optimisers include stochastic gradient descent (SGD), momentum, Adagrad, Adadelta, RProp, RMSprop, Adam, AMSGrad, and Nadam. These optimisers differ in how they calculate the updates to the model's parameters, with some taking into account the historical gradient information, others using momentum to smooth out updates, and others adapting the learning rate based on the magnitude of the gradients. It's important to carefully consider the properties of the cost function and the structure of the model when selecting an optimiser, as this can have a significant impact on the speed and stability of the training process.</p> <h2 id=can-you-explain-with-example-how-these-optimizers-update-the-model-parameters>Can you explain with example how these optimizers update the model parameters?<a class=headerlink href=#can-you-explain-with-example-how-these-optimizers-update-the-model-parameters title="Permanent link">&para;</a></h2> <ul> <li> <p>Stochastic Gradient Descent (SGD) - It updates the model parameters by taking the gradient of the loss function with respect to the parameters and subtracting it from the parameters. <br> <span class=arithmatex>\(<span class=arithmatex>\(\theta = \theta - \alpha \frac{\partial J(\theta)}{\partial \theta}\)</span>\)</span> </p> <p>where <span class=arithmatex>\(<span class=arithmatex>\(\theta\)</span>\)</span> is the model parameter, <span class=arithmatex>\(<span class=arithmatex>\(\alpha\)</span>\)</span> is the learning rate, and <span class=arithmatex>\(<span class=arithmatex>\(J(\theta)\)</span>\)</span> is the cost function. </p> </li> <li> <p>Momentum - It accumulates the gradient of the previous steps to avoid oscillation and converge faster. <br> <span class=arithmatex>\(v = \beta v - \alpha \frac{\partial J(\theta)}{\partial \theta}\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\theta = \theta + v\)</span>\)</span> </p> <p>where <span class=arithmatex>\(v\)</span> is the velocity term, <span class=arithmatex>\(<span class=arithmatex>\(\beta\)</span>\)</span> is the momentum hyperparameter. </p> </li> <li> <p>Nesterov Accelerated Gradient (NAG) - It is an improved version of Momentum that takes into account the future position of the parameters based on the estimated gradient. <br> <span class=arithmatex>\(v = \beta v - \alpha \frac{\partial J(\theta + \beta v)}{\partial \theta}\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\theta = \theta + v\)</span>\)</span> </p> </li> <li> <p>Adagrad - It adapts the learning rate to the parameters, performing larger updates for infrequent parameters and smaller updates for frequent parameters. <br> <span class=arithmatex>\(<span class=arithmatex>\(G = G + \left(\frac{\partial J(\theta)}{\partial \theta}\right)^2\)</span>\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\theta = \theta - \frac{\alpha}{\sqrt{G + \epsilon}} \frac{\partial J(\theta)}{\partial \theta}\)</span>\)</span> </p> <p>where <span class=arithmatex>\(<span class=arithmatex>\(G\)</span>\)</span> is the sum of squares of past gradients, and <span class=arithmatex>\(<span class=arithmatex>\(\epsilon\)</span>\)</span> is a small value to prevent division by zero. </p> </li> <li> <p>Adadelta - It is an extension of Adagrad that reduces its aggressive, monotonically decreasing learning rate. <span class=arithmatex>\(E[g^2]t = \gamma E[g^2]{t-1} + (1 - \gamma)\left(\frac{\partial J(\theta)}{\partial \theta}\right)^2\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\Delta \theta_t = -\frac{\sqrt{E[\Delta \theta^2]_{t-1} + \epsilon}}{\sqrt{E[g^2]_t + \epsilon}} \frac{\partial J(\theta)}{\partial \theta}\)</span>\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\theta = \theta + \Delta \theta_t\)</span>\)</span> </p> <p>where <span class=arithmatex>\(E[g^2]\)</span> and <span class=arithmatex>\(E[\Delta \theta^2]\)</span> are the moving average of the square of gradients and the square of parameter updates, respectively, and <span class=arithmatex>\(<span class=arithmatex>\(\gamma\)</span>\)</span> is the decay rate. </p> </li> <li> <p>RProp - It uses the sign of the gradient to determine the direction of the update, with a dynamically adjusted step size for each parameter.</p> <p><span class=arithmatex>\(<span class=arithmatex>\(\Delta \theta_i = \text{sign}(\frac{\partial J(\theta)}{\partial \theta_i})\Delta \theta_{i,prev}\)</span>\)</span></p> <p><span class=arithmatex>\(<span class=arithmatex>\(\theta_i = \theta_i - \Delta \theta_i\)</span>\)</span></p> <p>where <span class=arithmatex>\(<span class=arithmatex>\(\theta_i\)</span>\)</span> is the current value of a model parameter, <span class=arithmatex>\(<span class=arithmatex>\(\frac{\partial J(\theta)}{\partial \theta_i}\)</span>\)</span> is the gradient of the loss function with respect to the parameter, <span class=arithmatex>\(<span class=arithmatex>\(\Delta \theta_{i,prev}\)</span>\)</span> is the previous update to the parameter, and <span class=arithmatex>\(<span class=arithmatex>\(\text{sign}(\cdot)\)</span>\)</span> is the sign function. The step size <span class=arithmatex>\(<span class=arithmatex>\(\Delta \theta_i\)</span>\)</span> is determined dynamically based on the magnitude of the gradient.</p> </li> <li> <p>Adam (Adaptive Moment Estimation) - It combines the advantages of Momentum and Adagrad, by considering both the average and the variance of the gradient for parameter updates. <br> <span class=arithmatex>\(<span class=arithmatex>\(m = \beta_1 m + (1 - \beta_1) \frac{\partial J(\theta)}{\partial \theta}\)</span>\)</span> </p> <div class=arithmatex>\[v = \beta_2 v + (1 - \beta_2) \left(\frac{\partial J(\theta)}{\partial \theta}\right)^2\]</div> <p><span class=arithmatex>\(<span class=arithmatex>\(\hat{m} = \frac{m}{1 - \beta_1^t}\)</span>\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\hat{v} = \frac{v}{1 - \beta_2^t}\)</span>\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\theta = \theta - \frac{\alpha}{\sqrt{\hat{v}} + \epsilon} \hat{m}\)</span>\)</span> </p> <p>where <span class=arithmatex>\(<span class=arithmatex>\(m\)</span>\)</span> and <span class=arithmatex>\(<span class=arithmatex>\(v\)</span>\)</span> are the first and second moment estimates, respectively, <span class=arithmatex>\(<span class=arithmatex>\(\beta_1\)</span>\)</span> and <span class=arithmatex>\(<span class=arithmatex>\(\beta_2\)</span>\)</span> are hyperparameters, and the rest of the terms are as defined above. </p> </li> <li> <p>AMSGrad - It is an extension of Adam that ensures the learning rate does not get too small, even if the gradient is small. <br> <span class=arithmatex>\(m = \beta_1 m + (1 - \beta_1) \frac{\partial J(\theta)}{\partial \theta}\)</span> </p> <div class=arithmatex>\[v = \max(\beta_2 v + (1 - \beta_2) \left(\frac{\partial J(\theta)}{\partial \theta}\right)^2, v_{t-1})\]</div> <p><span class=arithmatex>\(<span class=arithmatex>\(\hat{m} = \frac{m}{1 - \beta_1^t}\)</span>\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\hat{v} = \frac{v}{1 - \beta_2^t}\)</span>\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\theta = \theta - \frac{\alpha}{\sqrt{\hat{v}} + \epsilon} \hat{m}\)</span>\)</span> </p> </li> <li> <p>Nadam (Nesterov-Accelerated Adaptive Moment Estimation) - It combines NAG and Adam to take advantage of the rapid convergence of NAG and the adaptive learning rate of Adam. <br> <span class=arithmatex>\(<span class=arithmatex>\(m = \beta_1 m + (1 - \beta_1) \frac{\partial J(\theta)}{\partial \theta}\)</span>\)</span></p> <div class=arithmatex>\[v = \beta_2 v + (1 - \beta_2) \left(\frac{\partial J(\theta)}{\partial \theta}\right)^2\]</div> <p><span class=arithmatex>\(<span class=arithmatex>\(\hat{m} = \frac{m}{1 - \beta_1^t}\)</span>\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\hat{v} = \frac{v}{1 - \beta_2^t}\)</span>\)</span> </p> <p><span class=arithmatex>\(<span class=arithmatex>\(\theta = \theta - \frac{\alpha}{\sqrt{\hat{v}} + \epsilon} \left(\beta_1 \hat{m} + (1 - \beta_1) \frac{\partial J(\theta)}{\partial \theta}\right)\)</span>\)</span> </p> </li> </ul> <p><strong>Author</strong> <br> Dr Hari Thapliyal <br> <a href=https://linkedin.com/in/harithapliyal>https://linkedin.com/in/harithapliyal</a> <br> <a href=https://dasarpai.com>https://dasarpai.com</a> </p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>