<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Transformers_Demystified_A_Step-by-Step_Guide.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Transformers Demystified A Step-by-Step Guide - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#transformers-demystified-a-step-by-step-guide class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Transformers Demystified A Step-by-Step Guide </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=#what-was-need-of-this-work class=md-nav__link> <span class=md-ellipsis> What was need of this work? </span> </a> <nav class=md-nav aria-label="What was need of this work?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#limitations-of-previous-models class=md-nav__link> <span class=md-ellipsis> Limitations of Previous Models </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#nlp-tasks class=md-nav__link> <span class=md-ellipsis> NLP Tasks </span> </a> </li> <li class=md-nav__item> <a href=#background class=md-nav__link> <span class=md-ellipsis> Background </span> </a> <nav class=md-nav aria-label=Background> <ul class=md-nav__list> <li class=md-nav__item> <a href=#metrics class=md-nav__link> <span class=md-ellipsis> Metrics </span> </a> </li> <li class=md-nav__item> <a href=#benchmarks class=md-nav__link> <span class=md-ellipsis> Benchmarks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#key-terms class=md-nav__link> <span class=md-ellipsis> Key terms </span> </a> </li> <li class=md-nav__item> <a href=#with-encoder-only-architecture-we-can-do class=md-nav__link> <span class=md-ellipsis> With "Encoder Only" Architecture we can do. </span> </a> </li> <li class=md-nav__item> <a href=#with-decoder-only-architecture-we-can-do class=md-nav__link> <span class=md-ellipsis> With "Decoder only" Architecture we can do. </span> </a> </li> <li class=md-nav__item> <a href=#with-encoder-decoder-architecture-we-can-do-following-task class=md-nav__link> <span class=md-ellipsis> With Encoder-Decoder Architecture we can do following task. </span> </a> </li> <li class=md-nav__item> <a href=#why-do-we-need-encoder-decoder-architectures class=md-nav__link> <span class=md-ellipsis> Why do we need encoder-decoder architectures? </span> </a> <nav class=md-nav aria-label="Why do we need encoder-decoder architectures?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#resource-considerations class=md-nav__link> <span class=md-ellipsis> Resource Considerations </span> </a> </li> <li class=md-nav__item> <a href=#summary class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#popular-transformer-architectures class=md-nav__link> <span class=md-ellipsis> Popular Transformer Architectures </span> </a> </li> <li class=md-nav__item> <a href=#why-multi-headed-attention class=md-nav__link> <span class=md-ellipsis> Why Multi headed attention? </span> </a> <nav class=md-nav aria-label="Why Multi headed attention?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#syntactic-relationships class=md-nav__link> <span class=md-ellipsis> Syntactic Relationships: </span> </a> </li> <li class=md-nav__item> <a href=#semantic-relationships class=md-nav__link> <span class=md-ellipsis> Semantic Relationships: </span> </a> </li> <li class=md-nav__item> <a href=#coreference-resolution class=md-nav__link> <span class=md-ellipsis> Coreference Resolution: </span> </a> </li> <li class=md-nav__item> <a href=#long-range-dependencies class=md-nav__link> <span class=md-ellipsis> Long-Range Dependencies: </span> </a> </li> <li class=md-nav__item> <a href=#positional-information class=md-nav__link> <span class=md-ellipsis> Positional Information: </span> </a> </li> <li class=md-nav__item> <a href=#named-entity-recognition class=md-nav__link> <span class=md-ellipsis> Named Entity Recognition: </span> </a> </li> <li class=md-nav__item> <a href=#polarity-and-sentiment class=md-nav__link> <span class=md-ellipsis> Polarity and Sentiment: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#how-attention-works class=md-nav__link> <span class=md-ellipsis> How attention works? </span> </a> </li> <li class=md-nav__item> <a href=#how-multihead-attention-works class=md-nav__link> <span class=md-ellipsis> How multihead attention works. </span> </a> </li> <li class=md-nav__item> <a href=#how-q-k-v-calculated class=md-nav__link> <span class=md-ellipsis> How Q, K, V Calculated? </span> </a> <nav class=md-nav aria-label="How Q, K, V Calculated?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#steps-for-multi-head-attention class=md-nav__link> <span class=md-ellipsis> Steps for Multi-Head Attention: </span> </a> </li> <li class=md-nav__item> <a href=#scaled-dot-product-attention-per-head class=md-nav__link> <span class=md-ellipsis> Scaled Dot-Product Attention (Per Head): </span> </a> </li> <li class=md-nav__item> <a href=#concatenation-and-final-linear-layer class=md-nav__link> <span class=md-ellipsis> Concatenation and Final Linear Layer: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#floating-point-value-range class=md-nav__link> <span class=md-ellipsis> Floating Point Value Range </span> </a> <nav class=md-nav aria-label="Floating Point Value Range"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#common-precision-formats-for-embeddings class=md-nav__link> <span class=md-ellipsis> Common Precision Formats for Embeddings </span> </a> </li> <li class=md-nav__item> <a href=#in-summary class=md-nav__link> <span class=md-ellipsis> In Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#illustration-of-working-of-transformer class=md-nav__link> <span class=md-ellipsis> Illustration of Working of Transformer </span> </a> <nav class=md-nav aria-label="Illustration of Working of Transformer"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#step-1-tokenization-and-embedding class=md-nav__link> <span class=md-ellipsis> Step 1: Tokenization and Embedding </span> </a> </li> <li class=md-nav__item> <a href=#step-2-input-to-the-transformer-model class=md-nav__link> <span class=md-ellipsis> Step 2: Input to the Transformer Model </span> </a> </li> <li class=md-nav__item> <a href=#step-3-self-attention class=md-nav__link> <span class=md-ellipsis> Step 3: Self Attention </span> </a> </li> <li class=md-nav__item> <a href=#step-4-processing-through-transformer-layers class=md-nav__link> <span class=md-ellipsis> Step 4. Processing Through Transformer Layers </span> </a> </li> <li class=md-nav__item> <a href=#step-5-output-logits class=md-nav__link> <span class=md-ellipsis> Step 5. Output Logits </span> </a> </li> <li class=md-nav__item> <a href=#step-6-softmax-function class=md-nav__link> <span class=md-ellipsis> Step 6. Softmax Function </span> </a> </li> <li class=md-nav__item> <a href=#step-7-token-selection class=md-nav__link> <span class=md-ellipsis> Step 7. Token Selection </span> </a> </li> <li class=md-nav__item> <a href=#step-8-generating-the-next-token class=md-nav__link> <span class=md-ellipsis> Step 8. Generating the Next Token </span> </a> </li> <li class=md-nav__item> <a href=#generating-text class=md-nav__link> <span class=md-ellipsis> Generating Text </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#what-different-parameters-are-learned-during-transformer-training class=md-nav__link> <span class=md-ellipsis> What different parameters are learned during transformer training? </span> </a> <nav class=md-nav aria-label="What different parameters are learned during transformer training?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-weights-and-biases-in-transformer-architecture class=md-nav__link> <span class=md-ellipsis> 1. Weights and Biases in Transformer Architecture </span> </a> <nav class=md-nav aria-label="1. Weights and Biases in Transformer Architecture"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#attention-mechanism-weights class=md-nav__link> <span class=md-ellipsis> Attention Mechanism Weights </span> </a> </li> <li class=md-nav__item> <a href=#feed-forward-network-weights class=md-nav__link> <span class=md-ellipsis> Feed-Forward Network Weights </span> </a> </li> <li class=md-nav__item> <a href=#layer-normalization-weights class=md-nav__link> <span class=md-ellipsis> Layer Normalization Weights </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-overall-model-weights-and-biases class=md-nav__link> <span class=md-ellipsis> 2. Overall Model Weights and Biases </span> </a> <nav class=md-nav aria-label="2. Overall Model Weights and Biases"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#embedding-weights class=md-nav__link> <span class=md-ellipsis> Embedding Weights </span> </a> </li> <li class=md-nav__item> <a href=#layer-weights class=md-nav__link> <span class=md-ellipsis> Layer Weights </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#summary_1 class=md-nav__link> <span class=md-ellipsis> Summary </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#self-attention-mechanism class=md-nav__link> <span class=md-ellipsis> Self Attention Mechanism </span> </a> <nav class=md-nav aria-label="Self Attention Mechanism"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#compute-q-k-v-matrices-for-multi-head-attention-2-heads class=md-nav__link> <span class=md-ellipsis> Compute Q, K, V Matrices for Multi-Head Attention (2 heads) </span> </a> </li> <li class=md-nav__item> <a href=#compute-q-k-v-for-head-1 class=md-nav__link> <span class=md-ellipsis> Compute Q, K, V for Head 1 </span> </a> <nav class=md-nav aria-label="Compute Q, K, V for Head 1"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#compute-q-k-v-for-each-token-for-head-1 class=md-nav__link> <span class=md-ellipsis> Compute Q, K, V for each token for Head 1: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#step-3-self-attention-calculation class=md-nav__link> <span class=md-ellipsis> Step 3: Self-Attention Calculation </span> </a> <nav class=md-nav aria-label="Step 3: Self-Attention Calculation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#compute-attention-scores class=md-nav__link> <span class=md-ellipsis> Compute attention scores: </span> </a> </li> <li class=md-nav__item> <a href=#apply-softmax-to-obtain-attention-weights class=md-nav__link> <span class=md-ellipsis> Apply softmax to obtain attention weights: </span> </a> </li> <li class=md-nav__item> <a href=#compute-the-weighted-sum-of-the-values class=md-nav__link> <span class=md-ellipsis> Compute the weighted sum of the values: </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#position-embedding-mechanism class=md-nav__link> <span class=md-ellipsis> Position Embedding Mechanism </span> </a> <nav class=md-nav aria-label="Position Embedding Mechanism"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#formulas-for-position-embedding class=md-nav__link> <span class=md-ellipsis> Formulas for Position Embedding </span> </a> </li> <li class=md-nav__item> <a href=#explanation class=md-nav__link> <span class=md-ellipsis> Explanation </span> </a> </li> <li class=md-nav__item> <a href=#example class=md-nav__link> <span class=md-ellipsis> Example </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/Transformers_Demystified_A_Step-by-Step_Guide.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/Transformers_Demystified_A_Step-by-Step_Guide.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="Transformers Demystified A Step-by-Step Guide" src=../assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg></p> <h1 id=transformers-demystified-a-step-by-step-guide>Transformers Demystified A Step-by-Step Guide<a class=headerlink href=#transformers-demystified-a-step-by-step-guide title="Permanent link">&para;</a></h1> <p>All modern Transformers are based on a paper "Attention is all you need"</p> <h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h2> <p>This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have. - The need Why this paper was needed? What problem it solved? - What is transformer? What is encoder transformer? What is decoder transformer? What is encoder-decoder transformer? - What is embedding? What is need for embedding? What are different types of embedding? What embeddingg is proposed in this work - What benchmark dataset was used, what metrics were used and what was the performance of this model? - Finally we will looks all the calculations with one illustration.</p> <p>Encourage all to read this <a href=https://arxiv.org/abs/1706.03762>original paper</a>.</p> <h2 id=what-was-need-of-this-work>What was need of this work?<a class=headerlink href=#what-was-need-of-this-work title="Permanent link">&para;</a></h2> <p>This paper addressed several limitations of previous sequence-to-sequence models used for tasks such as machine translation, text summarization, and other natural language processing (NLP) applications. The need for this paper arose from various challenges and limitations in existing models:</p> <h3 id=limitations-of-previous-models>Limitations of Previous Models<a class=headerlink href=#limitations-of-previous-models title="Permanent link">&para;</a></h3> <ol> <li><strong>Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) Networks</strong>:</li> <li><strong>Sequential Computation</strong>: RNNs and LSTMs process sequences step-by-step, which makes it difficult to parallelize computations and slows down training and inference.</li> <li><strong>Long-Range Dependencies</strong>: RNNs and LSTMs struggle to capture dependencies in long sequences, leading to difficulties in understanding context over long distances.</li> <li> <p><strong>Gradient Issues</strong>: These models can suffer from vanishing or exploding gradient problems, particularly with long sequences.</p> </li> <li> <p><strong>Convolutional Neural Networks (CNNs)</strong>:</p> </li> <li><strong>Fixed Context Size</strong>: CNNs have a fixed receptive field, which can limit their ability to capture long-range dependencies effectively.</li> <li><strong>Complexity</strong>: Extending CNNs to capture longer contexts can significantly increase the model's complexity and computational cost.</li> </ol> <p>The introduction of the Transformer architecture had a profound impact on the field of NLP and beyond. It paved the way for the development of large-scale pre-trained language models like BERT, GPT, and others, which have since become the foundation for many state-of-the-art AI applications. The principles of the Transformer architecture have also been adapted for other domains, such as image processing and reinforcement learning.</p> <h2 id=nlp-tasks>NLP Tasks<a class=headerlink href=#nlp-tasks title="Permanent link">&para;</a></h2> <ol> <li><strong>Text Classification</strong>:</li> <li><strong>Spam Detection</strong>: Classifying messages as spam or non-spam.</li> <li><strong>Topic Classification</strong>: Categorizing text into predefined topics or categories.</li> <li><strong>Sarcasm Detection</strong></li> <li> <p><strong>Offensive Language Detection</strong></p> </li> <li> <p><strong>Sentiment and Emotion Analysis</strong>: Determining the sentiment or emotional tone expressed in a text.</p> </li> <li> <p><strong>Named Entity Recognition (NER)</strong>: Identifying and classifying named entities (e.g., people, organizations, locations) within a text.</p> </li> <li> <p><strong>Part-of-Speech Tagging (POS Tagging)</strong>: Assigning parts of speech (e.g., nouns, verbs, adjectives) to each word in a text.</p> </li> <li> <p><strong>Machine Translation</strong>: Translating text from one language to another (e.g., English to French).</p> </li> <li> <p><strong>Language Modeling</strong>: Predicting the next word or character in a sequence, often used in generating text or improving other NLP tasks.</p> </li> <li> <p><strong>Text Summarization</strong>:</p> </li> <li><strong>Extractive Summarization</strong>: Extracting key sentences from a text to create a summary.</li> <li> <p><strong>Abstractive Summarization</strong>: Generating a concise summary that captures the main ideas of the text.</p> </li> <li> <p><strong>Question Answering</strong>: Providing answers to questions based on a given text or dataset.</p> </li> <li> <p><strong>Text Generation</strong>: Generating coherent and contextually relevant text, such as in chatbots or story generation.</p> </li> <li> <p><strong>Text Similarity</strong>: Measuring how similar two pieces of text are, which can be used in tasks like duplicate detection or paraphrase identification.</p> </li> <li> <p><strong>Coreference Resolution</strong>: Identifying when different expressions in a text refer to the same entity.</p> </li> <li> <p><strong>Speech Recognition</strong>: Converting spoken language into text.</p> </li> <li> <p><strong>Speech Synthesis (Text-to-Speech)</strong>: Converting text into spoken language.</p> </li> <li> <p><strong>Dialogue Systems</strong>:</p> <ul> <li><strong>Chatbots</strong>: Engaging in conversation with users.</li> <li><strong>Virtual Assistants</strong>: Assisting users with tasks through natural language interactions.</li> </ul> </li> <li> <p><strong>Information Retrieval</strong>: Finding relevant information within large datasets or the web, such as search engines.</p> </li> <li> <p><strong>Dependency Parsing</strong>: Analyzing the grammatical structure of a sentence to establish relationships between words.</p> </li> <li> <p><strong>Grammar and Spelling Correction</strong>: Identifying and correcting grammatical errors and typos in text.</p> </li> <li> <p><strong>Textual Entailment</strong>: Determining if one sentence logically follows from another.</p> </li> <li> <p><strong>Word Sense Disambiguation</strong>: Identifying which sense of a word is used in a given context.</p> </li> <li> <p><strong>Natural Language Inference (NLI)</strong>: Determining if a premise logically entails a hypothesis.</p> </li> </ol> <p>Each of these tasks involves different techniques and models, often leveraging machine learning and deep learning to achieve state-of-the-art performance.</p> <h2 id=background>Background<a class=headerlink href=#background title="Permanent link">&para;</a></h2> <p>Google Translate was launched on April 28, 2006. Initially, it was a statistical machine translation service that used United Nations and European Parliament transcripts to gather linguistic data.</p> <p>It used a statistical machine translation (SMT) approach. This method relied on statistical models to translate text based on patterns found in large volumes of bilingual text corpora. The SMT system analyzed these patterns to make educated guesses about the most likely translations.</p> <p>In 2016, Google Translate transitioned to using a neural machine translation (NMT) system, specifically the Google Neural Machine Translation (GNMT) system. This system uses deep learning techniques and neural networks to provide more accurate and natural translations by considering the entire sentence as a whole, rather than translating piece by piece.</p> <h3 id=metrics>Metrics<a class=headerlink href=#metrics title="Permanent link">&para;</a></h3> <p>BLEU (Bilingual Evaluation Understudy) score, which measures the quality of machine-translated text against reference translations. </p> <p>Google reported that the new NMT system achieved improvements ranging from 55% to 85% across various language pairs in terms of BLEU scores compared to their previous SMT system. This was a substantial leap in translation quality.</p> <ul> <li>For Chinese to English translations, the BLEU score improvement was reported to be around 24.2, a significant increase from the previous SMT system.</li> <li>For English to French, the BLEU score improvement was noted as being around 5-8 points higher than the SMT system.</li> </ul> <h3 id=benchmarks>Benchmarks<a class=headerlink href=#benchmarks title="Permanent link">&para;</a></h3> <ol> <li><strong>WMT 2014 English-to-German (En-De) Translation Task</strong>:</li> <li><strong>Dataset</strong>: The dataset consisted of 4.5 million sentence pairs.</li> <li> <p><strong>Performance</strong>: The Transformer model achieved a BLEU score of 28.4, which was a significant improvement over the previous state-of-the-art models.</p> </li> <li> <p><strong>WMT 2014 English-to-French (En-Fr) Translation Task</strong>:</p> </li> <li><strong>Dataset</strong>: The dataset consisted of 36 million sentence pairs.</li> <li><strong>Performance</strong>: The Transformer model achieved a BLEU score of 41.8, which also outperformed previous models.</li> </ol> <h2 id=key-terms>Key terms<a class=headerlink href=#key-terms title="Permanent link">&para;</a></h2> <ol> <li> <p><strong>Transformer Architecture</strong>: The paper introduces the Transformer, a novel architecture solely based on attention mechanisms, dispensing with recurrence and convolutions entirely.</p> </li> <li> <p><strong>Attention Mechanism</strong>: The core idea is the use of self-attention, allowing the model to weigh the importance of different words in a sentence when forming a representation of each word.</p> </li> <li> <p><strong>Self-Attention</strong>: Self-attention allows each word to focus on different parts of the input sentence, making it easier for the model to understand context and relationships between words.</p> </li> <li> <p><strong>Multi-Head Attention</strong>: Instead of performing a single attention function, the Transformer employs multiple attention heads, each focusing on different parts of the sentence, capturing diverse aspects of the information.</p> </li> <li> <p><strong>Positional Encoding</strong>: Since the Transformer does not have recurrence, it uses positional encodings to give the model information about the position of each word in the sentence.</p> </li> <li> <p><strong>Layer Normalization and Residual Connections</strong>: Each sub-layer (multi-head attention and feed-forward) is followed by layer normalization and residual connections, aiding in training deep networks.</p> </li> <li> <p><strong>Encoder-Decoder Structure</strong>: The Transformer is composed of an encoder (which processes the input) and a decoder (which generates the output). Each consists of multiple layers of self-attention and feed-forward networks.</p> </li> <li> <p><strong>Performance</strong>: The Transformer achieves state-of-the-art performance on several NLP tasks while being more parallelizable and faster to train than recurrent architectures like LSTMs and GRUs.</p> </li> <li> <p><strong>Scalability</strong>: Due to its architecture, the Transformer scales well with the amount of available data and computational power, making it suitable for large-scale tasks.</p> </li> </ol> <h2 id=with-encoder-only-architecture-we-can-do>With "Encoder Only" Architecture we can do.<a class=headerlink href=#with-encoder-only-architecture-we-can-do title="Permanent link">&para;</a></h2> <p>An encoder-only architecture, such as BERT (Bidirectional Encoder Representations from Transformers), is typically used for tasks that require understanding and representing input sequences without generating sequences. Here are some common NLP tasks that can be effectively handled using an encoder-only architecture:</p> <ol> <li><strong>Text Classification</strong>:</li> <li><strong>Sentiment Analysis</strong>: Determining the sentiment (positive, negative, neutral) expressed in a piece of text.</li> <li><strong>Spam Detection</strong>: Classifying messages as spam or not spam.</li> <li> <p><strong>Topic Classification</strong>: Categorizing text into predefined topics or categories.</p> </li> <li> <p><strong>Named Entity Recognition (NER)</strong>: Identifying and classifying named entities (e.g., persons, organizations, locations) within the text.</p> </li> <li> <p><strong>Part-of-Speech Tagging (POS Tagging)</strong>: Assigning parts of speech (e.g., nouns, verbs, adjectives) to each word in the text.</p> </li> <li> <p><strong>Question Answering (Extractive)</strong>: Extracting an answer from a given context based on a query. For instance, answering questions from a passage of text.</p> </li> <li> <p><strong>Textual Entailment</strong>: Determining whether a given hypothesis logically follows from a premise (also known as Natural Language Inference, NLI).</p> </li> <li> <p><strong>Sentence Pair Classification</strong>:</p> </li> <li><strong>Paraphrase Detection</strong>: Identifying whether two sentences are paraphrases of each other.</li> <li> <p><strong>Semantic Similarity</strong>: Measuring how similar two sentences or phrases are in meaning.</p> </li> <li> <p><strong>Coreference Resolution</strong>: Determining which expressions in a text refer to the same entity.</p> </li> <li> <p><strong>Text Summarization (Extractive)</strong>: Selecting the most important sentences from a document to create a summary.</p> </li> <li> <p><strong>Grammar and Spelling Correction</strong>: Identifying and correcting grammatical errors and typos in text.</p> </li> <li> <p><strong>Information Retrieval</strong>: Ranking and retrieving relevant documents based on a query.</p> </li> <li> <p><strong>Document Classification</strong>: Categorizing entire documents into classes or categories.</p> </li> <li> <p><strong>Named Entity Disambiguation</strong>: Resolving ambiguities in named entities to identify the correct entity among potential candidates.</p> </li> <li> <p><strong>Feature Extraction for Downstream Tasks</strong>: Generating contextualized embeddings from text to be used as features in other machine learning models.</p> </li> </ol> <p>Encoder-only models are particularly effective in tasks that benefit from understanding the context and semantics of the input text, as they are designed to capture rich, contextual representations of the input data.</p> <h2 id=with-decoder-only-architecture-we-can-do>With "Decoder only" Architecture we can do.<a class=headerlink href=#with-decoder-only-architecture-we-can-do title="Permanent link">&para;</a></h2> <p>Decoder-only architectures, such as GPT (Generative Pre-trained Transformer), are primarily designed for tasks involving text generation. These models are well-suited for autoregressive tasks where the goal is to predict or generate text based on a given context. Here are some common NLP tasks that can be effectively handled using a decoder-only architecture:</p> <ol> <li><strong>Text Generation</strong>:</li> <li><strong>Creative Writing</strong>: Generating coherent and contextually relevant creative content, such as stories, poems, or dialogues.</li> <li> <p><strong>Content Generation</strong>: Creating blog posts, articles, or other types of written content.</p> </li> <li> <p><strong>Language Modeling</strong>:</p> </li> <li><strong>Next Word Prediction</strong>: Predicting the next word or token in a sequence given the preceding context.</li> <li> <p><strong>Completion</strong>: Providing completion for a partially written sentence or text.</p> </li> <li> <p><strong>Conversational AI</strong>:</p> </li> <li><strong>Chatbots</strong>: Engaging in conversation with users, generating responses to user inputs.</li> <li> <p><strong>Virtual Assistants</strong>: Assisting with tasks through natural language interactions.</p> </li> <li> <p><strong>Text Summarization (Abstractive)</strong>: Generating a concise summary of a text that captures the main ideas, potentially rephrasing and reorganizing information.</p> </li> <li> <p><strong>Machine Translation</strong>: Translating text from one language to another in an autoregressive manner, generating translated sentences token by token.</p> </li> <li> <p><strong>Dialogue Generation</strong>:</p> </li> <li><strong>Interactive Fiction</strong>: Generating dialogues in interactive fiction or role-playing scenarios.</li> <li> <p><strong>Conversational Agents</strong>: Generating responses in a conversation based on the context of the dialogue.</p> </li> <li> <p><strong>Storytelling</strong>: Creating narratives or expanding on a given prompt to generate a complete story or narrative.</p> </li> <li> <p><strong>Autoregressive Text Editing</strong>: Modifying or editing text based on a given context, such as rewriting or expanding text.</p> </li> <li> <p><strong>Text-based Games</strong>: Generating responses and interactions in text-based games or interactive storytelling environments.</p> </li> </ol> <p>Decoder-only architectures excel in generating text sequences and modeling complex language patterns due to their autoregressive nature. They predict the next token in a sequence based on the previous tokens, which makes them ideal for tasks that involve creating or completing text.</p> <h2 id=with-encoder-decoder-architecture-we-can-do-following-task>With Encoder-Decoder Architecture we can do following task.<a class=headerlink href=#with-encoder-decoder-architecture-we-can-do-following-task title="Permanent link">&para;</a></h2> <p>Encoder-decoder architectures, like those used in the original Transformer model and its derivatives (e.g., T5, BART), are particularly well-suited for tasks that involve transforming one sequence into another. These models leverage the encoder to process and understand the input sequence and the decoder to generate the output sequence. Here are some key tasks that benefit from an encoder-decoder architecture:</p> <ol> <li> <p><strong>Machine Translation</strong>: Translating text from one language to another. The encoder processes the source language text, while the decoder generates the translated text in the target language.</p> </li> <li> <p><strong>Text Summarization</strong>:</p> </li> <li> <p><strong>Abstractive Summarization</strong>: Generating a concise and coherent summary of a document, potentially rephrasing and synthesizing information from the source text.</p> </li> <li> <p><strong>Text-to-Text Tasks</strong>: Treating various NLP tasks as text-to-text problems, where both the input and output are sequences of text. Examples include:</p> </li> <li><strong>Question Answering</strong>: Generating answers to questions based on a provided context or passage.</li> <li> <p><strong>Text Generation with Constraints</strong>: Generating text based on specific constraints or instructions.</p> </li> <li> <p><strong>Image Captioning</strong>: Generating a textual description of an image. The encoder processes features extracted from the image, and the decoder generates a descriptive sentence.</p> </li> <li> <p><strong>Speech-to-Text</strong>: Converting spoken language (speech) into written text. The encoder processes the audio features, while the decoder generates the corresponding text.</p> </li> <li> <p><strong>Text-Based Conversational Systems</strong>:</p> </li> <li> <p><strong>Dialogue Generation</strong>: Generating responses in a conversation where the input may be a previous dialogue context or user query, and the output is a coherent response.</p> </li> <li> <p><strong>Paraphrase Generation</strong>: Rewriting or generating paraphrased versions of a given text while preserving its meaning.</p> </li> <li> <p><strong>Story Generation</strong>: Generating complete stories or narratives based on a prompt or initial context.</p> </li> <li> <p><strong>Semantic Parsing</strong>: Converting natural language into a structured format or query (e.g., converting a sentence into a SQL query).</p> </li> <li> <p><strong>Text Style Transfer</strong>: Transforming the style of a given text while preserving its original meaning (e.g., changing a formal text into an informal one).</p> </li> <li> <p><strong>Multi-Modal Tasks</strong>: Combining multiple types of input (e.g., text and images) to generate output in one modality (e.g., generating text from images or audio).</p> </li> </ol> <p>Encoder-decoder architectures are versatile and powerful for tasks that require generating output sequences based on complex input sequences, making them suitable for a wide range of applications in natural language processing and beyond.</p> <h2 id=why-do-we-need-encoder-decoder-architectures>Why do we need encoder-decoder architectures?<a class=headerlink href=#why-do-we-need-encoder-decoder-architectures title="Permanent link">&para;</a></h2> <p>Both encoder-only and decoder-only architectures have their own strengths and are suited to different types of tasks. The choice between them often depends on the specific requirements of the task and the trade-offs in terms of computational resources, complexity, and performance. Here's a comparison of why you might choose an encoder-decoder architecture over a decoder-only one, and considerations about resource usage:</p> <ol> <li><strong>Handling Complex Input-Output Relationships</strong>:</li> <li><strong>Task Complexity</strong>: Encoder-decoder models excel at tasks where the input and output are significantly different in structure or length, such as machine translation or summarization. The encoder captures the complex relationships in the input sequence, and the decoder generates a well-formed output sequence.</li> <li> <p><strong>Contextual Encoding</strong>: The encoder can effectively represent the entire input sequence in a structured way, allowing the decoder to generate a sequence that reflects the input's context accurately.</p> </li> <li> <p><strong>Improved Performance on Sequence-to-Sequence Tasks</strong>:</p> </li> <li> <p><strong>Attention Mechanism</strong>: The encoder-decoder framework allows for sophisticated attention mechanisms that can focus on different parts of the input sequence while generating the output. This is crucial for tasks where the output needs to reference specific parts of the input.</p> </li> <li> <p><strong>Versatility</strong>:</p> </li> <li> <p><strong>Generalization</strong>: Encoder-decoder models can be adapted for a variety of tasks beyond just sequence generation, including text-to-text transformations and multi-modal tasks (e.g., generating text from images).</p> </li> <li> <p><strong>Decoupling of Representation and Generation</strong>:</p> </li> <li><strong>Modularity</strong>: The separation of encoding and decoding processes allows for specialized models and training procedures. This can be advantageous when tuning models for specific tasks.</li> </ol> <h3 id=resource-considerations>Resource Considerations<a class=headerlink href=#resource-considerations title="Permanent link">&para;</a></h3> <ol> <li><strong>Computational Resources</strong>:</li> <li><strong>Encoder-Decoder Models</strong>: Typically require more resources compared to decoder-only models because they need to handle both encoding and decoding processes. This involves more parameters and more complex computations, particularly for tasks with long input sequences.</li> <li> <p><strong>Decoder-Only Models</strong>: Can be more resource-efficient for tasks that involve generating text based on a fixed context, as they focus solely on the generation process.</p> </li> <li> <p><strong>Training and Inference</strong>:</p> </li> <li><strong>Encoder-Decoder Models</strong>: Training can be more resource-intensive due to the dual structure (encoder and decoder). Inference can also be slower because it involves both encoding the input and generating the output.</li> <li><strong>Decoder-Only Models</strong>: Training might be less complex since there is only one component involved, and inference can be faster for generation tasks due to the lack of a separate encoding step.</li> </ol> <h3 id=summary>Summary<a class=headerlink href=#summary title="Permanent link">&para;</a></h3> <ul> <li><strong>Encoder-Decoder Models</strong>: Best suited for tasks where the input and output sequences are complex and need sophisticated handling. They provide a structured approach to managing different sequence transformations and can handle a wide range of sequence-to-sequence tasks.</li> <li><strong>Decoder-Only Models</strong>: More efficient for tasks focused solely on text generation or autoregressive modeling, where the context is provided, and the focus is on generating a continuation or response.</li> </ul> <p>Choosing between encoder-decoder and decoder-only architectures depends on the specific task requirements and the trade-offs between performance and computational efficiency. For tasks involving intricate input-output relationships, an encoder-decoder model is often preferred despite the higher resource demands. For tasks centered on generating sequences based on a fixed context, a decoder-only model may be more resource-efficient.</p> <h2 id=popular-transformer-architectures>Popular Transformer Architectures<a class=headerlink href=#popular-transformer-architectures title="Permanent link">&para;</a></h2> <p>Transformer architecture has evolved significantly since the original "Attention Is All You Need" paper. Here are some notable variations and advancements:</p> <ol> <li><strong>Original Transformer (Vaswani et al., 2017)</strong>: </li> <li><strong>Structure</strong>: Comprises an encoder-decoder architecture with self-attention and multi-head attention mechanisms.</li> <li> <p><strong>Use Case</strong>: Initially designed for machine translation.</p> </li> <li> <p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>:</p> </li> <li><strong>Structure</strong>: Uses only the encoder part of the Transformer.</li> <li><strong>Training</strong>: Pre-trained using a masked language model (MLM) and next sentence prediction (NSP) objectives.</li> <li> <p><strong>Use Case</strong>: Effective for various NLP tasks like question answering, sentiment analysis, and named entity recognition.</p> </li> <li> <p><strong>GPT (Generative Pre-trained Transformer)</strong>:</p> </li> <li><strong>Structure</strong>: Uses only the decoder part of the Transformer.</li> <li><strong>Training</strong>: Pre-trained using a unidirectional (left-to-right) language model objective.</li> <li><strong>Variants</strong>: GPT-2 and GPT-3, with GPT-4 being the latest, have scaled up the number of parameters and improved performance significantly.</li> <li> <p><strong>Use Case</strong>: Text generation, language translation, and more.</p> </li> <li> <p><strong>T5 (Text-to-Text Transfer Transformer)</strong>:</p> </li> <li><strong>Structure</strong>: Converts all NLP tasks into a text-to-text format.</li> <li><strong>Training</strong>: Pre-trained on a diverse mixture of tasks and fine-tuned for specific tasks.</li> <li> <p><strong>Use Case</strong>: Versatile across different NLP tasks.</p> </li> <li> <p><strong>RoBERTa (A Robustly Optimized BERT Pretraining Approach)</strong>:</p> </li> <li><strong>Structure</strong>: An optimized version of BERT with more training data and longer training times.</li> <li> <p><strong>Use Case</strong>: Improved performance on various NLP benchmarks compared to BERT.</p> </li> <li> <p><strong>ALBERT (A Lite BERT)</strong>:</p> </li> <li><strong>Structure</strong>: Reduces the number of parameters using factorized embedding parameterization and cross-layer parameter sharing.</li> <li> <p><strong>Use Case</strong>: Efficient and lightweight version of BERT for various NLP tasks.</p> </li> <li> <p><strong>DistilBERT</strong>:</p> </li> <li><strong>Structure</strong>: A smaller, faster, and cheaper version of BERT.</li> <li><strong>Training</strong>: Uses knowledge distillation during pre-training.</li> <li> <p><strong>Use Case</strong>: Suitable for environments with limited computational resources.</p> </li> <li> <p><strong>XLNet</strong>:</p> </li> <li><strong>Structure</strong>: Integrates autoregressive and autoencoding approaches.</li> <li><strong>Training</strong>: Uses permutation-based training to capture bidirectional context.</li> <li> <p><strong>Use Case</strong>: Effective for language modeling and various downstream NLP tasks.</p> </li> <li> <p><strong>Transformer-XL</strong>:</p> </li> <li><strong>Structure</strong>: Introduces a segment-level recurrence mechanism.</li> <li><strong>Training</strong>: Handles long-term dependencies better than traditional Transformers.</li> <li> <p><strong>Use Case</strong>: Suitable for tasks requiring long context understanding, like document modeling.</p> </li> <li> <p><strong>Vision Transformer (ViT)</strong>:</p> <ul> <li><strong>Structure</strong>: Applies Transformer architecture to image classification tasks.</li> <li><strong>Training</strong>: Treats image patches as tokens and processes them similarly to text.</li> <li><strong>Use Case</strong>: Effective for various computer vision tasks.</li> </ul> </li> <li> <p><strong>DeBERTa (Decoding-enhanced BERT with disentangled attention)</strong>:</p> <ul> <li><strong>Structure</strong>: Enhances BERT with disentangled attention and improved position embeddings.</li> <li><strong>Use Case</strong>: Achieves state-of-the-art results on various NLP benchmarks.</li> </ul> </li> <li> <p><strong>Swin Transformer</strong>:</p> <ul> <li><strong>Structure</strong>: Applies hierarchical vision Transformer architecture with shifted windows.</li> <li><strong>Training</strong>: Designed for image classification and dense prediction tasks.</li> <li><strong>Use Case</strong>: Effective for object detection, semantic segmentation, and more.</li> </ul> </li> </ol> <p>These variations demonstrate the versatility and adaptability of Transformer architectures across a wide range of applications in both natural language processing and computer vision.</p> <h2 id=why-multi-headed-attention>Why Multi headed attention?<a class=headerlink href=#why-multi-headed-attention title="Permanent link">&para;</a></h2> <p>The multi-head attention mechanism in Transformers enables the model to focus on different aspects of the input data simultaneously. Here are examples of various aspects that attention mechanisms can capture:</p> <h3 id=syntactic-relationships>Syntactic Relationships:<a class=headerlink href=#syntactic-relationships title="Permanent link">&para;</a></h3> <p>Example: In the sentence "The cat sat on the mat," different heads might focus on different parts of speech relationships, such as subject-verb ("cat" and "sat") and preposition-object ("on" and "mat").</p> <h3 id=semantic-relationships>Semantic Relationships:<a class=headerlink href=#semantic-relationships title="Permanent link">&para;</a></h3> <p>Example: In the sentence "He played the piano beautifully," one head might focus on the verb "played" and its direct object "piano," while another head focuses on the adverb "beautifully" modifying "played."</p> <h3 id=coreference-resolution>Coreference Resolution:<a class=headerlink href=#coreference-resolution title="Permanent link">&para;</a></h3> <p>Example: In the text "Alice went to the market. She bought apples," one head might track the coreference between "Alice" and "She."</p> <h3 id=long-range-dependencies>Long-Range Dependencies:<a class=headerlink href=#long-range-dependencies title="Permanent link">&para;</a></h3> <p>Example: In the sentence "The book that you recommended to me last week was fascinating," one head might focus on the relationship between "book" and "fascinating," which are far apart in the sentence.</p> <h3 id=positional-information>Positional Information:<a class=headerlink href=#positional-information title="Permanent link">&para;</a></h3> <p>Example: Different heads can capture relative positional information, such as the beginning, middle, and end of a sentence, which is crucial for understanding the structure.</p> <h3 id=named-entity-recognition>Named Entity Recognition:<a class=headerlink href=#named-entity-recognition title="Permanent link">&para;</a></h3> <p>Example: In the sentence "Barack Obama was born in Hawaii," one head might focus on identifying "Barack Obama" as a person.</p> <h3 id=polarity-and-sentiment>Polarity and Sentiment:<a class=headerlink href=#polarity-and-sentiment title="Permanent link">&para;</a></h3> <ul> <li>Could capture positive or negative sentiment associated with different parts of the text.</li> <li>May focus on identifying subjective vs. objective statements. Ho</li> </ul> <h2 id=how-attention-works>How attention works?<a class=headerlink href=#how-attention-works title="Permanent link">&para;</a></h2> <p>The input embedding is linearly projected into three different spaces to generate queries (𝑄), keys (𝐾), and values (𝑉).</p> <div class=arithmatex>\[{Attention}(Q, K, V) = {softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\]</div> <p>Where: - Q is the query matrix. - K is the key matrix. - V is the value matrix. - <span class=arithmatex>\(d_k\)</span> is the dimension of the keys.</p> <h2 id=how-multihead-attention-works>How multihead attention works.<a class=headerlink href=#how-multihead-attention-works title="Permanent link">&para;</a></h2> <ul> <li>Base model has 512 dim embedding vector, large model has 1024 dim embedding vector.</li> <li>Position vector of the same size is used.</li> <li>Both vectors are pair wise added.</li> <li>Base model has 8 heads and large model has 16 heads. Thus each head has 512/8 or 1024/16 i.e. 64 dim vector.</li> </ul> <h2 id=how-q-k-v-calculated>How Q, K, V Calculated?<a class=headerlink href=#how-q-k-v-calculated title="Permanent link">&para;</a></h2> <ol> <li> <p><strong>Input Embedding Dimension for Base Model</strong>: 512</p> </li> <li> <p><strong>Number of Heads</strong>: 8</p> </li> <li> <p><strong>Dimension per Head</strong>: Each head will handle <span class=arithmatex>\(<span class=arithmatex>\(\frac{512}{8} = 64\)</span>\)</span> dimensions.</p> </li> </ol> <h3 id=steps-for-multi-head-attention>Steps for Multi-Head Attention:<a class=headerlink href=#steps-for-multi-head-attention title="Permanent link">&para;</a></h3> <ol> <li><strong>Linear Projections</strong>:</li> <li>The input embedding (of dimension 512) is linearly projected into three different spaces to generate queries (<span class=arithmatex>\(<span class=arithmatex>\(Q\)</span>\)</span>), keys (<span class=arithmatex>\(K\)</span>), and values (<span class=arithmatex>\(V\)</span>).</li> <li>Each projection is typically done using separate learned weight matrices.</li> <li>These projections result in three vectors: <span class=arithmatex>\(Q\)</span>, <span class=arithmatex>\(K\)</span>, and <span class=arithmatex>\(V\)</span>, each of dimension 512.</li> <li> <p>Example: <strong>Linear Projections</strong>:</p> <ul> <li>For the input <span class=arithmatex>\(X\)</span> of shape $<span class=arithmatex>\([ batch_size, sequence_length, 512 ]\)</span>:</li> <li><span class=arithmatex>\(Q = XW_Q\)</span>, where <span class=arithmatex>\(W_Q\)</span> is a weight matrix of shape <span class=arithmatex>\([512, 512]\)</span>.</li> <li><span class=arithmatex>\(K = XW_K\)</span>, where <span class=arithmatex>\(W_K\)</span> is a weight matrix of shape <span class=arithmatex>\([512, 512]\)</span>.</li> <li><span class=arithmatex>\(V = XW_V\)</span>, where <span class=arithmatex>\(W_V\)</span> is a weight matrix of shape <span class=arithmatex>\([512, 512]\)</span>.</li> </ul> </li> <li> <p><strong>Splitting into Heads</strong>:</p> </li> <li>After the projection, the <span class=arithmatex>\(<span class=arithmatex>\(Q\)</span>\)</span>, <span class=arithmatex>\(<span class=arithmatex>\(K\)</span>\)</span>, and <span class=arithmatex>\(<span class=arithmatex>\(V\)</span>\)</span> vectors are split into 8 parts (heads).</li> <li>Each part will have $$ \frac{512}{8} = 64 $$ dimensions.</li> <li>This means each head gets a 64-dimensional sub-vector from <span class=arithmatex>\(<span class=arithmatex>\(Q\)</span>\)</span>, <span class=arithmatex>\(<span class=arithmatex>\(K\)</span>\)</span>, and <span class=arithmatex>\(<span class=arithmatex>\(V\)</span>\)</span>.</li> <li>Example: <strong>Splitting into Heads</strong>:<ul> <li>Each of the <span class=arithmatex>\(<span class=arithmatex>\(Q\)</span>\)</span>, <span class=arithmatex>\(<span class=arithmatex>\(K\)</span>\)</span>, and <span class=arithmatex>\(<span class=arithmatex>\(V\)</span>\)</span> matrices (of shape $$ {batch_size}, {sequence_length}, 512 $$ is reshaped and split into 8 heads.</li> <li>For each matrix, this reshaping results in shape $$ batch_size, sequence_length, 8, 64 $$.</li> <li>The 8 heads mean we now have 8 sets of <span class=arithmatex>\(<span class=arithmatex>\(Q\)</span>\)</span>, <span class=arithmatex>\(<span class=arithmatex>\(K\)</span>\)</span>, and <span class=arithmatex>\(<span class=arithmatex>\(V\)</span>\)</span> vectors, each of dimension 64.</li> </ul> </li> </ol> <h3 id=scaled-dot-product-attention-per-head>Scaled Dot-Product Attention (Per Head):<a class=headerlink href=#scaled-dot-product-attention-per-head title="Permanent link">&para;</a></h3> <p>Each of the 8 heads performs scaled dot-product attention independently: $$ {Attention}(Q_i, K_i, V_i) = {softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i $$ where $$ d_k = 64 $$ is the dimension of each head.</p> <h3 id=concatenation-and-final-linear-layer>Concatenation and Final Linear Layer:<a class=headerlink href=#concatenation-and-final-linear-layer title="Permanent link">&para;</a></h3> <ol> <li>The outputs from all 8 heads are concatenated:</li> <li> <p>Concatenated output shape: <span class=arithmatex>\(<span class=arithmatex>\(batch\_size, sequence\_length, 8 \times 64 = batch\_size, sequence\_length, 512\)</span>\)</span> .</p> </li> <li> <p>This concatenated vector is then passed through a final linear layer (with weight matrix of shape <span class=arithmatex>\(<span class=arithmatex>\([512, 512]\)</span>\)</span>) to produce the final output of the multi-head attention mechanism.</p> </li> </ol> <p>This process ensures that each head independently attends to different parts of the input, capturing diverse aspects of the data.</p> <h2 id=floating-point-value-range>Floating Point Value Range<a class=headerlink href=#floating-point-value-range title="Permanent link">&para;</a></h2> <p>These embedding vectors holds floating point numbers. These numbers may be 64bit, 32bit, 16bit, 8bit, 4bit. What will be the range of value if we use these different bit size to hold floating number. This is useful when you are doing quantization.</p> <p>Embedding vectors in many deep learning frameworks typically use IEEE 754 double-precision floating-point format (64-bit) for representing values, but there are also other precision formats used depending on the specific requirements and hardware capabilities.</p> <p>Apart from number of parameters these floating point precision also make model bulky. To reduce the model size we reduce these floating point precision. This process is called quantization. For edge devices or making model run on low graded machine we can choose quantization of the model to 4bit or 8bit floating points, off course we suffer the quality of output due to this.</p> <h3 id=common-precision-formats-for-embeddings>Common Precision Formats for Embeddings<a class=headerlink href=#common-precision-formats-for-embeddings title="Permanent link">&para;</a></h3> <ol> <li><strong>Double Precision (64-bit)</strong>:</li> <li><strong>Format</strong>: IEEE 754 double-precision floating-point.</li> <li><strong>Precision</strong>: Provides about 15-17 significant decimal digits of precision.</li> <li><strong>Range</strong>: Can represent values from approximately $$ \pm 5 \times 10^{-324} $$ to $$ \pm 1.79 \times 10^{308} $$.</li> <li> <p><strong>Use Case</strong>: Often used when high precision is required, but it's less common in practice for embeddings due to the increased computational and memory overhead.</p> </li> <li> <p><strong>Single Precision (32-bit)</strong>:</p> </li> <li><strong>Format</strong>: IEEE 754 single-precision floating-point.</li> <li><strong>Precision</strong>: Provides about 6-9 significant decimal digits of precision.</li> <li><strong>Range</strong>: Can represent values from approximately $$ \pm 1.18 \times 10^{-38} $$ to $$ \pm 3.4 \times 10^{38} $$.</li> <li> <p><strong>Use Case</strong>: More common for embeddings due to a good balance between precision and computational efficiency.</p> </li> <li> <p><strong>Half Precision (16-bit)</strong>:</p> </li> <li><strong>Format</strong>: IEEE 754 half-precision floating-point.</li> <li><strong>Precision</strong>: Provides about 3-4 significant decimal digits of precision.</li> <li><strong>Range</strong>: Can represent values from approximately $$ \pm 6.1 \times 10^{-5} $$ to $$ \pm 6.5 \times 10^{4} $$.</li> <li> <p><strong>Use Case</strong>: Used to reduce memory usage and increase computational efficiency, especially during training with GPUs that support mixed precision.</p> </li> <li> <p><strong>BFloat16 (16-bit)</strong>:</p> </li> <li><strong>Format</strong>: A variant of 16-bit floating-point with a different exponent and mantissa configuration, designed to be more efficient for certain computations.</li> <li><strong>Precision</strong>: Similar to half precision but with a larger exponent range.</li> <li><strong>Use Case</strong>: Used in some TensorFlow models and other frameworks to optimize performance while maintaining acceptable precision.</li> </ol> <h3 id=in-summary>In Summary<a class=headerlink href=#in-summary title="Permanent link">&para;</a></h3> <ul> <li><strong>IEEE 754 Double Precision</strong> is used when high precision is crucial, but it is less common for embeddings due to the larger memory footprint and computation requirements.</li> <li><strong>IEEE 754 Single Precision</strong> is the most common format for embeddings in many deep learning applications due to its efficiency and sufficient precision.</li> <li><strong>Half Precision</strong> and <strong>BFloat16</strong> are used for further optimization, particularly in training scenarios where memory and computational efficiency are critical.</li> </ul> <p>The choice of precision format depends on the trade-offs between precision, memory usage, and computational efficiency.</p> <h2 id=illustration-of-working-of-transformer>Illustration of Working of Transformer<a class=headerlink href=#illustration-of-working-of-transformer title="Permanent link">&para;</a></h2> <p>For the sake of simplicity, let's walk through an example of how a ChatGPT-like architecture generates text, using an 8-dimensional word embedding, a vocabulary size of 100, and a multi-headed attention mechanism with 2 heads and 3 layers. We will also perform the computations for the self-attention using the Query (Q), Key (K), and Value (V) matrices.</p> <h3 id=step-1-tokenization-and-embedding>Step 1: Tokenization and Embedding<a class=headerlink href=#step-1-tokenization-and-embedding title="Permanent link">&para;</a></h3> <p><strong>Vocabulary</strong>: 100 unique tokens.</p> <p><strong>Input Sentence</strong>: "Hello world"</p> <p><strong>Token IDs</strong>: - "Hello" might be token ID 42. - "world" might be token ID 85.</p> <p><strong>Embedding</strong>: Each token ID is mapped to an 8-dimensional vector. - Embedding for "Hello" (token ID 42): [0.1, -0.2, 0.3, 0.4, -0.5, 0.2, -0.1, 0.0] - Embedding for "world" (token ID 85): [-0.3, 0.1, 0.2, -0.4, 0.5, -0.2, 0.3, -0.1]</p> <p>We also need to compute position embedding. Refer <a href=#position-embedding-mechanism>Position Embedding Mechanism</a> </p> <h3 id=step-2-input-to-the-transformer-model>Step 2: Input to the Transformer Model<a class=headerlink href=#step-2-input-to-the-transformer-model title="Permanent link">&para;</a></h3> <p>These embeddings are passed as input to the model.</p> <p><strong>Input Matrix</strong>:</p> <div class=arithmatex>\[ \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \\ -0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \\ \end{bmatrix} \]</div> <h3 id=step-3-self-attention>Step 3: <a href=#self-attention-mechanism>Self Attention</a><a class=headerlink href=#step-3-self-attention title="Permanent link">&para;</a></h3> <ul> <li><strong>Multi-Head Self-Attention</strong></li> <li><strong>Combining Multi-Head Attention</strong></li> </ul> <p>For Head 2, similar computations will be performed to obtain $$ Q_2 $$, $$ K_2 $$, $$ V_2 $$, and the attention output. The outputs from both heads will be concatenated and then projected back into the original embedding dimension using a weight matrix $$ W^O $$.</p> <h3 id=step-4-processing-through-transformer-layers>Step 4. <strong>Processing Through Transformer Layers</strong><a class=headerlink href=#step-4-processing-through-transformer-layers title="Permanent link">&para;</a></h3> <p>The output of the multi-head attention layer will then pass through the feed-forward neural network (FFNN) and normalization layers, completing the processing for one layer of the transformer. This process is repeated for the remaining layers, in our example 3 layers.</p> <ul> <li><strong>Feedforward Neural Networks</strong></li> <li><strong>Layer Normalization</strong></li> <li><strong>Residual Connections</strong></li> </ul> <p>Each layer refines the embeddings based on the input context.</p> <h3 id=step-5-output-logits>Step 5. <strong>Output Logits</strong><a class=headerlink href=#step-5-output-logits title="Permanent link">&para;</a></h3> <p>After processing through the Transformer layers, we get output vectors (logits) for each token position.</p> <p><strong>Output Logits for the Next Token</strong>: Let's assume our logits for the next token are a 100-dimensional vector (one value per token in the vocabulary). For simplicity:</p> <div class=arithmatex>\[ \begin{bmatrix} 1.5, &amp; -0.3, &amp; \dots, &amp; 0.8 \\ \end{bmatrix} \]</div> <h3 id=step-6-softmax-function>Step 6. <strong>Softmax Function</strong><a class=headerlink href=#step-6-softmax-function title="Permanent link">&para;</a></h3> <p>The logits are converted to probabilities using the softmax function.</p> <p><strong>Softmax Output</strong>:</p> <div class=arithmatex>\[ \begin{bmatrix} 0.1, &amp; 0.05, &amp; \dots, &amp; 0.15 \\ \end{bmatrix} \]</div> <h3 id=step-7-token-selection>Step 7. <strong>Token Selection</strong><a class=headerlink href=#step-7-token-selection title="Permanent link">&para;</a></h3> <p>A token is selected based on the probabilities. Using sampling, top-k, or greedy decoding: - Let's say token ID 75 is selected, corresponding to the token "everyone".</p> <h3 id=step-8-generating-the-next-token>Step 8. <strong>Generating the Next Token</strong><a class=headerlink href=#step-8-generating-the-next-token title="Permanent link">&para;</a></h3> <p>The process repeats. The input now includes the previous tokens "Hello world" and the new token "everyone". The model generates the next token based on this updated context.</p> <h3 id=generating-text>Generating Text<a class=headerlink href=#generating-text title="Permanent link">&para;</a></h3> <p>To generate text, the model uses these layers iteratively, predicting the next token in the sequence based on the previously generated tokens and the context provided by the input sequence. The output is passed through a final linear layer and softmax to produce probabilities for the next token, from which the next token is sampled or chosen.</p> <p>By repeating this process, the model generates text token by token until a specified end condition is met.</p> <h2 id=what-different-parameters-are-learned-during-transformer-training>What different parameters are learned during transformer training?<a class=headerlink href=#what-different-parameters-are-learned-during-transformer-training title="Permanent link">&para;</a></h2> <p>Transformer models like GPT3, GPT3.5, GPT4.0, Gemma, PaLM, Llama etc has billions of parameters. What are these parameters which are learned during training?</p> <p>In large language models like ChatGPT, weights and biases are integral to the model's operation, especially within the transformer architecture. Here’s a detailed breakdown of different weights and biases used in such models, including those related to attention mechanisms:</p> <h3 id=1-weights-and-biases-in-transformer-architecture>1. <strong>Weights and Biases in Transformer Architecture</strong><a class=headerlink href=#1-weights-and-biases-in-transformer-architecture title="Permanent link">&para;</a></h3> <h4 id=attention-mechanism-weights><strong>Attention Mechanism Weights</strong><a class=headerlink href=#attention-mechanism-weights title="Permanent link">&para;</a></h4> <ul> <li> <p><strong>Query Weights (W_q)</strong>: These weights transform the input embeddings or hidden states into query vectors. In the attention mechanism, the query vector is compared against keys to compute attention scores.</p> </li> <li> <p><strong>Key Weights (W_k)</strong>: These weights transform the input embeddings or hidden states into key vectors. The attention scores are computed by comparing these keys with the query vectors.</p> </li> <li> <p><strong>Value Weights (W_v)</strong>: These weights transform the input embeddings or hidden states into value vectors. The output of the attention mechanism is a weighted sum of these value vectors, based on the attention scores.</p> </li> <li> <p><strong>Output Weights (W_o)</strong>: After applying the attention mechanism, the output vectors are transformed by these weights before being passed to subsequent layers.</p> </li> </ul> <h4 id=feed-forward-network-weights><strong>Feed-Forward Network Weights</strong><a class=headerlink href=#feed-forward-network-weights title="Permanent link">&para;</a></h4> <ul> <li><strong>Weights (W_ff)</strong>: The feed-forward network within each transformer block has its own set of weights for transforming the hidden states. This usually includes two sets of weights:</li> <li> <p><strong>Weight Matrices for Linear Transformations</strong>: These weights perform linear transformations in the feed-forward network, often involving two layers with an activation function (e.g., ReLU) in between.</p> </li> <li> <p><strong>Biases (b_ff)</strong>: Biases are used along with the weights in the feed-forward network to adjust the activation values.</p> </li> </ul> <h4 id=layer-normalization-weights><strong>Layer Normalization Weights</strong><a class=headerlink href=#layer-normalization-weights title="Permanent link">&para;</a></h4> <ul> <li><strong>Gamma (γ)</strong>: Scaling parameter used in layer normalization to adjust the normalized output.</li> <li><strong>Beta (β)</strong>: Shifting parameter used in layer normalization to adjust the mean of the normalized output.</li> </ul> <h3 id=2-overall-model-weights-and-biases>2. <strong>Overall Model Weights and Biases</strong><a class=headerlink href=#2-overall-model-weights-and-biases title="Permanent link">&para;</a></h3> <h4 id=embedding-weights><strong>Embedding Weights</strong><a class=headerlink href=#embedding-weights title="Permanent link">&para;</a></h4> <ul> <li> <p><strong>Token Embedding Weights</strong>: These weights map input tokens (words or subwords) to continuous vector representations (embeddings).</p> </li> <li> <p><strong>Position Embedding Weights</strong>: These weights add positional information to the embeddings to encode the order of tokens in the sequence.</p> </li> </ul> <h4 id=layer-weights><strong>Layer Weights</strong><a class=headerlink href=#layer-weights title="Permanent link">&para;</a></h4> <ul> <li> <p><strong>Weights in Each Transformer Layer</strong>: Each layer of the transformer model has its own set of weights and biases for both the attention mechanism and the feed-forward network.</p> </li> <li> <p><strong>Residual Connection Weights</strong>: Residual connections (or skip connections) within each transformer layer often involve weights for combining the input and output of the layer.</p> </li> </ul> <h3 id=summary_1>Summary<a class=headerlink href=#summary_1 title="Permanent link">&para;</a></h3> <p>In summary, the different weights and biases in a model like ChatGPT are:</p> <ul> <li><strong>Attention Mechanism</strong>:</li> <li>Query Weights (W_q)</li> <li>Key Weights (W_k)</li> <li>Value Weights (W_v)</li> <li> <p>Output Weights (W_o)</p> </li> <li> <p><strong>Feed-Forward Network</strong>:</p> </li> <li>Weights (W_ff)</li> <li> <p>Biases (b_ff)</p> </li> <li> <p><strong>Layer Normalization</strong>:</p> </li> <li>Gamma (γ)</li> <li> <p>Beta (β)</p> </li> <li> <p><strong>Embedding Weights</strong>:</p> </li> <li>Token Embedding Weights</li> <li>Position Embedding Weights</li> </ul> <p>These weights and biases are learned during the training phase and are used during inference to generate responses based on the input data. Each component of the model contributes to its ability to understand and generate human-like text.</p> <h2 id=self-attention-mechanism>Self Attention Mechanism<a class=headerlink href=#self-attention-mechanism title="Permanent link">&para;</a></h2> <h3 id=compute-q-k-v-matrices-for-multi-head-attention-2-heads>Compute Q, K, V Matrices for Multi-Head Attention (2 heads)<a class=headerlink href=#compute-q-k-v-matrices-for-multi-head-attention-2-heads title="Permanent link">&para;</a></h3> <p>We'll compute Q, K, and V matrices for each head.</p> <p><strong>Weight Matrices for Q, K, V for Head 1</strong>:</p> <div class=arithmatex>\[ W^Q_1, W^K_1, W^V_1 \in \mathbb{R}^{8 \times 4}\]</div> <p><strong>Weight Matrices for Q, K, V for Head 2</strong>:</p> <div class=arithmatex>\[W^Q_2, W^K_2, W^V_2 \in \mathbb{R}^{8 \times 4} \]</div> <p>For simplicity, let's use random matrices. In practice, these are learned during training.</p> <p><strong>Embeddings</strong>:</p> <div class=arithmatex>\[ X = \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \\ -0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \\ \end{bmatrix} \]</div> <p><strong>Weight Matrices</strong> (randomly initialized for this example):</p> <p>Head 1:</p> <p>$$ W^Q_1 = \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ \end{bmatrix}</p> <p>W^K_1 = \begin{bmatrix} 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ \end{bmatrix}</p> <p>W^V_1 = \begin{bmatrix} 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ \end{bmatrix} $$</p> <p>Head 2:</p> <p>$$ W^Q_2 = \begin{bmatrix} 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \ 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \ \end{bmatrix}</p> <p>W^K_2 = \begin{bmatrix} 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ \end{bmatrix}</p> <p>W^V_2 = \begin{bmatrix} 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \ 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \ 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \ 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \ 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \ 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \ 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \ 0.2 &amp; 0.1 &amp; 0.3 &amp; 0.4 \ \end{bmatrix} $$</p> <p><strong>Compute Q, K, V for each token for each head</strong>:</p> <p><strong>Head 1</strong>:</p> <p>$$ Q_1 = X \cdot W^Q_1 = \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \ -0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \ \end{bmatrix} \cdot W^Q_1</p> <p>K_1 = X \cdot W^K_1</p> <p>V_1 = X \cdot W^V_1 $$</p> <p><strong>Head 2</strong>:</p> <p>$$ Q_2 = X \cdot W^Q_2</p> <p>K_2 = X \cdot W^K_2</p> <p>V_2 = X \cdot W^V_2 $$</p> <p>Let's compute these step by step.</p> <h3 id=compute-q-k-v-for-head-1>Compute Q, K, V for Head 1<a class=headerlink href=#compute-q-k-v-for-head-1 title="Permanent link">&para;</a></h3> <p><strong>Input Embeddings</strong>:</p> <div class=arithmatex>\[ X = \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \\ -0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \\ \end{bmatrix} \]</div> <p><strong>Weight Matrices for Head 1</strong>:</p> <p>$$ W^Q_1 = \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \ \end{bmatrix}</p> <p>W^K_1 = \begin{bmatrix} 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ 0.2 &amp; 0.1 &amp; 0.4 &amp; 0.3 \ \end{bmatrix}</p> <p>W^V_1 = \begin{bmatrix} 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ 0.3 &amp; 0.4 &amp; 0.1 &amp; 0.2 \ \end{bmatrix} $$</p> <h4 id=compute-q-k-v-for-each-token-for-head-1>Compute Q, K, V for each token for Head 1:<a class=headerlink href=#compute-q-k-v-for-each-token-for-head-1 title="Permanent link">&para;</a></h4> <div class=arithmatex>\[ Q_1 = X \cdot W^Q_1 = \begin{bmatrix} 0.1 &amp; -0.2 &amp; 0.3 &amp; 0.4 &amp; -0.5 &amp; 0.2 &amp; -0.1 &amp; 0.0 \\ -0.3 &amp; 0.1 &amp; 0.2 &amp; -0.4 &amp; 0.5 &amp; -0.2 &amp; 0.3 &amp; -0.1 \\ \end{bmatrix} \cdot \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ 0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 \\ \end{bmatrix} \]</div> <p>Performing the matrix multiplication:</p> <div class=arithmatex>\[ Q_1 = \begin{bmatrix} (0.1 \times 0.1) + (-0.2 \times 0.1) + (0.3 \times 0.1) + (0.4 \times 0.1) + (-0.5 \times 0.1) + (0.2 \times 0.1) + (-0.1 \times 0.1) + (0 \times 0.1) &amp; \dots &amp; \\ (-0.3 \times 0.1) + (0.1 \times 0.1) + (0.2 \times 0.1) + (-0.4 \times 0.1) + (0.5 \times 0.1) + (-0.2 \times 0.1) + (0.3 \times 0.1) + (-0.1 \times 0.1) &amp; \dots &amp; \\ \end{bmatrix} \]</div> <p>Simplifying:</p> <div class=arithmatex>\[ Q_1 = \begin{bmatrix} -0.01 &amp; -0.02 &amp; -0.03 &amp; -0.04 \\ 0.02 &amp; 0.04 &amp; 0.06 &amp; 0.08 \\ \end{bmatrix} \]</div> <p>Following the same steps for $$ K_1 $$ and $$ V_1 $$:</p> <p>$$ K_1 = X \cdot W^K_1</p> <p>= \begin{bmatrix} 0.15 &amp; 0.30 &amp; 0.45 &amp; 0.60 \ 0.10 &amp; 0.20 &amp; 0.30 &amp; 0.40 \ \end{bmatrix}</p> <p>V_1 = X \cdot W^V_1</p> <p>= \begin{bmatrix} 0.28 &amp; 0.56 &amp; 0.84 &amp; 1.12 \ 0.24 &amp; 0.48 &amp; 0.72 &amp; 0.96 \ \end{bmatrix} $$</p> <p><strong>Similarly you compute for head 2. Finally you concatenate both vectors and get 2x8 size matrix (same size which was input for the self attention)</strong></p> <h3 id=step-3-self-attention-calculation>Step 3: Self-Attention Calculation<a class=headerlink href=#step-3-self-attention-calculation title="Permanent link">&para;</a></h3> <h4 id=compute-attention-scores>Compute attention scores:<a class=headerlink href=#compute-attention-scores title="Permanent link">&para;</a></h4> <div class=arithmatex>\[ \text{Scores} = Q_1 \cdot K_1^T = \begin{bmatrix} -0.01 &amp; -0.02 &amp; -0.03 &amp; -0.04 \\ 0.02 &amp; 0.04 &amp; 0.06 &amp; 0.08 \\ \end{bmatrix} \cdot \begin{bmatrix} 0.15 &amp; 0.10 \\ 0.30 &amp; 0.20 \\ 0.45 &amp; 0.30 \\ 0.60 &amp; 0.40 \\ \end{bmatrix} \]</div> <p>Performing the matrix multiplication:</p> <div class=arithmatex>\[ \text{Scores} = \begin{bmatrix} -0.01 \times 0.15 + -0.02 \times 0.30 + -0.03 \times 0.45 + -0.04 \times 0.60 &amp; -0.01 \times 0.10 + -0.02 \times 0.20 + -0.03 \times 0.30 + -0.04 \times 0.40 \\ 0.02 \times 0.15 + 0.04 \times 0.30 + 0.06 \times 0.45 + 0.08 \times 0.60 &amp; 0.02 \times 0.10 + 0.04 \times 0.20 + 0.06 \times 0.30 + 0.08 \times 0.40 \\ \end{bmatrix} \]</div> <p>Simplifying:</p> <p>$$ \text{Scores} = \begin{bmatrix} -0.015 - 0.006 - 0.0135 - 0.024 &amp; -0.01 - 0.004 - 0.009 - 0.016 \ 0.03 + 0.012 + 0.027 + 0.048 &amp; 0.02 + 0.008 + 0.018 + 0.032 \ \end{bmatrix}</p> <p>\text{Scores} = \begin{bmatrix} -0.0585 &amp; -0.039 \ 0.117 &amp; 0.078 \ \end{bmatrix} $$</p> <h4 id=apply-softmax-to-obtain-attention-weights>Apply softmax to obtain attention weights:<a class=headerlink href=#apply-softmax-to-obtain-attention-weights title="Permanent link">&para;</a></h4> <p>$$ \text{Attention Weights} = \text{softmax}(\text{Scores})</p> <p>\text{Attention Weights} = \begin{bmatrix} \frac{e<sup -0.0585=-0.0585>{-0.0585}}{e</sup> &amp; \frac{e} + e^{-0.039}<sup -0.0585=-0.0585>{-0.039}}{e</sup> \ \frac{e} + e^{-0.039}<sup 0.117=0.117>{0.117}}{e</sup> &amp; \frac{e} + e^{0.078}<sup 0.117=0.117>{0.078}}{e</sup> \ \end{bmatrix} $$} + e^{0.078}</p> <p>Simplifying:</p> <div class=arithmatex>\[ \text{Attention Weights} = \begin{bmatrix} \frac{1}{1 + e^{0.0195}} &amp; \frac{e^{0.0195}}{1 + e^{0.0195}} \\ \frac{1}{1 + e^{-0.039}} &amp; \frac{e^{-0.039}}{1 + e^{-0.039}} \\ \end{bmatrix} \]</div> <p>Approximating the values:</p> <div class=arithmatex>\[ \text{Attention Weights} = \begin{bmatrix} 0.495 &amp; 0.505 \\ 0.510 &amp; 0.490 \\ \end{bmatrix} \]</div> <h4 id=compute-the-weighted-sum-of-the-values>Compute the weighted sum of the values:<a class=headerlink href=#compute-the-weighted-sum-of-the-values title="Permanent link">&para;</a></h4> <div class=arithmatex>\[ \text{Output} = \text{Attention Weights} \cdot V_1 = \begin{bmatrix} 0.495 &amp; 0.505 \\ 0.510 &amp; 0.490 \\ \end{bmatrix} \cdot \begin{bmatrix} 0.28 &amp; 0.56 &amp; 0.84 &amp; 1.12 \\ 0.24 &amp; 0.48 &amp; 0.72 &amp; 0.96 \\ \end{bmatrix} \]</div> <p>Performing the matrix multiplication:</p> <div class=arithmatex>\[ \text{Output} = \begin{bmatrix} 0.495 \times 0.28 + 0.505 \times 0.24 &amp; 0.495 \times 0.56 + 0.505 \times 0.48 &amp; 0.495 \times 0.84 + 0.505 \times 0.72 &amp; 0.495 \times 1.12 + 0.505 \times 0.96 \\ 0.510 \times 0.28 + 0.490 \times 0.24 &amp; 0.510 \times 0.56 + 0.490 \times 0.48 &amp; 0.510 \times 0.84 + 0.490 \times 0.72 &amp; 0.510 \times 1.12 + 0.490 \times 0.96 \\ \end{bmatrix} \]</div> <p>Simplifying:</p> <p>$$ \text{Output} = \begin{bmatrix} 0.1386 + 0.1212 &amp; 0.2772 + 0.2424 &amp; 0.4158 + 0.3636 &amp; 0.5544 + 0.4848 \ 0.1428 + 0.1176 &amp; 0.2856 + 0.2304 &amp; 0.4284 + 0.3432 &amp; 0.5712 + 0.456 \ \end{bmatrix}</p> <p>\text{Output} = \begin{bmatrix} 0.2598 &amp; 0.5196 &amp; 0.7794 &amp; 1.0392 \ 0.2604 &amp; 0.516 &amp; 0.7716 &amp; 1.0272 \ \end{bmatrix} $$</p> <h2 id=position-embedding-mechanism>Position Embedding Mechanism<a class=headerlink href=#position-embedding-mechanism title="Permanent link">&para;</a></h2> <p>Position embeddings are used in transformers to provide information about the order of tokens in a sequence. The most common method for computing position embeddings is using sine and cosine functions of different frequencies. This method was introduced in the original transformer paper "Attention Is All You Need". The formulas for the position embeddings are as follows:</p> <h3 id=formulas-for-position-embedding>Formulas for Position Embedding<a class=headerlink href=#formulas-for-position-embedding title="Permanent link">&para;</a></h3> <p>For a given position $$ pos $$ and embedding dimension $$ i $$:</p> <ol> <li><strong>Sine Function for Even Indices:</strong></li> </ol> <div class=arithmatex>\[ PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \]</div> <ol> <li><strong>Cosine Function for Odd Indices:</strong></li> </ol> <div class=arithmatex>\[ PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \]</div> <p>Where: - pos is the position of the token in the sequence (starting from 0). - i is the dimension index (also starting from 0). - <span class=arithmatex>\(<span class=arithmatex>\(d_{\text{model}}\)</span>\)</span> is the dimensionality of the embeddings.</p> <h3 id=explanation>Explanation<a class=headerlink href=#explanation title="Permanent link">&para;</a></h3> <ul> <li><strong>Even Index:</strong> For even values of i, the position embedding is computed using the sine function.</li> <li><strong>Odd Index:</strong> For odd values of i, the position embedding is computed using the cosine function.</li> <li><strong>Frequency:</strong> The denominator <span class=arithmatex>\(<span class=arithmatex>\(10000^{\frac{2i}{d_{\text{model}}}}\)</span>\)</span> ensures that different dimensions have different frequencies. The values for sine and cosine vary more slowly for larger dimensions, capturing different levels of granularity.</li> </ul> <h3 id=example>Example<a class=headerlink href=#example title="Permanent link">&para;</a></h3> <p>Let's assume $$ d_{\text{model}} = 8 $$ (for simplicity), and calculate the position embeddings for $$ pos = 1 $$.</p> <p>For $$ i = 0 $$: $$ PE(1, 0) = \sin\left(\frac{1}{10000^{\frac{0}{8}}}\right) = \sin\left(1\right) $$</p> <p>For $$ i = 1 $$: $$ PE(1, 1) = \cos\left(\frac{1}{10000^{\frac{0}{8}}}\right) = \cos\left(1\right) $$</p> <p>For $$ i = 2 $$: $$ PE(1, 2) = \sin\left(\frac{1}{10000^{\frac{2}{8}}}\right) = \sin\left(\frac{1}{10}\right) $$</p> <p>For $$ i = 3 $$: $$ PE(1, 3) = \cos\left(\frac{1}{10000^{\frac{2}{8}}}\right) = \cos\left(\frac{1}{10}\right) $$</p> <p>For $$ i = 4 $$: $$ PE(1, 4) = \sin\left(\frac{1}{10000^{\frac{4}{8}}}\right) = \sin\left(\frac{1}{100}\right) $$</p> <p>For $$ i = 5 $$: $$ PE(1, 5) = \cos\left(\frac{1}{10000^{\frac{4}{8}}}\right) = \cos\left(\frac{1}{100}\right) $$</p> <p>For $$ i = 6 $$: $$ PE(1, 6) = \sin\left(\frac{1}{10000^{\frac{6}{8}}}\right) = \sin\left(\frac{1}{1000}\right) $$</p> <p>For $$ i = 7 $$: $$ PE(1, 7) = \cos\left(\frac{1}{10000^{\frac{6}{8}}}\right) = \cos\left(\frac{1}{1000}\right) $$</p> <p>These values are then added to the corresponding token embeddings to provide the model with information about the position of each token in the sequence.</p> <p><strong>Author</strong> <br> Dr Hari Thapliyaal <br> dasarpai.com <br> linkedin.com/in/harithapliyal </p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>