<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="My various blogs on Datascience, Management, Software Engineering, Philosophy, Vedanta, Sanskrit, Current Affair, History. "><meta name=author content="Hari Thapliyaal"><link rel=canonical href=https://dasarpai.github.io/dasarpai-mkdocs/dsblog/Types-of-Machine-Learning.html><link rel=icon href=../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.11"><title>Types of Machine Learning - DasarpAI</title><link rel=stylesheet href=../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../stylesheets/extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","{{ config.extra.GOOGLE_ANALYTICS }}"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","{{ config.extra.GOOGLE_ANALYTICS }}",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id={{ config.extra.GOOGLE_ANALYTICS }}",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link rel=stylesheet href=../assets/stylesheets/custom.7c86dd97.min.css></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#types-of-machine-learning class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <button class="md-banner__button md-icon" aria-label="Don't show this again"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> For updates follow <strong>@squidfunk</strong> on <a rel=me href=https://fosstodon.org/@squidfunk> <span class="twemoji mastodon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.5 102.5 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5m-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg> </span> <strong>Fosstodon</strong> </a> and <a href=https://x.com/squidfunk> <span class="twemoji twitter"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"/></svg> </span> <strong>Twitter</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=announce]");if(el){var content=el.querySelector(".md-typeset");__md_hash(content.innerHTML)===__md_get("__announce")&&(el.hidden=!0)}</script> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title=DasarpAI class="md-header__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> DasarpAI </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Types of Machine Learning </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1M8 13h8v-2H8zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4a5 5 0 0 0 5-5 5 5 0 0 0-5-5"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5M7 15a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=index.html class=md-tabs__link> Data Science </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/pmlogy-home.html class=md-tabs__link> Project Management </a> </li> <li class=md-tabs__item> <a href=../insiders-pages/wia-home.html class=md-tabs__link> SpiritualDrops </a> </li> <li class=md-tabs__item> <a href=../samskrutyatra/index.html class=md-tabs__link> Samskrut </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title=DasarpAI class="md-nav__button md-logo" aria-label=DasarpAI data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> DasarpAI </label> <div class=md-nav__source> <a href=https://github.com/dasarpai/dasrapai-mkdocs title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> dasarpai/dasrapai-mkdocs </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1> <div class="md-nav__link md-nav__container"> <a href=../index.html class="md-nav__link "> <span class=md-ellipsis> Home </span> </a> <label class="md-nav__link " for=__nav_1 id=__nav_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_1_label aria-expanded=false> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/aboutme.html class=md-nav__link> <span class=md-ellipsis> About Me </span> </a> </li> <li class=md-nav__item> <a href=../dscourses/index.html class=md-nav__link> <span class=md-ellipsis> DS/AI Courses/Services </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_1_4> <label class=md-nav__link for=__nav_1_4 id=__nav_1_4_label tabindex=0> <span class=md-ellipsis> Project/Work Catalog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_1_4> <span class="md-nav__icon md-icon"></span> Project/Work Catalog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/project-index-page.html class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-al-ml-projects.md class=md-nav__link> <span class=md-ellipsis> Business Domain </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-my-technology-stacks.md class=md-nav__link> <span class=md-ellipsis> Technology Stack </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/summary-of-management-projects.md class=md-nav__link> <span class=md-ellipsis> Project Management </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../management/index.html class=md-nav__link> <span class=md-ellipsis> PM Courses/Services </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/clients.html class=md-nav__link> <span class=md-ellipsis> Clients </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/testimonials.md class=md-nav__link> <span class=md-ellipsis> Testimonial </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/publications-home.html class=md-nav__link> <span class=md-ellipsis> Publications </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/corpus-home.html class=md-nav__link> <span class=md-ellipsis> History Corpus </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=index.html class="md-nav__link "> <span class=md-ellipsis> Data Science </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Data Science </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../dsresources/index.html class=md-nav__link> <span class=md-ellipsis> DS Resources </span> </a> </li> <li class=md-nav__item> <a href=../news/index.html class=md-nav__link> <span class=md-ellipsis> AI and Business News </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-books.md class=md-nav__link> <span class=md-ellipsis> Data Science-Books </span> </a> </li> <li class=md-nav__item> <a href=data-science-cheatsheets.md class=md-nav__link> <span class=md-ellipsis> Data Science Cheatsheets </span> </a> </li> <li class=md-nav__item> <a href=best-youtube-channels-for-ds.md class=md-nav__link> <span class=md-ellipsis> Video Channels to Learn DS </span> </a> </li> <li class=md-nav__item> <a href=ds-ai-ml-interview-resources.md class=md-nav__link> <span class=md-ellipsis> DS Interview Questions </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../pmblog/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Project Management </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-home.html class=md-nav__link> <span class=md-ellipsis> PMLOGY Home </span> </a> </li> <li class=md-nav__item> <a href=../pmglossary.md class=md-nav__link> <span class=md-ellipsis> PM Glossary </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmlogy-tags.md class=md-nav__link> <span class=md-ellipsis> PM Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-tags.md class=md-nav__link> <span class=md-ellipsis> PMBOK6 Topics </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/pmbok6-summary.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 </span> </a> </li> <li class=md-nav__item> <a href=../pmbok6/index.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Explorer </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8 id=__nav_3_8_label tabindex=0> <span class=md-ellipsis> PM Resources </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_8_label aria-expanded=false> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> PM Resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmi-templates.html class=md-nav__link> <span class=md-ellipsis> PMBOK6 Templates </span> </a> </li> <li class=md-nav__item> <a href=../insiders-pages/prince2-templates.html class=md-nav__link> <span class=md-ellipsis> PRINCE2 Templates </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_9> <div class="md-nav__link md-nav__container"> <a href=../pmbok6hi/index.html class="md-nav__link "> <span class=md-ellipsis> Project Management Hindi </span> </a> <label class="md-nav__link " for=__nav_3_9 id=__nav_3_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Project Management Hindi </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/pmbok6hi-summary.html class=md-nav__link> <span class=md-ellipsis> PMBoK6 Hindi </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../wiaposts/index.html class="md-nav__link "> <span class=md-ellipsis> SpiritualDrops </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> SpiritualDrops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/wia-home.html class=md-nav__link> <span class=md-ellipsis> WIA Home </span> </a> </li> <li class=md-nav__item> <a href=../quotations/index.html class=md-nav__link> <span class=md-ellipsis> WIA Quotes </span> </a> </li> <li class=md-nav__item> <a href=../gk/index.html class=md-nav__link> <span class=md-ellipsis> GK Blog </span> </a> </li> <li class=md-nav__item> <a href=../booksummary/index.html class=md-nav__link> <span class=md-ellipsis> Book Summary </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../samskrutyatra/index.html class="md-nav__link "> <span class=md-ellipsis> Samskrut </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Samskrut </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../insiders-pages/samskrut-home.html class=md-nav__link> <span class=md-ellipsis> SamskrutYatra Home </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=#what-is-pre-training-and-what-is-pre-trained-model class=md-nav__link> <span class=md-ellipsis> What is pre-training and what is pre-trained model? </span> </a> <nav class=md-nav aria-label="What is pre-training and what is pre-trained model?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#benefits-of-pretrained-models class=md-nav__link> <span class=md-ellipsis> Benefits of Pretrained Models </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#fine-tuning-vs-shot-learning class=md-nav__link> <span class=md-ellipsis> Fine-tuning vs shot-learning </span> </a> </li> <li class=md-nav__item> <a href=#shot-learning-in-the-context-of-deep-learning-vs-llm class=md-nav__link> <span class=md-ellipsis> Shot learning in the context of Deep Learning vs LLM </span> </a> </li> <li class=md-nav__item> <a href=#what-is-zero-shot-learning class=md-nav__link> <span class=md-ellipsis> What is Zero-shot learning? </span> </a> <nav class=md-nav aria-label="What is Zero-shot learning?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#key-points class=md-nav__link> <span class=md-ellipsis> Key Points </span> </a> </li> <li class=md-nav__item> <a href=#example class=md-nav__link> <span class=md-ellipsis> Example </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#machine-learning-techniques class=md-nav__link> <span class=md-ellipsis> Machine Learning Techniques </span> </a> <nav class=md-nav aria-label="Machine Learning Techniques"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-supervised-learning class=md-nav__link> <span class=md-ellipsis> 1. Supervised learning </span> </a> </li> <li class=md-nav__item> <a href=#2-unsupervised-learning class=md-nav__link> <span class=md-ellipsis> 2. Unsupervised learning </span> </a> </li> <li class=md-nav__item> <a href=#3-reinforcement-learning class=md-nav__link> <span class=md-ellipsis> 3. Reinforcement learning </span> </a> </li> <li class=md-nav__item> <a href=#4-semi-supervised-learning class=md-nav__link> <span class=md-ellipsis> 4. Semi-supervised learning </span> </a> </li> <li class=md-nav__item> <a href=#5-transfer-learning class=md-nav__link> <span class=md-ellipsis> 5. Transfer learning </span> </a> </li> <li class=md-nav__item> <a href=#6-online-learning class=md-nav__link> <span class=md-ellipsis> 6. Online learning </span> </a> </li> <li class=md-nav__item> <a href=#7-deep-learning class=md-nav__link> <span class=md-ellipsis> 7. Deep learning </span> </a> </li> <li class=md-nav__item> <a href=#8-evolutionary-algorithms class=md-nav__link> <span class=md-ellipsis> 8. Evolutionary algorithms </span> </a> </li> <li class=md-nav__item> <a href=#9-bayesian-learning class=md-nav__link> <span class=md-ellipsis> 9. Bayesian learning </span> </a> </li> <li class=md-nav__item> <a href=#10-instance-based-learning class=md-nav__link> <span class=md-ellipsis> 10. Instance-based learning </span> </a> </li> <li class=md-nav__item> <a href=#11-kernel-methods class=md-nav__link> <span class=md-ellipsis> 11. Kernel methods </span> </a> </li> <li class=md-nav__item> <a href=#12-decision-tree-learning class=md-nav__link> <span class=md-ellipsis> 12. Decision tree learning </span> </a> </li> <li class=md-nav__item> <a href=#13-ensemble-learning class=md-nav__link> <span class=md-ellipsis> 13. Ensemble learning </span> </a> </li> <li class=md-nav__item> <a href=#14-self-supervised-learning class=md-nav__link> <span class=md-ellipsis> 14. Self-supervised learning </span> </a> </li> <li class=md-nav__item> <a href=#shot-learning class=md-nav__link> <span class=md-ellipsis> Shot Learning </span> </a> <nav class=md-nav aria-label="Shot Learning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#15-zero-shot-learning class=md-nav__link> <span class=md-ellipsis> 15. Zero-shot learning </span> </a> </li> <li class=md-nav__item> <a href=#16-one-shot-learning class=md-nav__link> <span class=md-ellipsis> 16. One-shot learning </span> </a> </li> <li class=md-nav__item> <a href=#17-few-shot-learning class=md-nav__link> <span class=md-ellipsis> 17. Few-shot learning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#18-active-learning class=md-nav__link> <span class=md-ellipsis> 18. Active learning </span> </a> </li> <li class=md-nav__item> <a href=#19-multi-task-learning class=md-nav__link> <span class=md-ellipsis> 19. Multi-task learning </span> </a> </li> <li class=md-nav__item> <a href=#20-active-transfer-learning class=md-nav__link> <span class=md-ellipsis> 20. Active transfer learning </span> </a> </li> <li class=md-nav__item> <a href=#21-meta-learning class=md-nav__link> <span class=md-ellipsis> 21. Meta-learning </span> </a> </li> <li class=md-nav__item> <a href=#22-deep-reinforcement-learning class=md-nav__link> <span class=md-ellipsis> 22. Deep reinforcement learning </span> </a> </li> <li class=md-nav__item> <a href=#23-adversarial-learning class=md-nav__link> <span class=md-ellipsis> 23. Adversarial learning </span> </a> </li> <li class=md-nav__item> <a href=#24-contrastive-learning class=md-nav__link> <span class=md-ellipsis> 24. Contrastive learning </span> </a> </li> <li class=md-nav__item> <a href=#25-adaptive-learning class=md-nav__link> <span class=md-ellipsis> 25. Adaptive Learning </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#26-federated-learning class=md-nav__link> <span class=md-ellipsis> 26. Federated learning </span> </a> </li> <li class=md-nav__item> <a href=#conclusion class=md-nav__link> <span class=md-ellipsis> Conclusion </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/dasarpai/dasrapai-mkdocs/edit/master/docs/dsblog/Types-of-Machine-Learning.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg> </a> <a href=https://github.com/dasarpai/dasrapai-mkdocs/raw/master/docs/dsblog/Types-of-Machine-Learning.md title="View source of this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg> </a> <p><img alt="Types of Machine Learning" src=../assets/images/dspost/dsp6056-Types-of-Machine-Learning.jpg> </p> <h1 id=types-of-machine-learning>Types of Machine Learning<a class=headerlink href=#types-of-machine-learning title="Permanent link">&para;</a></h1> <h2 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h2> <p>Machine learning is a field of artificial intelligence that focuses on developing algorithms that can learn from data and make predictions or decisions. There are several types of machine learning techniques, each with its strengths and weaknesses. In this post, we will explore some of the most commonly used machine learning techniques, including supervised learning, unsupervised learning, reinforcement learning, and more. This post is not about deep diving into these topics but to give you a oneliner understanding and the difference between these different techniques.</p> <p>Before we dive into different kind of learning let's understand few related important concepts.</p> <h2 id=what-is-pre-training-and-what-is-pre-trained-model>What is pre-training and what is pre-trained model?<a class=headerlink href=#what-is-pre-training-and-what-is-pre-trained-model title="Permanent link">&para;</a></h2> <p><strong>Pre-Training</strong>: The process of training a machine learning model on a large, general dataset to learn broad patterns and features.</p> <p><strong>Pre-Trained Model</strong>: A model that has already undergone pre-training and learned general features, which can be further adapted or fine-tuned for specific tasks.</p> <p>Pre-trained models are developed using extensive and diverse datasets, sometime diverse modalities, allowing them to learn a broad range of features, patterns, and representations. For example, a pre-trained image classification model might be trained on millions of images across various categories.</p> <p>Pre-trained models are often used in transfer learning, where they are adapted to a new, often more specific task. Instead of training a model from scratch, which can be time-consuming and require large amounts of data, you start with a pre-trained model and fine-tune it on your specific dataset.</p> <p>Using pre-trained models can significantly reduce training time and computational resources, as the model already has learned useful features and representations that can be leveraged for new tasks.</p> <p>In NLP Models like GPT-3, BERT, and T5 are pre-trained on large text corpora and can be fine-tuned for specific tasks like sentiment analysis, translation, or summarization. In Computer Vision Models like ResNet, VGG, and EfficientNet are pre-trained on large image datasets like ImageNet and can be fine-tuned for tasks like object detection or medical image analysis.</p> <h3 id=benefits-of-pretrained-models>Benefits of Pretrained Models<a class=headerlink href=#benefits-of-pretrained-models title="Permanent link">&para;</a></h3> <ul> <li><strong>Reduced Training Time</strong>: Leveraging existing pre-trained models saves time compared to training a model from scratch.</li> <li><strong>Improved Performance</strong>: Pre-trained models often achieve better performance on specific tasks due to their exposure to extensive and diverse data during pre-training.</li> <li><strong>Resource Efficiency</strong>: Less computational power is required since the model is already trained on large datasets.</li> </ul> <h2 id=fine-tuning-vs-shot-learning>Fine-tuning vs shot-learning<a class=headerlink href=#fine-tuning-vs-shot-learning title="Permanent link">&para;</a></h2> <p>Fine-tuning and shot learning looks same but here is a shuttle difference between them.</p> <table> <thead> <tr> <th>Aspect</th> <th>Fine-Tuning</th> <th>Shot Learning</th> </tr> </thead> <tbody> <tr> <td><strong>Definition</strong></td> <td>Further training of a pre-trained model on a smaller, task-specific dataset.</td> <td>Training a model to perform well with very few examples or even just one.</td> </tr> <tr> <td><strong>Training Data</strong></td> <td>Typically involves a moderate-sized dataset related to the specific task.</td> <td>Involves minimal data (few-shot or one-shot examples).</td> </tr> <tr> <td><strong>Weight Updates</strong></td> <td>Weights of the model are updated during the fine-tuning process.</td> <td>Often involves minimal weight updates (for deep learing), no weight update in LLM; focuses on adapting to new tasks quickly.</td> </tr> <tr> <td><strong>Purpose</strong></td> <td>Adapt a general-purpose model to a specific application or domain.</td> <td>Enable quick adaptation to new tasks or classes with limited examples.</td> </tr> <tr> <td><strong>Examples</strong></td> <td>Fine-tuning a language model on specific industry jargon/concept/process/idea.</td> <td>Using a few examples to teach a model to classify new types of images.</td> </tr> <tr> <td><strong>Techniques</strong></td> <td>Transfer learning, domain adaptation.</td> <td>Meta-learning, prototypical networks, relation networks.</td> </tr> <tr> <td><strong>Computational Cost</strong></td> <td>Moderate, depending on the size of the fine-tuning dataset.</td> <td>Low, as only a small amount of data is used for adaptation.</td> </tr> </tbody> </table> <h2 id=shot-learning-in-the-context-of-deep-learning-vs-llm>Shot learning in the context of Deep Learning vs LLM<a class=headerlink href=#shot-learning-in-the-context-of-deep-learning-vs-llm title="Permanent link">&para;</a></h2> <p>Due to LLM flood in the market, nowadays when people hear about shot learning they think it is prompt engineering. But shot-learning is different in the context of deep learning vs its usage in LLM.</p> <p>Certainly! Hereâ€™s a comparison of shot learning in the context of Deep Learning versus Large Language Models (LLMs):</p> <table> <thead> <tr> <th>Aspect</th> <th>Deep Learning</th> <th>Large Language Models (LLMs)</th> </tr> </thead> <tbody> <tr> <td><strong>Definition</strong></td> <td>Training models to recognize or perform tasks with very few examples.</td> <td>Adapting models to handle new tasks with minimal examples in the prompt.</td> </tr> <tr> <td><strong>Training Process</strong></td> <td>Involves techniques like meta-learning, where the model's weights are updated with a few examples.</td> <td>Uses in-context learning, where the model adapts to tasks based on the examples provided in the prompt without updating weights.</td> </tr> <tr> <td><strong>Weight Updates</strong></td> <td>Model weights are updated during training with few examples (e.g., few-shot learning).</td> <td>No weight updates; the model uses examples to infer how to perform the task at inference time.</td> </tr> <tr> <td><strong>Examples</strong></td> <td>Few-shot learning for image classification where the model is fine-tuned with a few images per class.</td> <td>Few-shot prompting for text generation where a few example inputs and outputs are provided in the prompt.</td> </tr> <tr> <td><strong>Contextual Adaptation</strong></td> <td>Adapts through learning mechanisms specific to the task (e.g., metric learning or prototype-based methods).</td> <td>Adapts using the context provided in the prompt to understand and perform new tasks.</td> </tr> <tr> <td><strong>Common Techniques</strong></td> <td>Prototypical Networks, Matching Networks, Meta-Learning (e.g., MAML).</td> <td>In-context learning, prompt engineering, zero-shot and few-shot prompting.</td> </tr> <tr> <td><strong>Computational Cost</strong></td> <td>Can be higher due to fine-tuning or training with few examples.</td> <td>Generally lower, as it involves using the pre-trained model directly with example prompts.</td> </tr> <tr> <td><strong>Data Efficiency</strong></td> <td>Requires careful selection of few examples for effective learning.</td> <td>Leverages pre-trained knowledge to perform well with minimal data in prompts.</td> </tr> </tbody> </table> <h2 id=what-is-zero-shot-learning>What is Zero-shot learning?<a class=headerlink href=#what-is-zero-shot-learning title="Permanent link">&para;</a></h2> <p><strong>Zero-Shot Learning</strong> is a technique where a model performs a task or recognizes a class it was not explicitly trained on, using its pre-existing knowledge. Keep in mind this is not learning in traditional sense, because there is not weight and bias updatation happening. A pretrained model is being used for inference purpose in the development environment. If you are satisfied then you can move it to production.</p> <h3 id=key-points>Key Points<a class=headerlink href=#key-points title="Permanent link">&para;</a></h3> <ul> <li><strong>Pre-Trained Knowledge</strong>: The model has been trained on a large and diverse dataset, learning general patterns and features.</li> <li><strong>No Additional Training</strong>: The model performs the new task without further training or weight updates specific to that task.</li> <li><strong>Task Generalization</strong>: The model uses its broad understanding from pre-training to generalize and handle new tasks or categories.</li> <li><strong>Inference</strong>: The process involves applying learned knowledge to new tasks based on descriptions, prompts, or context provided.</li> </ul> <h3 id=example>Example<a class=headerlink href=#example title="Permanent link">&para;</a></h3> <p>A model trained on general text data can classify sentiment, answer questions, or perform named entity recognition on new, unseen tasks by leveraging its pre-trained knowledge.</p> <p>In essence, zero-shot learning enables models to tackle new tasks effectively without task-specific training.</p> <p><strong>Now let's understand different type of machine learning, which is the main topic of this artcile.</strong></p> <h2 id=machine-learning-techniques>Machine Learning Techniques<a class=headerlink href=#machine-learning-techniques title="Permanent link">&para;</a></h2> <h3 id=1-supervised-learning>1. Supervised learning<a class=headerlink href=#1-supervised-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning where the model is trained on labeled data. In supervised learning, the model learns from a set of examples where the input data and the corresponding output labels are provided. Supervised learning is used for most common regression and classification tasks.</p> <p>Some examples of Supervised learning algorithms:</p> <ul> <li>Linear regression</li> <li>Logistic regression</li> <li>Decision trees</li> <li>Random forests</li> <li>Support vector machines (SVMs)</li> <li>Naive Bayes</li> <li>K-nearest neighbors (KNN)</li> <li>Neural networks</li> <li>Gradient Boosting Machines (GBMs)</li> <li>Convolutional Neural Networks (CNNs)</li> <li>Recurrent Neural Networks (RNNs)</li> <li>Long Short-Term Memory (LSTM)</li> <li>Gated Recurrent Unit (GRU)</li> <li>Extreme Gradient Boosting (XGBoost)</li> <li>LightGBM.</li> </ul> <h3 id=2-unsupervised-learning>2. Unsupervised learning<a class=headerlink href=#2-unsupervised-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning where the model learns to recognize patterns in data without the need for labeled examples. Unsupervised learning is used for clustering, dimensionality reduction, and anomaly detection.</p> <p>Some examples of unsupervised learning algorithms:</p> <ul> <li>Clustering Algorithms (K-Means, Hierarchical Clustering, DBSCAN, etc.)</li> <li>Dimensionality Reduction Algorithms (PCA, t-SNE, LLE, etc.)</li> <li>Anomaly Detection Algorithms (Isolation Forest, One-Class SVM, etc.)</li> <li>Association Rule Learning Algorithms (Apriori, ECLAT, etc.)</li> <li>Generative Adversarial Networks (GANs)</li> <li>Autoencoders</li> <li>Self-Organizing Maps (SOMs)</li> <li>Independent Component Analysis (ICA)</li> <li>Principal Component Analysis (PCA)</li> <li>Non-negative Matrix Factorization (NMF)</li> </ul> <h3 id=3-reinforcement-learning>3. Reinforcement learning<a class=headerlink href=#3-reinforcement-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning where the model learns through trial and error by interacting with an environment. The model receives feedback in the form of rewards or penalties for the actions it takes and learns to maximize the rewards over time. RL is used when we don't have enough data to train the model, or when the data does not represent the real world, which has changed or changing fast. RL model building is time-consuming because learning is the process of acting. A robot that can perform a task where the law of physics needs to be obeyed needs RL.</p> <p>Some examples of reinforcement learning algorithms:</p> <ul> <li>Q-Learning: Q-learning is a model-free reinforcement learning algorithm that learns to determine the value of an action in a particular state. It does not require a model of the environment and can handle problems with stochastic transitions and rewards.</li> <li>SARSA (State-Action-Reward-State-Action): SARSA is another model-free reinforcement learning algorithm that is similar to Q-learning. However, instead of learning the value of the optimal action in a given state, it learns the value of the action actually taken.</li> <li>Deep Q-Networks (DQN): DQN is a deep reinforcement learning algorithm that uses a neural network to approximate the action-value function. It has been successfully used to play Atari games and other complex tasks.</li> <li>Actor-Critic: Actor-Critic is a family of reinforcement learning algorithms that combine elements of policy-based and value-based methods. It uses two networks, one to estimate the value of a state and another to estimate the policy.</li> <li>Proximal Policy Optimization (PPO): PPO is a policy optimization method that updates the policy in small steps while ensuring that the policy does not move too far away from the previous policy. It has been shown to be effective in a wide range of tasks, including robotics and game playing.</li> <li>Asynchronous Advantage Actor-Critic (A3C): A3C is a distributed version of the Actor-Critic algorithm that allows multiple agents to learn concurrently. It has been shown to be effective in both continuous and discrete action spaces.</li> </ul> <h3 id=4-semi-supervised-learning>4. Semi-supervised learning<a class=headerlink href=#4-semi-supervised-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning that combines both supervised and unsupervised learning. Semi-supervised learning uses a small amount of labeled data along with a large amount of unlabeled data to train the model. Here, first, we create a model with small labeled data and then use the model to predict the label of unlabelled data. After that, we use these predicted labels and create a bigger dataset. Now from this bigger dataset, we can further train the model. We keep doing this and making note of how the model is performing on the originally labeled data. Semi-supervised clustering is used in applications such as gene expression analysis and social network analysis.</p> <p>Some popular approaches for semi-supervised learning include:</p> <ul> <li>Self-Training: Self-training is a simple approach for semi-supervised learning. It involves training a model on the labeled data and then using that model to predict labels for the unlabeled data. The confident predictions are then added to the labeled dataset, and the process is repeated.</li> <li>Co-Training: Co-Training is a semi-supervised learning approach where the algorithm uses multiple views of the data to train models. It involves training multiple models, each on a different subset of the features, and then exchanging the predicted labels for the unlabeled data between the models.</li> <li>Generative Models: Generative models, such as Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN), can also be used for semi-supervised learning. These models learn to generate new data points that are similar to the training data and can be used to label the unlabeled data.</li> <li>Graph-Based Methods: Graph-based semi-supervised learning methods use the structure of the data to propagate the labels from the labeled to the unlabeled data. Some examples of graph-based methods include Label Propagation and Graph Convolutional Networks (GCN).</li> <li>Entropy Regularization: Entropy regularization is a technique that encourages the model to make confident predictions by penalizing the entropy of the model's output distribution. This technique can be used to make the most of the limited labeled data by leveraging the information in the unlabeled data.</li> </ul> <h3 id=5-transfer-learning>5. Transfer learning<a class=headerlink href=#5-transfer-learning title="Permanent link">&para;</a></h3> <p>It is a technique that enables a model to transfer knowledge learned from one task to another. Transfer learning can be used to improve the performance of a model on a new task where labeled data is scarce or where a task is very common and data is similar. TL is very common in Computer Vision, NLP, and reinforcement Learning.</p> <p>Although every neural network based model can be used for transfer learning but for example sake various models for transfer learning are:</p> <ul> <li>VGG: VGG is a popular deep learning model known for its accuracy on image classification tasks. It is often used as a pre-trained model for transfer learning.</li> <li>ResNet: ResNet is another popular deep learning model known for its performance on image classification tasks. It has several variants, including ResNet-50 and ResNet-101.</li> <li>BERT: BERT is a popular pre-trained model for natural language processing (NLP) tasks, including text classification and question answering.</li> <li>GPT: GPT (Generative Pre-trained Transformer) is a pre-trained model for NLP tasks, including language modeling, question answering, and text completion.</li> <li>Inception: Inception is a deep learning model known for its performance on image classification tasks. It has several variants, including InceptionV3 and Inception-ResNet.</li> <li>MobileNet: MobileNet is a deep learning model optimized for mobile devices. It is often used for transfer learning on image classification tasks.</li> <li>AlexNet: AlexNet is a deep learning model known for its accuracy on image classification tasks. It is often used as a pre-trained model for transfer learning.</li> </ul> <h3 id=6-online-learning>6. Online learning<a class=headerlink href=#6-online-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning where the model learns incrementally from incoming data streams. Online learning is used for real-time applications where the model needs to continuously adapt to new data. So parameters of the model keep updating after some time or after a certain number of observations. When data is too large to fit in the memory or changing fast, like millions of people buying thousands of products online, fraud detection, online advertisement, a live cast of an event, city traffic, mass political rally, city law, and order situation, especially in tense time, etc.</p> <p>Examples of online learning algorithms - Online Gradient Descent - Stochastic Gradient Descent - Adaptive Gradient Descent - Passive-Aggressive Algorithm - Online Random Forests - Online Naive Bayes - Online Support Vector Machines - Recursive Least Squares - Incremental Decision Trees - Perceptron Algorithm</p> <h3 id=7-deep-learning>7. Deep learning<a class=headerlink href=#7-deep-learning title="Permanent link">&para;</a></h3> <p>It is a subset of machine learning that uses artificial neural networks with multiple layers to learn from data. Deep learning has achieved state-of-the-art results in tasks such as image recognition, natural language processing, and speech recognition.</p> <p>Some examples of deep learning algorithms:</p> <ul> <li>Convolutional Neural Networks (CNNs) - used for image and video analysis</li> <li>Recurrent Neural Networks (RNNs) - used for natural language processing, speech recognition, and time series analysis</li> <li>Long Short-Term Memory (LSTM) networks - a type of RNN used for modeling sequences with long-term dependencies</li> <li>Generative Adversarial Networks (GANs) - used for generating new data that resembles the training data</li> <li>Deep Belief Networks (DBNs) - used for unsupervised learning, feature learning, and classification tasks</li> <li>Autoencoders - used for dimensionality reduction, feature extraction, and image denoising</li> <li>Deep Reinforcement Learning (DRL) - is used for decision-making tasks in environments with complex rules and states.</li> </ul> <h3 id=8-evolutionary-algorithms>8. Evolutionary algorithms<a class=headerlink href=#8-evolutionary-algorithms title="Permanent link">&para;</a></h3> <p>Evolutionary algorithms are a family of optimization algorithms that are inspired by the process of natural selection. In this approach, a population of candidate solutions is evolved over multiple generations to find the best solution to a given problem. The basic idea is to apply the principles of survival of the fittest, reproduction, and mutation to search for optimal solutions.</p> <p>An example of an evolutionary algorithm is the genetic algorithm. In this algorithm, the candidate solutions are represented as chromosomes, which are encoded as binary strings or vectors of real numbers. The algorithm starts with a population of randomly generated chromosomes and then applies selection, crossover, and mutation operators to create new offspring chromosomes for the next generation.</p> <p>During the selection phase, the best chromosomes in the population are chosen based on their fitness, which is a measure of how well they solve the problem. The fitness function is problem-specific and evaluates the quality of the solution represented by each chromosome. The selected chromosomes are then combined through crossover, which involves swapping parts of their genetic material, to create new offspring chromosomes. Finally, the offspring chromosomes are subject to mutation, which randomly changes their genetic material to introduce new variations into the population.</p> <p>This process is repeated over multiple generations, with the hope that the best solution will emerge over time. Evolutionary algorithms have been used to solve a wide range of problems, including optimization, control, and machine learning.</p> <p>Evolutionary algorithms are often used when other methods such as deep learning or Bayesian learning are not feasible or appropriate for the problem at hand.</p> <p>Some examples of Evolutionary algorithms are - Genetic algorithm (GA) - Differential evolution (DE) - Particle swarm optimization (PSO) - Ant colony optimization (ACO) - Artificial bee colony (ABC) - Cultural algorithm (CA) - Harmony search (HS) - Firefly algorithm (FA) - Bat algorithm (BA) - Grey wolf optimizer (GWO) - Cuckoo search (CS) - Imperialist competitive algorithm (ICA) - Whale optimization algorithm (WOA) - Gravitational search algorithm (GSA) - Multi-objective optimization algorithms (MOO) such as NSGA-II and SPEA2.</p> <h3 id=9-bayesian-learning>9. Bayesian learning<a class=headerlink href=#9-bayesian-learning title="Permanent link">&para;</a></h3> <p>Bayesian learning is a probabilistic approach to machine learning that involves using Bayes' theorem to update the probability of a hypothesis as new evidence is acquired. In Bayesian learning, we start with a prior belief about the probability of a hypothesis and then update this belief as new evidence becomes available. The result is a posterior probability distribution over the hypotheses that reflects our updated belief.</p> <p>An example of Bayesian learning is in the spam email classification. Suppose we have a dataset of emails that are labeled as either spam or not spam. We want to build a model that can classify new emails as spam or not spam. We can use Bayesian learning to build a spam filter by treating each email as a hypothesis and updating our belief about the probability of the email being spam as new evidence becomes available.</p> <p>To do this, we start by assigning a prior probability distribution to the hypothesis (i.e., the email being spam or not spam) based on our prior knowledge about the dataset. As we receive new emails and classify them as spam or not spam, we update the prior probability distribution using Bayes' theorem. Specifically, we use the likelihood of the data (i.e., the features of the email) given the hypothesis and the prior probability distribution to calculate the posterior probability distribution over the hypotheses.</p> <p>Bayesian learning has the advantage of being able to incorporate prior knowledge into the learning process and update this knowledge as new evidence becomes available. This makes it particularly useful in situations where data is limited and prior knowledge is available.</p> <p>Examples of Bayesian learning algorithms:</p> <ul> <li>Naive Bayes Classifier: It is a simple probabilistic classifier based on Bayes' theorem. It assumes that the features are independent of each other.</li> <li>Bayesian Linear Regression: It is a regression technique that uses Bayesian inference to estimate the parameters of a linear regression model.</li> <li>Bayesian Neural Networks: It is a type of neural network that uses Bayesian inference to estimate the parameters of the network. It can provide uncertainty estimates for the model predictions.</li> <li>Bayesian Decision Theory: It is a framework that uses Bayesian inference to make decisions based on uncertain or incomplete information.</li> <li>Bayesian Belief Networks: It is a graphical model that represents a set of random variables and their conditional dependencies using a directed acyclic graph. It can be used for probabilistic inference and decision-making.</li> </ul> <h3 id=10-instance-based-learning>10. Instance-based learning<a class=headerlink href=#10-instance-based-learning title="Permanent link">&para;</a></h3> <p>Instance-based learning, also known as lazy learning, is a type of machine learning in which the algorithm learns by memorizing the training dataset instead of constructing a general model. It is a type of machine learning that relies on similarity measures between instances in a dataset to make predictions. In this approach, the algorithm makes predictions based on the similarity between new instances and the instances in the training dataset.</p> <p>For example, consider a dataset containing information about different cars such as their make, model, year of manufacture, mileage, and price. We want to predict the price of a new car based on its make, model, year of manufacture, and mileage.</p> <p>In instance-based learning, the algorithm would simply store the entire training dataset in memory, and when a new car is presented, it would find the k most similar cars in the training dataset and use their prices to predict the price of the new car. The similarity between instances can be calculated using distance measures such as Euclidean distance, Manhattan distance, or cosine similarity.</p> <p>One popular instance-based learning algorithm is k-nearest neighbors (k-NN), where k is the number of nearest neighbors to consider. In the car price prediction example, if k is set to 3, the algorithm would find the 3 most similar cars in the training dataset and use their prices to predict the price of the new car.</p> <p>Instance-based learning has the advantage of being able to handle complex and nonlinear relationships in the data, but it can be computationally expensive and requires a large amount of memory to store the training dataset. Instance-based learning is used in applications such as recommendation systems and anomaly detection.</p> <p>Examples of instance-based learning algorithms are: - k-Nearest Neighbors (k-NN) - Learning Vector Quantization (LVQ) - Case-based reasoning (CBR) - Locally weighted regression (LWR) - Self-organizing map (SOM)</p> <h3 id=11-kernel-methods>11. Kernel methods<a class=headerlink href=#11-kernel-methods title="Permanent link">&para;</a></h3> <p>These are techniques that use kernel functions to transform data into a higher-dimensional space, where linear models can be used to make predictions. Kernel methods are used in tasks such as classification, regression, and clustering. One of the most popular kernel methods is the support vector machine (SVM), which uses a kernel function to map input data into a higher-dimensional space and finds a hyperplane that maximizes the margin between two classes of data. Other examples of kernel methods include kernel PCA (principal component analysis) for dimensionality reduction, kernel k-means for clustering, and Gaussian processes for regression. Kernel methods are widely used in various domains, including image and speech recognition, natural language processing, and bioinformatics. They are particularly useful when the data has a complex structure and cannot be easily modeled using linear methods.</p> <p>Examples of kernel methods algorithms:</p> <ul> <li>Support Vector Machines (SVM)</li> <li>Kernel Principal Component Analysis (KPCA)</li> <li>Kernel Canonical Correlation Analysis (KCCA)</li> <li>Gaussian Processes (GP)</li> <li>Kernel Ridge Regression (KRR)</li> <li>Locally Linear Embedding (LLE) with the kernel trick</li> <li>Multiple Kernel Learning (MKL)</li> <li>Kernel Discriminant Analysis (KDA)</li> <li>Spectral Clustering with the kernel trick</li> <li>Regularized Least Squares Classification (RLSC)</li> </ul> <h3 id=12-decision-tree-learning>12. Decision tree learning<a class=headerlink href=#12-decision-tree-learning title="Permanent link">&para;</a></h3> <p>It is a method that builds a tree-like model of decisions and their possible consequences. Decision tree learning is used in many domains like medical diagnosis, fraud detection and credit risk analysis. Its simplicity and interpretability make it a popular choice for many businesses and organizations.</p> <p>Some examples of decision tree learning algorithms:</p> <ul> <li>ID3 (Iterative Dichotomiser 3): This is one of the earliest decision tree algorithms developed by Ross Quinlan in 1986. It uses the entropy measure to find the best split at each node.</li> <li>C4.5: This is an extension of the ID3 algorithm, developed by Quinlan in 1993. It uses the gain ratio measure to overcome the bias of ID3 towards attributes with a large number of values.</li> <li>CART (Classification and Regression Trees): This algorithm was developed by Breiman, Friedman, Olshen, and Stone in 1984. CART can be used for both classification and regression tasks and uses the Gini index as a measure of impurity.</li> </ul> <h3 id=13-ensemble-learning>13. Ensemble learning<a class=headerlink href=#13-ensemble-learning title="Permanent link">&para;</a></h3> <p>It is a technique that combines multiple models to improve their accuracy and robustness. Ensemble learning is used in applications such as classification and regression.</p> <p>Some examples of ensemble learning algorithms: - Random Forest: It is an ensemble of decision trees that are trained on different subsets of the dataset and use a random subset of features for each split. The final prediction is the mode of the predictions of all trees. - Bagging: It is an ensemble of models trained on bootstrapped samples of the dataset. The final prediction is the average of predictions of all models. - Stacking: It is an ensemble of models where the predictions of base models are used as input to a meta-model that learns to combine them. The final prediction is the output of the meta-model. - AdaBoost: AdaBoost is short for "Adaptive Boosting". In this algorithm, each base learner is trained on the samples that are misclassified by the previous base learners. It is called "adaptive" because the weights of the misclassified samples are increased, making them more important in subsequent iterations. This allows AdaBoost to focus on the difficult samples and achieve high accuracy with a small number of base learners. The final prediction is the weighted sum of the predictions of all learners. - Gradient Boosting: Gradient Boosting is similar to AdaBoost, but it uses the gradient of the loss function with respect to the model's prediction as a measure of the sample's importance. In each iteration, a new base learner is trained to fit the negative gradient of the loss function. This allows Gradient Boosting to handle differentiable loss functions and achieve high accuracy on a wide range of tasks. The final prediction is the sum of the predictions of all learners. - XGBoost: XGBoost stands for "Extreme Gradient Boosting". It is a more advanced version of Gradient Boosting that includes several additional features, such as regularization, tree pruning, and parallel processing. These features allow XGBoost to handle larger datasets and achieve even higher accuracy than Gradient Boosting. - LightGBM: LightGBM is a more recent boosting algorithm that is designed to be faster and more memory-efficient than XGBoost. It achieves this by using a novel approach called "Gradient-based One-Side Sampling" (GOSS) to select the most informative samples for each base learner. This allows LightGBM to achieve high accuracy with a smaller number of base learners and lower memory usage. - CatBoost: CatBoost is another recent boosting algorithm that is designed to handle categorical features more effectively than other boosting algorithms. It achieves this by using a novel approach called "Ordered Boosting", which creates a permutation of the categorical features that are ordered based on their impact on the loss function. This allows CatBoost to achieve higher accuracy on datasets with a large number of categorical features.</p> <h3 id=14-self-supervised-learning>14. Self-supervised learning<a class=headerlink href=#14-self-supervised-learning title="Permanent link">&para;</a></h3> <p>Self-supervised learning is a type of machine learning in which the algorithm learns from unlabeled data by creating labels from the data itself, rather than relying on human-labeled data.</p> <p>Suppose we have a large dataset of unlabeled images, and we want to train a model to recognize objects in those images. In a self-supervised approach, we can create labels for the images by asking the model to perform a related but easier task, such as predicting the rotation angle of the image.</p> <p>We start by randomly rotating each image by a certain angle (e.g., 90 degrees) to create a new, rotated version of the image. We then train the model to predict the rotation angle of the rotated image, given the original, unrotated image as input. The model learns to predict the rotation angle by extracting and encoding features from the image, such as the location of edges, corners, and other patterns.</p> <p>After training on this task, the model has learned a set of features that are useful for predicting the rotation angle but can also be used for other related tasks, such as object recognition. We can then fine-tune the model on a smaller set of labeled data, such as images with labeled object categories, to improve its performance on the specific task of object recognition.</p> <p>Overall, self-supervised learning is a powerful technique for training machine learning models on large amounts of unlabeled data and can lead to significant improvements in performance when combined with smaller amounts of labeled data. It has been applied to a wide range of tasks in computer vision, natural language processing, video processing, and other areas of machine learning.</p> <p>Some examples of self-supervised learning:</p> <ul> <li> <p>Contrastive Predictive Coding (CPC): In this method, a neural network is trained to predict the relationship between different parts of the same input. For example, in an image, the network learns to predict the relationship between two different patches of the same image.</p> </li> <li> <p>Autoencoder: An autoencoder is a neural network that learns to reconstruct the input data from a lower-dimensional representation. It is trained on unlabeled data, where the input and output are the same.</p> </li> <li> <p>Generative Adversarial Networks (GANs): GANs are a type of self-supervised learning method where two neural networks are trained simultaneously. One network generates fake data, while the other network tries to distinguish the fake data from real data.</p> </li> <li> <p>Pretext tasks: Pretext tasks are a general category of self-supervised learning methods that involve training a neural network to perform a specific task that is not the ultimate task of interest, but is related to it. For example, a network may be trained to predict the rotation or colorization of an image and then fine-tuned on a downstream task such as image classification.</p> </li> </ul> <h3 id=shot-learning>Shot Learning<a class=headerlink href=#shot-learning title="Permanent link">&para;</a></h3> <h4 id=15-zero-shot-learning>15. Zero-shot learning<a class=headerlink href=#15-zero-shot-learning title="Permanent link">&para;</a></h4> <p>Zero-shot learning is a machine learning technique that allows a model to recognize and classify objects that it has never seen before. In zero-shot learning, the model is trained on a set of classes but is expected to recognize and classify instances of new classes that were not part of the training set. This is achieved by learning a semantic embedding space that maps the objects to a semantic space, where classes are represented as points or regions, and objects are classified based on their proximity to these points or regions. Zero-shot learning is particularly useful in situations where it is difficult or impossible to obtain labeled training data for all possible classes. An example of zero-shot learning is recognizing a new species of bird based on its visual characteristics, without having any labeled examples of that bird species in the training set. </p> <p>Various Models which help Zero-shot learning: - Attribute-based models: These models learn a semantic representation of the attributes that describe the visual properties of objects, and use this to classify new objects. - Semantic embeddings: These models learn a mapping from the visual features of an object to a semantic embedding space, where objects can be compared and classified based on their similarity in the embedding space. - Generative models: These models learn a generative model of the data distribution, which can be used to generate new samples and classify new objects. - Hybrid models: These models combine different approaches, such as attribute-based and semantic embedding models, to improve performance and generalize across different types of objects.</p> <h4 id=16-one-shot-learning>16. One-shot learning<a class=headerlink href=#16-one-shot-learning title="Permanent link">&para;</a></h4> <p>It is a type of machine learning where the model can learn from a single example. One-shot learning is used in applications such as facial recognition and handwriting recognition.</p> <p>Models for one-shot learning, including:</p> <ul> <li>Siamese Networks: Siamese Networks use a twin neural network architecture that learns to compare two images and output a similarity score. It is trained on a set of pairs of images, one from the same class and another from a different class. Once the network is trained, it can compare a new image to the training set and output a similarity score, allowing it to classify the new image.</li> <li>Matching Networks: Matching Networks use a learnable attention mechanism to select a subset of the support set that is most relevant to the query image. This allows the network to effectively leverage the limited labeled data available in the few-shot scenario.</li> <li>Prototypical Networks: Prototypical Networks learn a metric space where images of the same class are close together and images of different classes are far apart. The network learns to encode images as a set of prototypes, one for each class, and classify new images based on their similarity to the prototypes.</li> <li>Meta-learning (or Learning-to-learn) Models: Meta-learning models learn to learn from a few examples by training on many similar tasks. The idea is to learn a set of shared parameters that can be quickly adapted to a new task with few examples. One example of such models is MAML (Model-Agnostic Meta-Learning).</li> </ul> <h4 id=17-few-shot-learning>17. Few-shot learning<a class=headerlink href=#17-few-shot-learning title="Permanent link">&para;</a></h4> <p>Few-shot learning is a type of machine learning where a model is trained to learn from a few examples of a new task or class. This is useful in scenarios where the dataset for a particular task is limited, but the model needs to generalize well and perform well on unseen data.</p> <p>For example, suppose we have a dataset of images of animals and want to train a model to recognize new species of animals that have only a few images available. In traditional machine learning, we would need a large labeled dataset of the new species to train a model that performs well. In few-shot learning, we can train a model using a small number of examples of the new species, for example, only 5 or 10 images, and then use this model to recognize new instances of the species.</p> <p>In few-shot learning, the model is typically trained on a small number of examples of the new task or class, along with examples from other tasks or classes. The model then uses this knowledge to recognize new instances of the new task or class.</p> <p>Various Models which help few-shot learning: - Siamese Networks: Siamese networks consist of two or more identical neural networks that share weights. They are often used for tasks such as image matching and one-shot learning, where the network learns to recognize similar or dissimilar pairs of inputs. - Prototypical Networks: Prototypical networks learn to classify new examples based on the distance to a set of learned prototypes or centroids. The prototypes are computed as the mean feature vectors of each class in the support set. - Matching Networks: Matching networks use attention mechanisms to weigh the importance of different parts of the input when making a decision. They are trained on a support set of labeled examples and can then generalize to new examples with few labels. - Meta-learning: Meta-learning, also known as "learning to learn," involves training a model to learn how to learn from a few examples. This can be done using a variety of techniques, such as gradient-based meta-learning and metric-based meta-learning. - Model-agnostic Meta-learning (MAML): MAML is a meta-learning approach that aims to learn a good initialization for a neural network such that it can quickly adapt to new tasks with only a few examples. MAML is model-agnostic, meaning it can be applied to a wide range of deep-learning models.</p> <h3 id=18-active-learning>18. Active learning<a class=headerlink href=#18-active-learning title="Permanent link">&para;</a></h3> <p>Active learning is a machine learning technique where an algorithm iteratively selects a subset of the available data to be labeled by an expert or a human annotator, with the goal of maximizing the performance of the model while minimizing the cost of annotation. Suppose we want to build a model that can classify customer support tickets into one of several categories (e.g., billing, technical support, product issues). We have a large set of unlabeled support tickets, but we can only afford to label a small fraction of them due to the cost of annotation.</p> <p>In an active learning approach, we start by randomly selecting a small subset of support tickets and annotating them with their corresponding category labels. We then train a classification model on this labeled subset, and use it to make predictions on the remaining unlabeled data.</p> <p>Next, instead of randomly selecting additional examples to label, the active learning algorithm selects the examples that it is most uncertain about, based on the model's current prediction probabilities. These uncertain examples are then sent to a human annotator to be labeled, and added to the training set. The model is retrained on the expanded labeled dataset, and the process repeats until performance reaches a satisfactory level or the available budget for annotation is exhausted.</p> <p>The key idea behind active learning is that by selectively choosing the most informative examples to be labeled, the algorithm can achieve better performance with fewer labeled examples than would be required by a random sampling approach.</p> <p>Examples of algorithms for Active Learning are. - Support Vector Machines (SVM): SVM is a popular model used for binary classification tasks in active learning. It is based on finding the hyperplane that separates the two classes with maximum margin. - Decision Trees: Decision trees can be used for classification tasks in active learning scenarios. They can be useful for selecting the most informative data points to label next. - Neural Networks: Neural networks have been used in active learning for both classification and regression tasks. They can be trained using various active learning strategies. - Bayesian Models: Bayesian models are probabilistic models that can be used in active learning to model the uncertainty of the model's predictions. - Random Forest: Random forests can be used for classification tasks in active learning scenarios. They can be useful for selecting the most informative data points to label next.</p> <h3 id=19-multi-task-learning>19. Multi-task learning<a class=headerlink href=#19-multi-task-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning where the model learns to perform multiple tasks simultaneously. The idea is that by sharing some of the learned representations across multiple tasks, the model can achieve better performance on each individual task. Multi-task learning is used in applications where the tasks are related, and the data is scarce. Suppose we want to build a machine learning model that can perform two related NLP tasks: sentiment analysis and topic classification. Sentiment analysis involves determining whether a given text expresses a positive or negative sentiment, while topic classification involves identifying the topic or subject of the text. Multi-task learning can be thought of as a type of multi-label classification (since it involves predicting multiple labels for each example), the focus of multi-label classification is the classification task itself, while the focus of multi-task learning is shared representations or knowledge transfer between tasks.</p> <p>Models used for multi-task learning are: - Hard Parameter Sharing: In this model, the input features are fed into a shared hidden layer, and then the outputs of the hidden layer are connected to task-specific output layers. All the tasks share the same parameters in the hidden layer. - Soft Parameter Sharing: This model is similar to hard parameter sharing, but instead of using the same parameters for all the tasks, it allows some degree of flexibility in the parameters by adding a regularization term. - Neural Task Graph: In this model, the tasks are represented as nodes in a graph, and the edges between the nodes represent the relationships between the tasks. The input features are fed into the graph, and the output of each task is computed based on its relationship with the other tasks. - Hierarchical Multi-Task Learning: This model uses a hierarchy of tasks, where the lower-level tasks provide features to the higher-level tasks. The lower-level tasks are typically simpler and more general, while the higher-level tasks are more specific and complex. - Adversarial Multi-Task Learning: This model uses adversarial training to learn multiple tasks simultaneously. It involves training a generator network to generate examples that are similar to the training data, and a discriminator network to distinguish between the generated examples and the real examples. The tasks are learned by training the generator to generate examples that are specific to each task, while the discriminator is trained to recognize the task-specific examples.</p> <h3 id=20-active-transfer-learning>20. Active transfer learning<a class=headerlink href=#20-active-transfer-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning where the model can transfer knowledge from "related different tasks" to improve its performance on the current task. Active transfer learning involves using knowledge acquired during the training of one model to improve the performance of a different, but related, model. This knowledge transfer typically involves fine-tuning the parameters of the new model using data from the old model, or using the old model as a source of features for the new model. Suppose we have a neural network that has been trained to classify images of dogs and cats. We now want to use this network to classify images of different breeds of dogs, but we only have a limited amount of labeled data for these new breeds. Active transfer learning is used in applications such as natural language processing and image recognition.</p> <p>Some examples of models for active transfer learning include:</p> <ul> <li>Adaptive SVM: This model uses a transfer-learning framework to adjust SVM decision boundaries for the target task by incorporating information from the source domain. Active learning is also used to select informative samples from the source domain.</li> <li>Multi-task Multi-view Gaussian Process (MT-MVGP): This model uses Gaussian processes to model multiple tasks and views simultaneously. It employs active transfer learning by selecting informative samples from the source domain to improve the accuracy of the target domain.</li> <li>Transfer active learning with oracle-guided instance selection (TAOIS): This model uses a two-stage approach. In the first stage, it selects informative samples from the source domain using active learning. In the second stage, it selects instances from the target domain that are most similar to the informative samples from the source domain to reduce the transfer error.</li> <li>Transfer Active Learning via Unsupervised Domain Adaptation (TALUDA): This model uses unsupervised domain adaptation to align the source and target domains. It then uses active learning to select the most informative samples from the source domain to adapt to the target domain.</li> </ul> <h3 id=21-meta-learning>21. Meta-learning<a class=headerlink href=#21-meta-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning where the model learns how to learn. Meta-learning is used in applications such as few-shot learning and reinforcement learning. Let's say the meta-learner has been trained on a set of classification tasks, such as classifying medical images, kitchen tools, and whether the given text is offensive or not. After training, the meta-learner has learned to recognize patterns and make accurate predictions for each of these tasks. Now, suppose a new classification task is presented to the system. For example, the task might involve classifying different species of animals based on images of their footprints. This is a new task because it involves a different type of data (footprint images), and a different set of categories to classify (animal species). The meta-learner analyzes the characteristics of the new task with a few samples and then uses its learned knowledge to generate a new model that is tailored to this specific task.</p> <p>Meta-learning can be approached through different types of models, some of which include: - Metric-based models: These models learn a distance metric between examples and use it to classify new examples. One example of a metric-based model is Prototypical Networks. - Model-based models: These models learn how to optimize parameters for a set of tasks, and use these optimized parameters to perform well on new tasks. An example of a model-based meta-learning algorithm is Model-Agnostic Meta-Learning (MAML). - Memory-based models: These models use a memory bank of examples to perform few-shot learning. Examples of memory-based models include Matching Networks and Relation Networks. - Optimization-based models: These models learn a set of optimization rules that can be applied to different tasks. An example of an optimization-based model is Learning to Learn by Gradient Descent by Gradient Descent (L2L-GD).</p> <h3 id=22-deep-reinforcement-learning>22. Deep reinforcement learning<a class=headerlink href=#22-deep-reinforcement-learning title="Permanent link">&para;</a></h3> <p>It is a subset of reinforcement learning that uses deep neural networks to learn from the environment. Deep reinforcement learning is used in applications such as game playing and robotics.</p> <p>Some popular models for deep reinforcement learning include: - Deep Q-Networks (DQN): DQN is a model that uses a deep neural network to approximate the Q-value function, which represents the expected total reward of taking an action in a given state and following the optimal policy thereafter. DQN was introduced by Google DeepMind in 2015 and was able to achieve superhuman performance in several Atari games. - Actor-Critic Models: Actor-critic models combine a policy network (the actor) that selects actions and a value network (the critic) that evaluates the value of state-action pairs. The two networks are trained together using gradient descent to maximize the expected reward. Examples of actor-critic models include Asynchronous Advantage Actor-Critic (A3C) and Trust Region Policy Optimization (TRPO). - Deep Deterministic Policy Gradient (DDPG): DDPG is an actor-critic model that is specifically designed for continuous action spaces. It uses a deterministic policy, meaning that given a state, it outputs a specific action rather than a probability distribution over actions. DDPG has been applied successfully in a variety of domains, including robotics and autonomous driving. - Proximal Policy Optimization (PPO): PPO is a family of policy optimization methods that use a trust region approach to update the policy parameters. It is a simpler and more stable alternative to TRPO, and has been shown to achieve state-of-the-art performance on a variety of benchmark environments. - Dueling Network Architectures (Dueling DQN): Dueling DQN is a modification of the DQN architecture that separates the Q-value function into two parts: one that estimates the value of being in a particular state and another that estimates the advantage of taking a particular action in that state. This separation allows the agent to better generalize across actions and states and has been shown to improve performance on a number of benchmark tasks.</p> <h3 id=23-adversarial-learning>23. Adversarial learning<a class=headerlink href=#23-adversarial-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning where the model is trained to defend against adversarial attacks. Adversarial learning is used in applications such as computer security and image recognition. If an image of "apple" is changed by adverserial attack but visually it looks the the same. In this situation, machine are bound to get confuse and identify this as some different object. We don't want machine should get confused by adverserial attack. We want machine should identify this is an "apple", as human can see. At the same time machine should tell this has been changed, which human cannot see.</p> <p>Some models commonly used in adversarial learning: - Generative Adversarial Networks (GANs): GANs are a type of generative model that consists of a generator and a discriminator. The generator learns to generate realistic examples that fool the discriminator, while the discriminator learns to distinguish between real and generated examples. - Adversarial Autoencoders (AAEs): AAEs are a type of generative model that consist of an encoder, a decoder, and a discriminator. The encoder maps the input data to a latent space, the decoder maps the latent space back to the input data, and the discriminator learns to distinguish between real and reconstructed examples. - Adversarial Training: Adversarial training involves training a model on adversarial examples as well as regular examples. This can improve the model's robustness to adversarial attacks. - Adversarial Examples Detection: Adversarial examples detection involves training a separate model to detect adversarial examples, which can be used to identify and reject such examples during inference. - Adversarial Regularization: Adversarial regularization involves adding a regularization term to the loss function that encourages the model to produce outputs that are robust to adversarial examples.</p> <h3 id=24-contrastive-learning>24. Contrastive learning<a class=headerlink href=#24-contrastive-learning title="Permanent link">&para;</a></h3> <p>It is a type of machine learning where the model learns to distinguish between similar and dissimilar instances in a dataset. Contrastive learning is used in applications such as representation learning and image recognition. An image of an "apple", a sound of word "apple", and text "apple" written on piece of paper, "apple" written on the image of apple, all are representing an idea "apple". If machine shows an image of apple and we type "orange" the system can say they both are different and your answer is wrong.</p> <p>Some popular models for contrastive learning are:</p> <ul> <li>SimCLR (Simple Contrastive Learning of Representations) - A simple framework for contrastive learning that achieves state-of-the-art performance on a wide range of downstream tasks.</li> <li>MoCo (Momentum Contrast) - A contrastive learning framework that uses a momentum encoder to update the representation of the query image.</li> <li>BYOL (Bootstrap Your Own Latent) - A self-supervised learning method that learns representations by predicting the representation of an augmented view of the same image.</li> <li>SwAV (Swapping Assignments between Modalities) - A method for contrastive learning that jointly learns representations from multiple modalities (e.g., images and text).</li> </ul> <h3 id=25-adaptive-learning>25. Adaptive Learning<a class=headerlink href=#25-adaptive-learning title="Permanent link">&para;</a></h3> <p>While buying online, or consuming online content, recommendation of new product or content is not based only on the past data about the product and buyer but what action buyer has taken recently. Adaptive learning gives more importance to the data points of current or present actions.</p> <p>There are many models and techniques that fall under adaptive learning. Here are a few examples: - Online gradient descent: This is a type of stochastic gradient descent (SGD) that updates the model parameters after every training example. It is commonly used in online learning, where data is streamed in real-time. - Actor-critic methods: These are reinforcement learning algorithms that have two components: an actor that selects actions, and a critic that evaluates the value of the selected actions. The actor-critic approach is widely used in robotics, gaming, and other applications. - Transfer learning: Transfer learning allows a model to leverage knowledge learned from a related task and apply it to a new task. This approach is commonly used in computer vision, natural language processing, and other fields. - Deep residual networks: These are deep neural networks that use residual connections to learn the difference between the input and the output, allowing for easier training and better performance. - Adaptive boosting (AdaBoost): This is an ensemble learning method that combines several weak learners to create a strong learner. The algorithm adapts the weight of the weak learners based on their performance. AdaBoost is commonly used in computer vision, speech recognition, and other applications.</p> <h2 id=26-federated-learning>26. Federated learning<a class=headerlink href=#26-federated-learning title="Permanent link">&para;</a></h2> <p>Federated learning is a machine learning approach that allows multiple devices or entities to collaboratively train a model without sharing their local data with each other or a central server. Instead, each entity trains the model on its local data and sends only the model parameters to a central server, which aggregates them to update the global model. This approach allows for privacy-preserving machine learning, as the raw data remains on the local devices and is not shared with any other parties.</p> <p>Federated learning has applications in various domains, including healthcare, finance, and Internet of Things (IoT) devices. For example, in healthcare, federated learning can be used to train a predictive model on patient data without compromising their privacy. Similarly, in finance, federated learning can be used to train models on transaction data from multiple banks without sharing sensitive information between them</p> <h2 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">&para;</a></h2> <p>Machine learning techniques are essential for solving many real-world problems, from image recognition and natural language processing to self-driving cars and medical diagnosis. Each method has its strengths and limitations, and the choice of technique depends on the specific problem and the available data. By understanding the different types of machine learning techniques, you can select the most appropriate method for your project and achieve better results. As the field of machine learning continues to evolve, we can expect to see new and more powerful techniques emerge, making it an exciting time for anyone interested in artificial intelligence.</p> <p><strong>Author</strong> <br> Dr Hari Thapliyaal <br> dasarpai.com <br> linkedin.com/in/harithapliyal </p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2025 Dr. Hari Thapliyaal </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/dasarpai target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=https://hub.docker.com/r/harithapliyal target=_blank rel=noopener title=hub.docker.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M349.9 236.3h-66.1v-59.4h66.1zm0-204.3h-66.1v60.7h66.1zm78.2 144.8H362v59.4h66.1zm-156.3-72.1h-66.1v60.1h66.1zm78.1 0h-66.1v60.1h66.1zm276.8 100c-14.4-9.7-47.6-13.2-73.1-8.4-3.3-24-16.7-44.9-41.1-63.7l-14-9.3-9.3 14c-18.4 27.8-23.4 73.6-3.7 103.8-8.7 4.7-25.8 11.1-48.4 10.7H2.4c-8.7 50.8 5.8 116.8 44 162.1 37.1 43.9 92.7 66.2 165.4 66.2 157.4 0 273.9-72.5 328.4-204.2 21.4.4 67.6.1 91.3-45.2 1.5-2.5 6.6-13.2 8.5-17.1zm-511.1-27.9h-66v59.4h66.1v-59.4zm78.1 0h-66.1v59.4h66.1zm78.1 0h-66.1v59.4h66.1zm-78.1-72.1h-66.1v60.1h66.1z"/></svg> </a> <a href=https://x.com/dasarpai target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.tooltips", "navigation.footer", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../assets/javascripts/custom.9e5da760.min.js></script> </body> </html>